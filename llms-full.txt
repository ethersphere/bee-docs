# Swarm Documentation

> Welcome to the Swarm

This file contains all documentation content in a single document following the llmstxt.org standard.

## Bee FAQ

## Running a Bee Node

### How can I become part of the Swarm network?

You can become part of the network by running a bee node. Bee is a peer-to-peer client that connects you with other peers all over the world to become part of the Swarm network, a global distributed p2p storage network that aims to store and distribute all of the world's data

Depending on your needs you can run an ultra-light, light or full node.

### What are the differences between Bee node types?

A bee node can be configured to run in various modes based on specific use cases and requirements. [See here](/docs/bee/installation/getting-started) for an overview of the differences.

#### What are the requirements for running a Bee node?

See the [getting started section](/docs/bee/installation/getting-started#software-requirements) for more information about running a Bee node.

##### Full node

- 20GB -30GB SSD (ideally NVME).
- 8GB RAM
- CPU with 2+ cores
- RPC connection to Gnosis Chain
- Min 0.1 xDAI for Gnosis GAS fees
- 1 xBZZ for initial chequebook deployment
- 10 xBZZ for staking (optional)

##### How much bandwidth is required for each node?

Typically, each node requires around 10 megabits per second (Mbps) of bandwidth during normal operation.

##### How do I Install Bee on Windows?

Bee is compatible with Windows and a Bee `.exe` file can be found on the [`releases` page](https://github.com/ethersphere/bee/releases) of the Bee repo.  

It is also possible to [build from the source](/docs/bee/installation/build-from-source).

##### How do I get the node's wallet's private key (use-case for Desktop app)?

See the [backup section](/docs/bee/working-with-bee/backups/) for more info.

##### How do I import my private key to Metamask?

You can import the `swarm.key` json file in MetaMask using your password file or the password you have set in your bee config file.

##### Where can I find my password?

You can find the password in the root of your data directory. See the [backup section](/docs/bee/working-with-bee/backups/) for more info.

## Connectivity

### Which p2p port does Bee use and which should I open in my router?

The default p2p port for Bee is 1634, please forward this using your router and allow traffic over your firewall as necessary. Bee also supports UPnP but it is recommended you do not use this protocol as it lacks security. For more detailed information see the connectivity section in the docs. https://docs.ethswarm.org/docs/bee/installation/connectivity

### How do I know if I am connected to other peers?

You may communicate with your Bee using its HTTP api. Type `curl http://localhost:1633/peers` at your command line to see a list of your peers.

## Errors

### What does "could not connect to peer" mean?

The "Could not connect to peer" error can occur for various reasons. One of the most common is that you have the identifier of a peer in your address book from a previous session. When trying to connect to this node again, the peer may no longer be online.

### What does "context deadline exceeded" error mean?

The "context deadline exceeded" is a non-critical warning. It means that a node took unexpectedly long to respond to a request from your node. Your node will automatically try again via another node.

### How do I set up a blockchain endpoint?

We recommend you run your own [Gnosis Node using Nethermind](https://docs.gnosischain.com/node/tools/sedge).

- If you use "bee start"

  - you can set it in your bee configuration under --blockchain-rpc-endpoint or BEE_BLOCKCHAIN_RPC_ENDPOINT
  - open ~/.bee.yaml
  - set `blockchain-rpc-endpoint: http://localhost:8545`

- If you use bee.service
  - you can set it in your bee configuration under --blockchain-rpc-endpoint or BEE_BLOCKCHAIN_RPC_ENDPOINT
  - open /etc/bee/bee.yaml
  - and then uncomment `blockchain-rpc-endpoint` configuration
  - and set it to `http://localhost:8545`
  - after that sudo systemctl restart bee

### How can I export my private keys?

See the section on [backups](/docs/bee/working-with-bee/backups) for exporting your keys.

### How to import bee node address to MetaMask?

1. See the [backup section](/docs/bee/working-with-bee/backups/) for info on exporting keys.
2. Go to Metamask and click "Account 1" --> "Import Account"
3. Choose the "Select Type" dropdown menu and choose "JSON file"
4. Paste the password (Make sure to do this first)
5. Upload exported JSON file  
6. Click "Import"

### What are the restart commands of bee?

If you use bee.service:

- Start: `sudo systemctl start bee.service`
- Stop: `sudo systemctl stop bee.service`
- Status: `sudo systemctl status bee.service`

If you use "bee start"

- Start: `bee start`
- Stop: `ctrl + c` or `cmd + c` or close terminal to stop process

### Relevant endpoints and explanations

See the [API Reference](https://docs.ethswarm.org/api/) pages for details.

### How can I check how many cashed out cheques do I have?

You can look at your chequebook contract at etherscan.
Get your chequebook contract address with: `curl http://localhost:1633/chequebook/address`

### Where can I find documents about the cashout commands?

Learn how to cash out [here](/docs/bee/working-with-bee/cashing-out).

### When I run http://localhost:1633/chequebook/balance I get "totalBalance" and "availableBalance" what is the difference?

`totalBalance` is the balance on the blockchain, and `availableBalance` is that balance minus the outstanding (non-cashed) cheques that you have issued to your peers. These latter cheques do not show up on the blockchain.

It's like what the bank thinks your balance is vs what your chequebook knows is actually available because of the cheques you've written that are still "in the mail" and not yet cashed.

### What determines the number of peers and how to influence their number? Why are there sometimes 300+ peers and sometimes 30?

The number of connected peers is determined by your node as it attempts to keep the distributed Kademlia well connected. As nodes come and go in the network your peer count will go up and down. If you watch bee's output logs for "successfully connected", there should be a mix of (inbound) and (outbound) at the end of those messages. If you only get (outbound) then you may need to get your p2p port opened through your firewall and/or forwarded by your router. Check out the connectivity section in the docs https://docs.ethswarm.org/docs/bee/installation/connectivity.

### What is the difference between "systemctl" and "bee start"?

_bee start_ and _systemctl start bee_ actually run 2 different instances with 2 different _bee.yaml_ files and two different data directories.

_bee start_ uses _~/.bee.yaml_ and the _~/.bee_ directory for data
_systemctl_ uses _/etc/bee/bee.yaml_ and (IIRC) _/var/lib/bee_ for data

## Swarm Protocol

### Can I use one Ethereum Address/Wallet for many nodes?

No, this violates the requirements of the Swarm Protocol and will break critical node functions such as staking, purchasing stamp batches, and uploading data. 

Therefore, the rule is, each node must have:

- 1 Ethereum address (this address, the Swarm network id, and a random nonce are used to determine the node's overlay address)
- 1 Chequebook
- 2 Unique ports for Bee API / p2p API

## Miscellaneous

### How can I add Gnosis / Sepolia to Metamask?

You can easily add Sepolia or Gnosis to metamask using the [official guide from Metamask](https://support.metamask.io/networks-and-sidechains/managing-networks/how-to-add-a-custom-network-rpc/).

If you are using a different wallet which does not have an easy option for adding networks like Metamask does, then you may need to add the networks manually. You need to fill in four pieces of information to do so:

#### Gnosis Chain

Network name: Gnosis
RPC URL: https://xdai.fairdatasociety.org
Chain ID: 100
Currency symbol: XDAI

#### Sepolia

Network name: Sepolia test network
RPC URL: https://sepolia.infura.io/v3/
Chain ID: 11155111
Currency symbol: SepoliaETH

---

## Build from Source

Bee is written using the [Go](https://golang.org) language.

You may build the Bee client software directly from the [source](https://github.com/ethersphere/bee).

Prerequisites for installing directly from source are:

-  **go** - download the latest release from [golang.org](https://golang.org/dl).
-  **git** - download from [git-scm.com](https://git-scm.com/).
-  **make** - [make](https://www.gnu.org/software/make/) is usually included by default in most UNIX operating systems, and can be installed and used on almost any other operating system where it is not included by default.

### Build from Source

1. Clone the repository:

   ```bash
   git clone https://github.com/ethersphere/bee
   cd bee
   ```

2. Use `git` to find the latest release:

   ```bash
   git describe --tags
   ```

3. Checkout the required version:

   ```bash
   git checkout v2.6.0
   ```

4. Build the binary:

   ```bash
   make binary
   ```

5. Check that you are able to run the `bee` command. Success can be verified by running:

   ```bash
   dist/bee version
   ```

   ```
   2.6.0
   ```

6. (optional) Additionally, you may also like to move the Bee binary to somewhere in your `$PATH`

   ```bash
   sudo cp dist/bee /usr/local/bin/bee
   ```

---

## Connectivity

To fully connect to the swarm, your Bee node needs to be able to both
send and receive messages from the outside world. Normally, your
router will not allow other IPs on the Internet to connect, unless
you have initiated the connection. Bees welcome newcomers in the
swarm, as long as they play by the rules! If a node misbehaves, we
will simply add it to a list of blocked nodes and refuse future
connections from them.

Here at Swarm, every Bee counts! To make sure all Bees can join the
swarm, below you will find a detailed guide to navigating your way
through your network and making it out into the wild so you can buzz
around fellow bees and maximize your chances of earning xBZZ. If
you still have problems, please join us in our [Discord
server](https://discord.gg/wdghaQsGq5) and we'll help you find the
way! üêù üêù üêù üêù üêù

:::info
To ensure your Bee has the best chance of participating in the swarm,
you must ensure your Bee is able to handle **both incoming and
outgoing connections from the global Internet to its p2p port
(`1634` by default)**. See below for a detailed guide on how to make sure
this is the case, or for the 1337: check your
`http://localhost:1633/addresses` to see which public IP and port
libp2p is advertising and verify its connectivity to the rest of the
Internet! You may need to alter your Bee node's `nat-addr`
configuration. ü§ì
:::

## Networking Basics

In a network, each computer is assigned an IP address. Each IP address
is then subdivided into thousands of _sockets_ or _ports_, each of
which has an incoming and outgoing component.

In a completely trusted network of computers, any connections to or
from any of these ports are allowed. However, to protect ourselves
from nefarious actors when we join the wider Internet, it is sometimes
important to filter this traffic so that some of these ports are off
limits to the public.

In order to allow messages to our p2p port from other Bee nodes that
we have previously not connected, we must ensure that our network is
set up to receive incoming connections (on port `1634` by default).

:::danger
There are also some ports which you should never expose to the outside Internet. Make sure that your `api-addr` (default `1633`) is never exposed to the internet. It is good practice to employ one or more firewalls that block traffic on every port except for those you are expecting to be open. If you do not use a firewall, make sure to change the default `api-addr` from `1633` to `127.0.0.1:1633` so that it is not publicly exposed.
:::

### Your IP Address

When you connect to the Internet, you are assigned a unique number
called an IP Address. IP stands for **Internet Protocol**. The most
prevalent IP version used is _still_ the archaic
[IPv4](https://en.wikipedia.org/wiki/IPv4) which was invented way back
in 1981. IPv6 is available but not well used. Due to the mitigation of
the deficiencies inherent in the IPv4 standard, some complications may arise.

### Datacenters and Computers Connected Directly to the Internet

If you are renting space in a datacenter, the chances are that your computer will be connected directly to the real Internet. This means that the IP of your networking interface will be directly set to be the same as your public IP.

You can investigate this by running:

```bash
ifconfig
```

or

```bash
ip address
```

Your output should contain something like:

```
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 178.128.196.191  netmask 255.255.240.0  broadcast 178.128.207.255
```

Here we can see our computer's **public IP address**
`178.128.196.191`. This is the address that is used by other computers
we connect to over the Internet. We can verify this using a third
party service such as _icanhazip_ or _ifconfig_.

```bash
curl icanhazip.com --ipv4
```

or

```bash
curl ifconfig.co --ipv4
```

The response may contain something like:

```
178.128.196.191
```

With Bee running, try to connect to your Bee's p2p port using the public IP address from another computer:

```bash
nc -zv 178.128.196.191 1634
```

If you have success, congratulations!

If this still doesn't work for you, see the last part of _Manual: Configure Your Router and Bee_ section below, as you may need to configure your `nat-addr`.

### Home, Commercial and Business Networks and Other Networks Behind NAT

To address the
[scarcity of IP numbers](https://en.wikipedia.org/wiki/IPv4_address_exhaustion),
Network Address Translation (NAT) was implemented. This approach
creates a smaller, private network which many devices connect to in
order to share a public IP address. Traffic destined for the Internet
at large is then mediated by another specialised computer. In the
cases of the a home network, this computer is the familiar home
router, normally also used to provide a WiFi network.

If we run the above commands to find the computer's IP in this scenario, we will see a different output.

```bash
ip address
```

```
en0: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
	...
	inet 192.168.0.10 netmask 0xffffff00 broadcast 192.168.0.255
	...
```

Here we can see that, instead of the public IP address, we can see
that our computer's IP address is `192.168.0.10`. This is part of the
IP address space that the Internet Engineering Task Force has
designated for
[private networks](https://en.wikipedia.org/wiki/Private_network).

As this IP won't work on the global Internet, our router remembers
that our computer has been assigned this IP. It then uses _Network
Address Translation_ (NAT) to modify all requests from our computer to
another computer somewhere in the Internet. As the requests pass
through the router it changes our local IP to the public IP of the
router, and vice versa when the responses are sent back, from the
public IP to the local one.

## Navigating Through the NAT

The presence of NAT presents two problems for p2p networking.

The first is that it can be difficult for programs running on our computer to know our real public IP as it is not explicitly known by our computer's networking interface, which is configured with a private network IP. This is a relatively easy problem to solve as we can simply discover our public IP and then specify it in Bee's configuration, or indeed determine it using other means.

The second issue is that our router has only 65535 ports to expose to
the public network. However, *each device on your private network is
capable of exposing 65535 ports*. To the global Internet, it appears
that there is only one set of ports to connect to, whereas, in actual
fact, there is a full set of ports for each of the devices which are
connected to the private network. To solve this second problem,
routers commonly employ an approach known as _port forwarding_.

Bee's solution to these problems come in two flavours, automatic and manual.

### Automatic: Universal Plug and Play (UPnP)

UPnP is a protocol designed to simplify the administration of NAT and
port forwarding for the end user by providing an API from which
software running within the network can use to ask the router for the
public IP and to request for ports to be forwarded to the private IP
of the computer running the software.

:::danger UPnP is a security risk!
UPnP is a security risk as it allows any host or process inside
(sometimes also outside) your network to open arbitrary ports which
may be used to transfer malicious traffic, for example a
[RAT](https://en.wikipedia.org/wiki/Remote_desktop_software#RAT). UPnP
can also be used to determine your IP, and in the case of using ToR or
a VPN, your _real_ public IP. We urge you to disable UPnP on your
router and use manual port forwarding as described below.
:::

Bee will use UPnP to determine your public IP, which is required for various internal processes.

In addition to this, a request will be sent to your router to ask it
to forward a random one of its ports, which are exposed directly to
the Internet, to the Bee p2p port (default `1634`) which your computer
is exposing only to the private network. Doing this creates a tunnel
through which other Bees may connect to your computer safely.

If you start your Bee node in a private network with UPnP available, the output of the addresses endpoint of your API will look something like this:

```json
[
  "/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP",
  "/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP",
  "/ip6/::1/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP",
  "/ip4/86.98.94.9/tcp/20529/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP"
]
```

Note that the port in the external
[multiaddress](https://docs.libp2p.io/concepts/addressing/) is the
router's randomly selected `20529` which is forwarded by the router to
`192.168.0.10:1634`. These addresses in this multiaddress are also
known as the underlay addresses.

### Manual: Configure Your Router and Bee

Inspecting the underlay addresses in the output of the addresses
endpoint of our API, we can see addresses only for _localhost_
`127.0.0.1` and our _private network IP_ `192.168.0.10`. Bee must be
having trouble navigating our NAT.

```json
[
  "/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB",
  "/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB",
  "/ip6/::1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB"
]
```

To help fix the first problem, let's determine our public IP address.

```bash
curl icanhazip.com
```

```
86.98.94.9
```

Now we can simply supply this IP in our Bee configuration on startup.

Solving our second problem is a little more difficult as we will need to interact with our router's firmware, which is a little cranky.

Each router is different, but the concept is usually the same. Log in to your router by navigating your browser to your router's configuration user interface, usually at [http://192.168.0.1](http://192.168.0.1). You will need to log in with a password. Sadly, passwords are often left to be the defaults, which can be found readily on the Internet.

Once logged in, find the interface to set up port forwarding. The [Port Forward](https://portforward.com/router.htm) website provides some good information, or you may refer to your router manual or provider.

Here, we will then set up a rule that forwards port `1634` of our
private IP address `192.168.0.10` to the same port `1634` of our
public IP.

Now, when requests arrive at our public address `86.98.94.9:1634` they
are modified by our router and forwarded to our private IP and port
`192.168.0.10:1634`.

Sometimes this can be a little tricky, so let's verify we are able to make a TCP connection using [netcat](https://nmap.org/ncat/).

First, with Bee **not** running, let's set up a simple TCP listener using Netcat on the same machine we would like to run Bee on.

```bash
nc -l 0.0.0.0 1634
```

```bash
nc -zv 86.98.94.9 1634
```

```
Connection to 86.98.94.9 port 1834 [tcp/*] succeeded!
```

Success! ‚ú®

If this didn't work for you, check out our Debugging Connectivity guide below.

If it did, let's start our Bee node with the `--nat-addr` configured.

```bash
bee start --nat-addr 86.98.94.9:1634
```

Checking our addresses endpoint again, we can now see that Bee has been able to successfully assign a public address! Congratulations, your Bee is now connected to the outside world!

```json
[
  "/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB",
  "/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB",
  "/ip6/::1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB",
  "/ip4/86.98.94.9/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB"
]
```

:::info
If you are regularly connecting and disconnecting to a network, you
may also want to use your router's firmware to configure the router to
reserve and only assign the same local network IP from its DHCP pool
to your computer's MAC address. This will ensure that your Bee
seamlessly connects when you rejoin the network!
:::

### Debugging Connectivity

The above guide navigates your NAT, but there are still a few hurdles to overcome. To make sure there is a clear path from your computer to the outside world, let's follow our Bee's journey from the inside out.

Let's set up a netcat listener on all interfaces on the computer we'd
like to run Bee on as we have above.

```bash
nc -l 0.0.0.0 1634
```

Now, let's verify we're able to connect to netcat by checking the connection from our local machine.

```bash
nc -zv 127.0.0.1 1634
```

```
Connection to 127.0.0.1 port 1634 [tcp/*] succeeded!
```

This should be a no brainer, the connection between localhost in not normally mediated.

If there is a problem here, the problem is with some other software running on your operating system or your operating system itself. Try a different port, such as `1734` and turning off any unnecessary software. If this doesn't work, you may need to try a different operating system environment. Please get in touch and we'll try to help!

If we were successful, let's move on to the next stage.

:::info
If you are not able to get access to some firewall settings, or
otherwise debug incoming connectivity, don't worry! All is not
lost. Bee can function just fine with just outgoing
connections. However, if you can, it is worth the effort to allow
incoming connections, as the whole swarm will benefit from the
increased connectivity.
:::

Let's find out what our IP looks like to the Internet.

```bash
curl icanhazip.com
```

```
86.98.94.9
```

Now try to connect to your port using the global IP.

```bash
nc -zv 86.98.94.9 1634
```

If this is successful, our Bee node's path is clear!

If not, we can try a few things to make sure there are no barriers stopping us from getting through.

1. Check your computer's firewall.

Sometimes your computer is configured to prevent connections. If you
are on a private network mediated by NAT, you can check if this is
the problem by trying to connect from another device on your network
using the local IP `nc -zv 192.168.0.10 1634`.

Ubuntu uses [UFW](https://help.ubuntu.com/community/UFW), MacOS can
be configured using the _Firewall_ tab in the _Security & Privacy_
section of _System Preferences_. Windows uses
[Defender Firewall](https://support.microsoft.com/en-us/help/4028544/windows-10-turn-microsoft-defender-firewall-on-or-off).

For each of these firewalls, set a special rule to allow UDP and TCP
traffic to pass through on port `1634`. You may want to limit this
traffic to the Bee application only.

2. Check your ingress' firewall.

For a datacenter hired server, this configuration will often take
place in somewhere in the web user interface. Refer to your server
hosting provider's documentation to work out how to open ports to
the open Internet. Ensure that both TCP and UDP traffic are allowed.

Similarly, if you are connecting from within a private network, you
may find that the port is blocked by the router. Each router is
different, so consult your router's firware documentation to make
sure there are no firewalls in place blocking traffic on your Bee's
designated p2p port.

You may check this using netcat by trying to connect using your
computer's public IP, as above `nc -zv 86.98.94.9 1634`.

3. Docker

Docker adds another level of complexity.

To debug docker connectivity issues, we may use netcat as above to
check port connections are working as expected. Double check that
you are exposing the right ports to your local network, either by
using the command line flags or in your docker-compose.yaml. You
should be able to successfully check the connection locally using
eg. `nc -zv localhost 1634` then follow instructions above to make
sure your local network has the correct ports exposed to the
Internet.

3. Something else entirely?

Networking is a complex topic, but it keeps us all together. If you
still can't connect to your Bee, get in touch via the [official node operator's Discord channel](https://discord.gg/kHRyMNpw7t) and we'll do our best to get
you connected. In Swarm, no Bee is left behind.

---

## Docker Install


The following is a guide for installing a Bee node using Docker. Docker images for Bee are hosted at [Docker Hub](https://hub.docker.com/r/ethersphere/bee). Using Docker to operate your Bee node offers many benefits, such as ease of deployment and consistency across environments. 

:::caution
In the examples below we specify the exact image version as 2.6.0. It's recommended to only use the exact version number tags. Make sure to check that you're on the latest version of Bee by reviewing the tags for Bee on [Docker Hub](https://hub.docker.com/r/ethersphere/bee/tags), and replace 2.6.0 in the commands below if there is a newer full release. 
:::

:::warning
Note that in all the examples below we map the Bee API to 127.0.0.1 (localhost), since we do not want to expose our Bee API endpoint to the public internet, as that would allow anyone to control our node. Make sure you do the same, and it's also recommended to use a  firewall to protect access to your node(s).
:::

:::info
This guide sets options using environment variables as part of the Docker startup commands such as `-e BEE_API_ADDR=":1633"`, however there are [several other methods available for configuring options](/docs/bee/working-with-bee/configuration). 
:::

## Node setup process 

This section will guide you through setting up and running a single full Bee node using Docker. In the guide, we use a single line command for running our Bee node, with the Bee config options being set through environment variables, and a single volume hosted for our node's data. 

### Start node

```bash
docker run -d --name bee-1 \
  --restart always \
  -p 127.0.0.1:1633:1633 \
  -p 1634:1634 \
  -e BEE_API_ADDR=":1633" \
  -e BEE_FULL_NODE="true" \
  -e BEE_SWAP_ENABLE="true" \
  -e BEE_PASSWORD="flummoxedgranitecarrot" \
  -e BEE_BLOCKCHAIN_RPC_ENDPOINT="https://xdai.fairdatasociety.org" \
  -v bee-1:/home/bee/.bee \
  ethersphere/bee:2.6.0 start
```

Here is the same command in a single line in case you run into issues with the line breaks in the command above:

```bash
docker run -d --name bee-1 --restart always -p 127.0.0.1:1633:1633 -p 1634:1634 -e BEE_API_ADDR=":1633" -e BEE_FULL_NODE="true" -e BEE_SWAP_ENABLE="true" -e BEE_PASSWORD="flummoxedgranitecarrot" -e BEE_BLOCKCHAIN_RPC_ENDPOINT="https://xdai.fairdatasociety.org" -v bee-1:/home/bee/.bee ethersphere/bee:2.6.0 start
```

#### Command explained:

- **`-d`**: Runs the container in the background.
- **`--restart always`**: Sets the [restart policy](https://docs.docker.com/engine/containers/start-containers-automatically/) for the container to `always`.
- **`--name bee-1`**: Names the container `bee-1`.
- **`-p 127.0.0.1:1633:1633`**: Exposes the API on port 1633, only accessible locally.
- **`-p 1634:1634`**: Exposes the P2P port 1634 to the public.
- **`-e BEE_API_ADDR=":1633"`**: Sets the Bee API to use port 1633.
- **`-e BEE_FULL_NODE="true"`**: Runs as a full node.
- **`-e BEE_SWAP_ENABLE="true"`**: Enables the SWAP protocol for payments.
- **`-e BEE_PASSWORD="flummoxedgranitecarrot"`**: Sets the keystore password, make sure to replace with your own.
- **`-e BEE_BLOCKCHAIN_RPC_ENDPOINT="https://xdai.fairdatasociety.org"`**: Connects to the Gnosis Chain.
- **`-v bee-1:/home/bee/.bee`**: Persists node data in the `bee-1` volume.
- **`ethersphere/bee:2.6.0 start`**: Runs Bee version 2.6.0 and starts the node.

This setup runs the Bee node in a container, with full node functionality, SWAP enabled, and connections to the Gnosis blockchain for chequebook and postage stamp management, while persisting its data using a volume. 

:::info
We have included the password directly in the start command as an environment variable with `-e BEE_PASSWORD="flummoxedgranitecarrot"`. You may wish to use a password file instead, which can be set with the `BEE_PASSWORD_FILE` command. However this will likely require some modifications on your host machine, the details of which will vary from system to system.
:::

```bash
docker ps
```

If everything is set up correctly, you should see your Bee node listed:

```bash
CONTAINER ID     IMAGE     COMMAND     CREATED     STATUS     PORTS     NAMES
37f4ad8b4060   ethersphere/bee:2.6.0   "bee start"   6 seconds ago   Up 5 seconds   127.0.0.1:1633->1633/tcp, 0.0.0.0:1634->1634/tcp, :::1634->1634/tcp   bee-1
```

And check the logs:

```bash
docker logs -f bee-1
```

The output should contain a line which prints a message notifying you of the minimum required xDAI for running a node as well as the address of your node. Copy the address and save it for use in the next section.

```bash
"time"="2024-09-24 22:06:51.363708" "level"="warning" "logger"="node/chequebook" "msg"="cannot continue until there is at least min xDAI (for Gas) available on address" "min_amount"="0.0003576874793" "address"="0x91A7e3AC06020750D32CeffbEeFD55B4c5e42bd6"
```

You can use `Ctrl + C` to exit the logs.

Before moving on to funding, stop your node:

```bash
docker stop bee-1
```

And let's confirm that it has stopped:

```bash
docker ps
```

We can confirm no Docker container processes are currently running.

```bash
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
````

### Fund node

Check the logs from the previous step. Look for the line which says: 

```
"time"="2024-09-24 18:15:34.520716" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
```
That address is your node's address on Gnosis Chain which needs to be funded with xDAI and xBZZ. Copy it and save it for the next step.

xDAI is widely available from many different centralized and decentralized exchanges, just make sure that you are getting xDAI on Gnosis Chain, and not DAI on some other chain. See [this page](https://www.ethswarm.org/get-bzz) for a list of resources for getting xBZZ (again, make certain that you are getting the Gnosis Chain version, and not BZZ on Ethereum).  

After acquiring some xDAI and some xBZZ, send them to the address you copied above.

***How Much to Send?***

Only a very small amount of xDAI is needed to get started, 0.1 is more than enough.
 
You can start with just 2 or 3 xBZZ for uploading small amounts of data, but you will need at least 10 xBZZ if you plan on staking.

### Initialize full node

After you have a small amount of xDAI in your node's Gnosis Chain address, you can now restart your node using the same command as before so that it can issue the required smart contract transactions and also sync data. 

```bash
docker start bee-1
```

Let's check the logs to see what's happening:

```bash
docker logs -f bee-1
```

Your logs should look something like this:

```bash
Welcome to Swarm.... Bzzz Bzzzz Bzzzz
                \     /
            \    o ^ o    /
              \ (     ) /
   ____________(%%%%%%%)____________
  (     /   /  )%%%%%%%(  \   \     )
  (___/___/__/           \__\___\___)
     (     /  /(%%%%%%%)\  \     )
      (__/___/ (%%%%%%%) \___\__)
              /(       )\
            /   (%%%%%)   \
                 (%%%)
                   !

DISCLAIMER:
This software is provided to you "as is", use at your own risk and without warranties of any kind.
It is your responsibility to read and understand how Swarm works and the implications of running this software.
The usage of Bee involves various risks, including, but not limited to:
damage to hardware or loss of funds associated with the Ethereum account connected to your node.
No developers or entity involved will be liable for any claims and damages associated with your use,
inability to use, or your interaction with other nodes or the software.

version: 2.6.0-06a0aca7 - planned to be supported until 11 December 2024, please follow https://ethswarm.org/

"time"="2024-09-24 22:21:04.543661" "level"="info" "logger"="node" "msg"="bee version" "version"="2.6.0-06a0aca7"
"time"="2024-09-24 22:21:04.590823" "level"="info" "logger"="node" "msg"="swarm public key" "public_key"="02f0e59eafa3c5c06542c0a7a7fe9579c55a163cf1d28d9f6945a34469f88d1b2a"
"time"="2024-09-24 22:21:04.686430" "level"="info" "logger"="node" "msg"="pss public key" "public_key"="02ea739530bbf48eed49197f21660f3b6564709b95bf558dc3b472688c34096418"
"time"="2024-09-24 22:21:04.686464" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x8288F1c8e3dE7c3bf42Ae67fa840EC61481D085e"
"time"="2024-09-24 22:21:04.700711" "level"="info" "logger"="node" "msg"="using overlay address" "address"="22dc155fe072e131449ec7ea2f77de16f4735f06257ebaa5daf2fdcf14267fd9"
"time"="2024-09-24 22:21:04.700741" "level"="info" "logger"="node" "msg"="starting with an enabled chain backend"
"time"="2024-09-24 22:21:05.298019" "level"="info" "logger"="node" "msg"="connected to blockchain backend" "version"="Nethermind/v1.28.0+9c4816c2/linux-x64/dotnet8.0.8"
"time"="2024-09-24 22:21:05.485287" "level"="info" "logger"="node" "msg"="using chain with network network" "chain_id"=100 "network_id"=1
"time"="2024-09-24 22:21:05.498845" "level"="info" "logger"="node" "msg"="starting debug & api server" "address"="[::]:1633"
"time"="2024-09-24 22:21:05.871498" "level"="info" "logger"="node" "msg"="using default factory address" "chain_id"=100 "factory_address"="0xC2d5A532cf69AA9A1378737D8ccDEF884B6E7420"
"time"="2024-09-24 22:21:06.059179" "level"="info" "logger"="node/chequebook" "msg"="no chequebook found, deploying new one."
"time"="2024-09-24 22:21:07.386747" "level"="info" "logger"="node/chequebook" "msg"="deploying new chequebook" "tx"="0x375ca5a5e0510f8ab307e783cf316dc6bf698c15902a080ade3c1ea0c6059510"
"time"="2024-09-24 22:21:19.101428" "level"="info" "logger"="node/transaction" "msg"="pending transaction confirmed" "sender_address"="0x8288F1c8e3dE7c3bf42Ae67fa840EC61481D085e" "tx"="0x375ca5a5e0510f8ab307e783cf316dc6bf698c15902a080ade3c1ea0c6059510"
"time"="2024-09-24 22:21:19.101450" "level"="info" "logger"="node/chequebook" "msg"="chequebook deployed" "chequebook_address"="0x66127e4393956F11947e9f54599787f9E455173d"
"time"="2024-09-24 22:21:19.506515" "level"="info" "logger"="node" "msg"="using datadir" "path"="/home/bee/.bee"
"time"="2024-09-24 22:21:19.518258" "level"="info" "logger"="migration-RefCountSizeInc" "msg"="starting migration of replacing chunkstore items to increase refCnt capacity"
"time"="2024-09-24 22:21:19.518283" "level"="info" "logger"="migration-RefCountSizeInc" "msg"="migration complete"
"time"="2024-09-24 22:21:19.566160" "level"="info" "logger"="node" "msg"="starting reserve repair tool, do not interrupt or kill the process..."
"time"="2024-09-24 22:21:19.566232" "level"="info" "logger"="node" "msg"="removed all bin index entries"
"time"="2024-09-24 22:21:19.566239" "level"="info" "logger"="node" "msg"="removed all chunk bin items" "total_entries"=0
"time"="2024-09-24 22:21:19.566243" "level"="info" "logger"="node" "msg"="counted all batch radius entries" "total_entries"=0
"time"="2024-09-24 22:21:19.566247" "level"="info" "logger"="node" "msg"="parallel workers" "count"=20
"time"="2024-09-24 22:21:19.566271" "level"="info" "logger"="node" "msg"="migrated all chunk entries" "new_size"=0 "missing_chunks"=0 "invalid_sharky_chunks"=0
"time"="2024-09-24 22:21:19.566294" "level"="info" "logger"="migration-step-04" "msg"="starting sharky recovery"
"time"="2024-09-24 22:21:19.664643" "level"="info" "logger"="migration-step-04" "msg"="finished sharky recovery"
"time"="2024-09-24 22:21:19.664728" "level"="info" "logger"="migration-step-05" "msg"="start removing upload items"
"time"="2024-09-24 22:21:19.664771" "level"="info" "logger"="migration-step-05" "msg"="finished removing upload items"
"time"="2024-09-24 22:21:19.664786" "level"="info" "logger"="migration-step-06" "msg"="start adding stampHash to BatchRadiusItems, ChunkBinItems and StampIndexItems"
"time"="2024-09-24 22:21:19.664837" "level"="info" "logger"="migration-step-06" "msg"="finished migrating items" "seen"=0 "migrated"=0
"time"="2024-09-24 22:21:19.664897" "level"="info" "logger"="node" "msg"="waiting to sync postage contract data, this may take a while... more info available in Debug loglevel"
```

Your node will take some time to finish [syncing postage contract data](https://docs.ethswarm.org/docs/develop/tools-and-features/buy-a-stamp-batch/) as indicated by the final line:

```bash
"msg"="waiting to sync postage contract data, this may take a while... more info available in Debug loglevel"
```

You may need to wait 5 - 10 minutes for your node to finish syncing in this step. 

Eventually you will be able to see when your node finishes syncing, and the logs will indicate your node is starting in full node mode:

```bash
"time"="2024-09-24 22:30:19.154067" "level"="info" "logger"="node" "msg"="starting in full mode"
"time"="2024-09-24 22:30:19.155320" "level"="info" "logger"="node/multiresolver" "msg"="name resolver: no name resolution service provided"
"time"="2024-09-24 22:30:19.341032" "level"="info" "logger"="node/storageincentives" "msg"="entered new phase" "phase"="reveal" "round"=237974 "block"=36172090
"time"="2024-09-24 22:30:33.610825" "level"="info" "logger"="node/kademlia" "msg"="disconnected peer" "peer_address"="6ceb30c7afc11716f866d19b7eeda9836757031ed056b61961e949f6e705b49e"
```

Your node will now begin syncing chunks from the network, this process can take several hours. You check your node's progress with the `/status` endpoint:

```bash
curl -s  http://localhost:1633/status | jq
```

```bash
{
  "overlay": "22dc155fe072e131449ec7ea2f77de16f4735f06257ebaa5daf2fdcf14267fd9",
  "proximity": 256,
  "beeMode": "full",
  "reserveSize": 686217,
  "reserveSizeWithinRadius": 321888,
  "pullsyncRate": 497.8747754074074,
  "storageRadius": 11,
  "connectedPeers": 148,
  "neighborhoodSize": 4,
  "batchCommitment": 74510761984,
  "isReachable": false,
  "lastSyncedBlock": 36172390
}
```
We can see that our node has not yet finished syncing chunks since the `pullsyncRate` is around 497 chunks per second. Once the node is fully synced, this value will go to zero. It can take several hours for syncing to complete, but we do not need to wait until our node is fully synced before staking, so we can move directly to the next step. 

### Stake node

You can use the following command to stake 10 xBZZ:

```bash
curl -XPOST localhost:1633/stake/100000000000000000
```

If the staking transaction is successful a `txHash` will be returned:

```
{"txHash":"0x258d64720fe7abade794f14ef3261534ff823ef3e2e0011c431c31aea75c2dd5"}
```

We can also confirm that our node has been staked with the `/stake` endpoint:

```bash
curl localhost:1633/stake
```

The results will be displayed in PLUR units (1 PLUR is equal to 1e-16 xBZZ). If you have properly staked the minimum 10 xBZZ, you should see the output below:

```bash
{"stakedAmount":"100000000000000000"}
```

Congratulations! You have now installed your Bee node and successfully connected it to the network as a full staking node. Your node will now be in the process of syncing chunks from the network. Once it is fully synced, your node will finally be eligible for earning staking rewards. 

### Set Target Neighborhood

When installing your Bee node it will automatically be assigned a neighborhood. However, when running a full node with staking there are benefits to periodically updating your node's neighborhood. Learn more about why and how to set your node's target neighborhood [here](/docs/bee/installation/set-target-neighborhood).

### Logs and monitoring

Docker provides convenient built-in tools for logging and monitoring your node, which you've already encountered if you've read through earlier sections of this guide. For a more detailed guide, [refer to the section on logging](/docs/bee/working-with-bee/logs-and-files).

**Viewing node logs:**

To monitor your node‚Äôs logs in real-time, use the following command:

```bash
docker logs -f bee-1
```

This command will continuously output the logs of your Bee node, helping you track its operations. The `-f` flag ensures that you see new log entries as they are written. Press `Ctrl + C` to stop following the logs.

You can read more about how Docker manages container logs [in their official docs](https://docs.docker.com/reference/cli/docker/container/logs/).

**Checking the Node's status with the Bee API**

To check your node's status as a staking node, we can use the `/redistributionstate` endpoint:

```bash
curl -s http://localhost:1633/redistributionstate | jq
```

Below is the output for a node which has been running for several days:

```bash
{
  "minimumGasFunds": "11080889201250000",
  "hasSufficientFunds": true,
  "isFrozen": false,
  "isFullySynced": true,
  "phase": "claim",
  "round": 212859,
  "lastWonRound": 207391,
  "lastPlayedRound": 210941,
  "lastFrozenRound": 210942,
  "lastSelectedRound": 212553,
  "lastSampleDuration": 491687776653,
  "block": 32354719,
  "reward": "1804537795127017472",
  "fees": "592679945236926714",
  "isHealthy": true
}
```

For a complete breakdown of this output, check out [this section in the Bee docs](https://docs.ethswarm.org/docs/bee/working-with-bee/bee-api#redistributionstate).

You can read more other important endpoints for monitoring your Bee node in the [official Bee docs](https://docs.ethswarm.org/docs/bee/working-with-bee/bee-api), and you can find complete information about all available endpoints in [the API reference docs](https://docs.ethswarm.org/api/).

**Stopping Your Node**

To gracefully stop your Bee node, use the following command:

```bash
docker stop bee-1
```

Replace `bee-1` with the name of your node if you've given it a different name.

## Back Up Keys

Once your node is up and running, make sure to [back up your keys](/docs/bee/working-with-bee/backups). 

## Getting help

The CLI has documentation built-in. Running `bee` gives you an entry point to the documentation. Running `bee start -h` from within your Docker container or `bee start --help` will tell you how you can configure your Bee node via the command line arguments.

You may also check out the [configuration guide](/docs/bee/working-with-bee/configuration), or simply run your Bee terminal command with the `--help` flag, eg. `bee start --help` or `bee --help`.

## Next Steps to Consider

### Access the Swarm
If you'd like to start uploading or downloading files to Swarm, [start here](/docs/develop/introduction).

### Explore the API
The [Bee API](/docs/bee/working-with-bee/bee-api) is the primary method for interacting with Bee and getting information about Bee. After installing Bee and getting it up and running, it's a good idea to start getting familiar with the API.

### Run a hive! 
If you would like to run a hive of many Bees, check out the [hive operators](/docs/bee/installation/hive) section for information on how to operate and monitor many Bees at once.

### Start building DApps on Swarm
If you would like to start building decentralised applications on Swarm, check out our section for [developing with Bee](/docs/develop/introduction).

---

## Fund Your Node

## Overview
Bee nodes require **xDAI** (for gas fees) and **xBZZ** (for storage and bandwidth) to function properly. The amount needed depends on your node type and use case.

### xDAI is Required For:
- **Buying Postage Stamps** ([Uploading Data](/docs/develop/tools-and-features/buy-a-stamp-batch))
- **Stake Management Transactions** ([Staking](/docs/bee/working-with-bee/staking/))
- **Storage Incentives Transactions** ([Redistribution Game](/docs/concepts/incentives/redistribution-game/) )
- **Chequebook Deployment** ([Bandwidth Payments](/docs/concepts/incentives/bandwidth-incentives/))

### xBZZ is Required For:
- **Buying Postage Stamps** (scales with data size and duration)
- **Staking** (Minimum **10 xBZZ**, **20 xBZZ** for reserve doubling)
- **Bandwidth Payments** (~**0.5 xBZZ per GB downloaded**)

## Token Amounts by Use Case

| **Use Case** | **Node Type** | **xDAI Required** | **xBZZ Required** |
|-------------|--------------|------------------|------------------|
| Free tier downloads | Ultra-Light, Light, Full | None | None |
| Downloading beyond free tier | Light, Full | None |Scales with volume‚Äîstart with ~0.1 xBZZ, increase as needed  |
| Uploading | Light, Full | None | Scales with volume‚Äîstart with ~0.1 xBZZ, increase as needed |
| Purchasing Postage Stamp Batches| Light, Full | < 0.01 xDAI / tx  | Scales with volume & duration. Can start with ~0.2 xBZZ for small uploads. |
| Staking | Full | < 0.01 xDAI / tx | 10 xBZZ (minimum) |
| Storage Incentives Transactions | Full | < 0.01 xDAI / tx - needs topups over time since these are reoccurring transactions | None |
| Bandwidth Payments | Light, Full | None | Scales with bandwidth (~0.5 xBZZ/GB downloaded) |
| Chequebook Deployment | Light, Full | < 0.001 xDAI  | None |

## Getting Tokens

### How to Get xDAI
- **Free xDAI Faucets**: You may try one of the [Gnosis Chain faucets](https://docs.gnosischain.com/tools/Faucets) listed in the official Gnosis Chain documentation, however the amount offered may not meet your needs.
- **Purchasing xDAI**: You can also purchase xDAI from [various exchanges](https://docs.gnosischain.com/about/tokens/xdai) listed in the Gnosis Chain documentation. xDAI is also widely available on most major cryptocurrency exchanges. 
:::warning
Make sure that you are withdrawing the Gnosis Chain version of xDAI, as xDAI has been bridged to several other chains as well.
:::
- **Bridging From Ethereum**: If you already have xDAI on Ethereum, you can also consider using the [Gnosis Chain bridge](https://bridge.gnosischain.com/) to transfer it over to Gnosis Chain.

### How to Get xBZZ
- **Buying xBZZ**: xBZZ can be purchased from a variety of [centralized and decentralized exchanges](https://www.ethswarm.org/get-bzz#how-to-get-bzz) listed on the official Ethswarm.org website.

### Getting Testnet Tokens (Sepolia ETH & sBZZ)
- **Sepolia ETH**: Try [these faucets](https://faucetlink.to/sepolia).
- **sBZZ**: Buy on [Uniswap](https://app.uniswap.org/swap?outputCurrency=0x543dDb01Ba47acB11de34891cD86B675F04840db&inputCurrency=ETH) (ensure **Sepolia testnet** is selected in MetaMask and **Testnet mode** is enabled in the Uniswap web app settings).

## Node Wallet & Chequebook
- **Wallet Creation**: A Gnosis Chain wallet is auto-created when you install Bee.
- **Chequebook Deployment**: A chequebook contract will be automatically deployed when a Bee node is configured to run as a light or full node and has been funded with sufficient xDAI to pay for the chequebook deployment transaction. Required for bandwidth payments.
- **Wallet Access**: Located in `keys/` in Bee's `data-dir` (importable to MetaMask). Also requires a password which is specified through your node's configuration (either passed directly with the `password` option or as a password file specified with the `password-file` option).

## Funding Your Wallet

In order to fund your wallet, first you need to identify your wallet address. The easiest way to do so is to first start your Bee node in ultra-light mode (Bee will start in ultra-light mode when started with the default settings) and then query the Bee API to find your address:

```bash
curl -s localhost:1633/addresses | jq .ethereum
```

```bash
"0x9a73f283cd9212b99b5e263f9a81a0ddc847cd93"
```

Fund your node with the appropriate amount of xDAI and xBZZ based on the recommended amounts specified in [the chart above](/docs/bee/installation/fund-your-node#token-amounts-by-use-case). 

*For support, ask in the [Develop on Swarm](https://discord.com/channels/799027393297514537/811574542069137449) Discord channel.*

---

## Getting Started

*If you want to get a Bee node up and running ASAP, check out the [Quick Start](/docs/bee/installation/quick-start) guide.* 

## Overview

This guide provides the essential background information to help you start running a Bee node, including:

- [Types of Bee nodes and their features](/docs/bee/installation/getting-started#node-types)
- [Choosing the right node type](/docs/bee/installation/getting-started#choosing-a-node-type)
- [Software requirements](/docs/bee/installation/getting-started#software-requirements)
- [Hardware requirements](/docs/bee/installation/getting-started#hardware-requirements)
- [Network requirements](/docs/bee/installation/getting-started#network-requirements)
- [Installation methods](/docs/bee/installation/getting-started#installation-methods)

:::caution[New Bee Users: Read This Guide in Full]
For new Bee users, it is strongly recommended to read through this ***entire guide page*** before proceeding with installation and setup.
:::

## Node Types

Bee nodes can be run in three different modes, ***full***, ***light***, or ***ultra-light***. Full nodes provide complete access to all of Swarm's features including downloads, uploads, full participation in Swarm's incentives systems, and advanced messaging features such as PSS and GSOC. Light nodes are primarily for downloading and uploading only. Ultra-light nodes are the most limited, and only allow users to download a small amount of data with the free-tier limits set by full node operators.

The [Node Types](/docs/bee/working-with-bee/node-types) page provides you with an in-depth look into the features and limitations of each node type along with instructions for how to set node options for all three types. 

## Choosing a Node Type
The node type you need to run will differ depending on your use-case:

| Use Case                     | Recommended Node Type(s) | Details |
|------------------------------|-------------------------|---------|
| **Basic Interaction with Swarm** | Ultra-Light, Light | Ultra-light nodes allow limited free-tier downloads. Light nodes support both uploads and downloads and run efficiently in the background. [Swarm Desktop](https://www.ethswarm.org/build/desktop) provides an easy way to set up either type. |
| **DApp Development**          | Light, Full           | Light nodes are sufficient for many DApp use cases. Full nodes are required for advanced features like GSOC and PSS. |
| **Earning xBZZ & Supporting the Network** | Full | Full nodes are necessary for storage incentives and long-term xBZZ earnings. Running multiple nodes? Consider using [Docker](https://www.docker.com/), [Docker Compose](https://docs.docker.com/compose/), or [Kubernetes](https://kubernetes.io/) for easier management. |

Refer to the [Node Types](/docs/bee/working-with-bee/node-types) page for deep dive into each node type, their features and limitations, and configuration instructions.

##  Software Requirements

### Recommended Operating Systems
- Officially supported systems are listed in the [Bee releases](https://github.com/ethersphere/bee/releases).
- You can [build from source](/docs/bee/installation/build-from-source) if your OS is unsupported.
- **Swarm Desktop users** can use macOS, Windows, or Linux.
- **Linux/macOS recommended**: Most tools and documentation are designed for Unix-based systems.
- **Windows users**: While a Window release of Bee is available, you may also consider using [WSL](https://learn.microsoft.com/en-us/windows/wsl/install) and using a Linux version of Bee.

### Essential Tools

While not strictly required, these tools will *greatly* simplify your experience working with Bee nodes:

- **[`jq`](https://jqlang.github.io/jq/)**: Formats JSON responses (recommended for API users).
- **[`curl`](https://curl.se/)**: Used for sending API requests (required for API interactions).
- **[Swarm CLI](/docs/bee/working-with-bee/swarm-cli/)**: Terminal-based Bee node management.
- **[Bee JS](/docs/develop/tools-and-features/bee-js)**: JavaScript library for programmatic API access.

## Hardware Requirements

### Light and Ultra-Light

Light and ultra-light nodes can be run with practically any commercially available modern computer hardware and internet provider, and have very minimal CPU, RAM and network requirements. 

### Full Node

Requires significant storage and processing power:
- **Processor**: Recent 2 GHz dual-core.
- **RAM**: 8 GB.
- **Storage**: 30 GB SSD (HDD not recommended).
- **Internet**: High-speed and stable connection.

For staking and storage incentives, test node performance with [`/rchash`](https://docs.ethswarm.org/docs/bee/working-with-bee/bee-api/#rchash).

## Network Requirements

A reliable, high-speed internet connection is recommended when running a full node, while ultra-light and light nodes require less bandwidth. The actual amount of bandwidth consumption depends on the node type and use-case:

- **Full Node**: High bandwidth usage due to constant chunk syncing, and even greater utilization if also used for uploads / downloads.
- **Light Node**: Moderate usage, based on data transfer volume.
- **Ultra-Light Node**: Minimal usage, bandwidth utilization restricted based on free-tier download limits.

### RPC Endpoint  

:::warning
***Free public RPC endpoints are discouraged*** since they may enforce rate limiting or may not store the historical smart contract data required by Bee nodes. [Read more](/docs/bee/working-with-bee/configuration#setting-blockchain-rpc-endpoint).
:::

An [RPC (Remote Procedure Call) endpoint](/docs/references/glossary#rpc-endpoint) is required to allow your node to interact with **Gnosis Chain**, which is required for transactions like purchasing postage stamps, staking xBZZ, and storage incentives related transactions.  

Bee nodes use the **`--blockchain-rpc-endpoint`** configuration option to specify which Gnosis Chain RPC service to connect to. 

This can be:  

- A [self-hosted Gnosis Chain node](https://docs.gnosischain.com/node), giving full control over blockchain interactions but requiring additional setup and maintenance (recommended).   
- A **private and paid endpoint** from a third-party service provider.  
- A **public and free endpoint**, such as this free one from the Fair Data Society: `https://xdai.fairdatasociety.org` 

:::info
A well-maintained list of both free and paid RPC endpoint providers can be found in the [Gnosis Chain documentation](https://docs.gnosischain.com/tools/RPC%20Providers/).
:::

Without a properly configured RPC endpoint, a Bee node cannot interact with the blockchain, meaning it will be ***unable*** to:
* Buy postage stamps
* Stake tokens
* Make blockchain transactions

### NAT and Port Forwarding

If running Bee on a home network, there is a good chance it is behind NAT by default. Often simply [enabling port forwarding](https://www.noip.com/support/knowledgebase/general-port-forwarding-guide) will be enough to allow your node to start communicating smoothly with the rest of the network.

See [this page](/docs/bee/installation/connectivity/) for more information on how to make sure your node can communicate with the network.

For VPS/cloud-based setups, connectivity is typically unrestricted.

If your home network happens to be using [CGNAT (Carrier-Grade NAT)](https://en.wikipedia.org/wiki/Carrier-grade_NAT), you may face significant difficulty with setting up your node so it can connect with the rest of the network. Contacting your IP provider may be required. 

## Installation Methods

### [Swarm Desktop](/docs/desktop/introduction)
- Best for beginners.
- GUI-based interface.

### [Shell Script Install](/docs/bee/installation/shell-script-install)
- Quick setup using a minimal script.
- Requires manual configuration for background operation.

### [Docker Install](/docs/bee/installation/docker)
- Suitable for running multiple nodes.
- Offers easy container management.

### [Package Manager Install](/docs/bee/installation/package-manager-install)
- Uses APT, RPM, or Homebrew.
- Runs Bee as a background service.

### [Building from Source](/docs/bee/installation/build-from-source)
- Most flexible, but requires advanced setup.

---

## Hive

Due to the mechanics of Swarm's [storage incentives](/docs/concepts/incentives/redistribution-game), node operators may wish to run multiple nodes in order to maximize earning potential. Read [The Book of Swarm](https://www.ethswarm.org/the-book-of-swarm-2.pdf) for more information on how the
swarm comes together.

### Docker

Up-to-date [Docker images for Bee](/docs/bee/installation/docker) are provided.

### Docker Compose

Running multiple Bee nodes is easier with
`docker-compose`. Check out the Docker compose section of the
[Docker README](https://github.com/ethersphere/bee/tree/master/packaging/docker).

### Helm

If you plan to run a large number of Bee nodes and you have experience using Kubernetes with Helm, you can have a look at how we manage our cluster under [Ethersphere/helm](https://github.com/ethersphere/helm/tree/master/charts/bee).

### Manual Setup

If you just want to run a handful of Bee nodes, you can run multiple Bee nodes by creating separate configuration files.

Create your first configuration file by running

```console
bee printconfig &> bee-config-1.yaml
```

Make as many copies of bee-config-1.yaml as you want to run Bee nodes. Increment the number in the name (`bee-config-1` to `bee-config-2`) for each new configuration file.

Configure your nodes as desired, but ensure that the values `api-addr`, `data-dir` and `p2p-addr` are unique for each configuration.

### Monitoring

See the [logging section](/docs/bee/working-with-bee/logs-and-files) for more information on how to access your node's metrics. Share your community creations (such as [swarmMonitor](https://github.com/doristeo/SwarmMonitoring) - thanks doristeo!) in the [#node-operators](https://discord.gg/X3ph5yGRFU) channel of our Discord server so we can add you to our list of all things that are [awesome](https://github.com/ethersphere/awesome-swarm) and Swarm. üß°

---

## Package Manager Install

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The Bee client can be [installed through a variety of package managers,](/docs/bee/installation/package-manager-install) including [APT](https://en.wikipedia.org/wiki/APT_(software)), [RPM](https://en.wikipedia.org/wiki/RPM_Package_Manager), and [Homebrew](https://en.wikipedia.org/wiki/Homebrew_(package_manager)). 

:::caution
  When installed using a package manager, Bee is set up so it can be started to run as a background service using `systemctl` or `brew services` (depending on the package manager used). 
  
  However, Bee node installed via a package manager can also be started using the standard `bee start` command.

  When a node is started using the `bee start` command the node process will be bound to the terminal session and will exit if the terminal is closed. 
  
  Furthermore, depending on which of these startup methods was used, [*the default Bee directories will be different*](/docs/bee/working-with-bee/configuration#default-data-and-config-directories). For each startup method, a different default data directory is used, so each startup method will essentially be spinning up a totally different node.
:::

## Install Bee

Bee is available for Linux in .rpm and .deb package format for a variety of system architectures, and is available for MacOS through Homebrew. See the [releases](https://github.com/ethersphere/bee/releases) page of the Bee repo for all available packages. One of the advantages of this method is that it automatically configures Bee to run as a background service during installation.

<Tabs
defaultValue="debian"
values={[
{label: 'Debian', value: 'debian'},
{label: 'RPM', value: 'rpm'},
{label: 'MacOS', value: 'macos'},
]}>

<TabItem value="debian">

Get GPG key:

```bash
curl -fsSL https://repo.ethswarm.org/apt/gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/ethersphere-apt-keyring.gpg
```

Set up repo inside apt-get sources:

```bash
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ethersphere-apt-keyring.gpg] https://repo.ethswarm.org/apt \
  * *" | sudo tee /etc/apt/sources.list.d/ethersphere.list > /dev/null
```

Install package:

```bash
sudo apt-get update
sudo apt-get install bee
```

</TabItem>

<TabItem value="rpm">

Set up repo:

```bash
echo "[ethersphere]
name=Ethersphere Repo
baseurl=https://repo.ethswarm.org/yum/
enabled=1
gpgcheck=0" | sudo tee /etc/yum.repos.d/ethersphere.repo
```
Install package:

```bash
yum install bee
```
</TabItem>
<TabItem value="macos">

```bash
brew tap ethersphere/tap
brew install swarm-bee
```

</TabItem>
</Tabs>

You should see the following output to your terminal after a successful install (your default 'Config' location will vary depending on your operating system):

```bash
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following NEW packages will be installed:
  bee
0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.
Need to get 0 B/27.2 MB of archives.
After this operation, 50.8 MB of additional disk space will be used.
Selecting previously unselected package bee.
(Reading database ... 82381 files and directories currently installed.)
Preparing to unpack .../archives/bee_2.3.0_amd64.deb ...
Unpacking bee (2.3.0) ...
Setting up bee (2.3.0) ...

Logs:   journalctl -f -u bee.service
Config: /etc/bee/bee.yaml

Bee requires a Gnosis Chain RPC endpoint to function. By default this is expected to be found at ws://localhost:8546.

Please see https://docs.ethswarm.org/docs/installation/install for more details on how to configure your node.

After you finish configuration run 'sudo bee-get-addr' and fund your node with XDAI, and also XBZZ if so desired.

Created symlink /etc/systemd/system/multi-user.target.wants/bee.service ‚Üí /lib/systemd/system/bee.service.
```

## Configure Bee

When Bee is installed using a package manager, a `bee.yaml` file containing the default configuration will be generated. 

:::info
While this package manager install guide uses the `bee.yaml` file for setting configuration options, there are  [several other available methods for setting node options](/docs/bee/working-with-bee/configuration).
:::

After installation, you can check that the file was successfully generated and contains the [default configuration](https://github.com/ethersphere/bee/blob/master/packaging) for your system:

<Tabs
defaultValue="linux"
values={[
{label: 'Linux', value: 'linux'},
{label: 'macOS arm64 (Apple Silicon)', value: 'macos-arm64'},
{label: 'macOS amd64 (Intel)', value: 'macos-amd64'},
]}>
<TabItem value="linux">

```bash
  test -f /etc/bee/bee.yaml && echo "bee.yaml exists."
  cat /etc/bee/bee.yaml
```

</TabItem>

<TabItem value="macos-arm64">

```bash
  test -f /opt/homebrew/etc/swarm-bee/bee.yaml && echo "$FILE exists."
  cat /opt/homebrew/etc/swarm-bee/bee.yaml
```

</TabItem>

<TabItem value="macos-amd64">

```bash
  test -f /usr/local/etc/swarm-bee/bee.yaml && echo "$FILE exists."
  cat /usr/local/etc/swarm-bee/bee.yaml
```
</TabItem>
</Tabs>

The configuration printed to the terminal should match the default configuration for your operating system. See the [the packaging section of the Bee repo](https://github.com/ethersphere/bee/tree/master/packaging) for the default configurations for a variety of systems. In particular, pay attention to the `config` and `data-dir` values, as these differ depending on your system. 

If your config file is missing you will need to create it yourself.

<Tabs
defaultValue="linux"
values={[
{label: 'Linux', value: 'linux'},
{label: 'MacOS arm64 (Apple Silicon)', value: 'macos-arm64'},
{label: 'MacOS amd64 (Intel)', value: 'macos-amd64'},
]}>
<TabItem value="linux">

Create the `bee.yaml` config file and save it with [the default configuration](https://github.com/ethersphere/bee/blob/master/packaging/bee.yaml).

```bash
sudo touch /etc/bee/bee.yaml
sudo vi /etc/bee/bee.yaml
```

</TabItem>

<TabItem value="macos-arm64">

Create the `bee.yaml` config file and save it with the [the default configuration](https://github.com/ethersphere/bee/blob/master/packaging/homebrew-arm64/bee.yaml).

```bash
sudo touch /opt/homebrew/etc/swarm-bee/bee.yaml
sudo sudo vi /opt/homebrew/etc/swarm-bee/bee.yaml
```

</TabItem>

<TabItem value="macos-amd64">

Create the `bee.yaml` config file and save it with the [the default configuration](https://github.com/ethersphere/bee/blob/master/packaging/homebrew-amd64/bee.yaml).

```bash
sudo touch /usr/local/etc/swarm-bee/bee.yaml
sudo vi /usr/local/etc/swarm-bee/bee.yaml
```
</TabItem>

</Tabs>

### Set Node Type

See the [Getting Started guide](/docs/bee/installation/getting-started#choosing-a-node-type) if you're not sure which type of node to run.
 
Once you've decided which node type is appropriate for you, refer to the [configuration section](/docs/bee/working-with-bee/configuration#node-types) for instructions on setting the options for your preferred node type.

### Set Target Neighborhood

When installing your Bee node it will automatically be assigned a neighborhood. However, when running a full node with staking there are benefits to periodically updating your node's neighborhood. Learn more about why and how to set your node's target neighborhood [here](/docs/bee/installation/set-target-neighborhood).

## Start Node

Use the appropriate command for your system to start your node:

<Tabs
defaultValue="linux"
values={[
{label: 'Linux', value: 'linux'},
{label: 'MacOS', value: 'macos'},
]}>
<TabItem value="linux">

```bash
sudo systemctl start bee
```

</TabItem>

<TabItem value="macos">

```bash
brew services start swarm-bee
```

</TabItem>
</Tabs>

```bash
Welcome to Swarm.... Bzzz Bzzzz Bzzzz
                \     /
            \    o ^ o    /
              \ (     ) /
   ____________(%%%%%%%)____________
  (     /   /  )%%%%%%%(  \   \     )
  (___/___/__/           \__\___\___)
     (     /  /(%%%%%%%)\  \     )
      (__/___/ (%%%%%%%) \___\__)
              /(       )\
            /   (%%%%%)   \
                 (%%%)
                   !

DISCLAIMER:
This software is provided to you "as is", use at your own risk and without warranties of any kind.
It is your responsibility to read and understand how Swarm works and the implications of running this software.
The usage of Bee involves various risks, including, but not limited to:
damage to hardware or loss of funds associated with the Ethereum account connected to your node.
No developers or entity involved will be liable for any claims and damages associated with your use,
inability to use, or your interaction with other nodes or the software.

version: 2.2.0-06a0aca7 - planned to be supported until 11 December 2024, please follow https://ethswarm.org/

"time"="2024-09-24 18:15:34.383102" "level"="info" "logger"="node" "msg"="bee version" "version"="2.2.0-06a0aca7"
"time"="2024-09-24 18:15:34.428546" "level"="info" "logger"="node" "msg"="swarm public key" "public_key"="0373fe2ab33ab836635fc35864cf708fa0f4a775c0cf76ca851551e7787b58d040"
"time"="2024-09-24 18:15:34.520686" "level"="info" "logger"="node" "msg"="pss public key" "public_key"="03a341032724f1f9bb04f1d9b22607db485cccd74174331c701f3a6957d94d95c1"
"time"="2024-09-24 18:15:34.520716" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
"time"="2024-09-24 18:15:34.533789" "level"="info" "logger"="node" "msg"="fetching target neighborhood from suggester" "url"="https://api.swarmscan.io/v1/network/neighborhoods/suggestion"
"time"="2024-09-24 18:15:36.773501" "level"="info" "logger"="node" "msg"="mining a new overlay address to target the selected neighborhood" "target"="00100010110"
"time"="2024-09-24 18:15:36.776550" "level"="info" "logger"="node" "msg"="using overlay address" "address"="22d502d022de0f8e9d477bc61144d0d842d9d82b8241568c6fe4e41f0b466615"
"time"="2024-09-24 18:15:36.776576" "level"="info" "logger"="node" "msg"="starting with an enabled chain backend"
"time"="2024-09-24 18:15:37.388997" "level"="info" "logger"="node" "msg"="connected to blockchain backend" "version"="erigon/2.60.7/linux-amd64/go1.21.5"
"time"="2024-09-24 18:15:37.577840" "level"="info" "logger"="node" "msg"="using chain with network network" "chain_id"=100 "network_id"=1
"time"="2024-09-24 18:15:37.593747" "level"="info" "logger"="node" "msg"="starting debug & api server" "address"="127.0.0.1:1633"
"time"="2024-09-24 18:15:37.969782" "level"="info" "logger"="node" "msg"="using default factory address" "chain_id"=100 "factory_address"="0xC2d5A532cf69AA9A1378737D8ccDEF884B6E7420"
"time"="2024-09-24 18:15:38.160249" "level"="info" "logger"="node/chequebook" "msg"="no chequebook found, deploying new one."
"time"="2024-09-24 18:15:38.728534" "level"="warning" "logger"="node/chequebook" "msg"="cannot continue until there is at least min xDAI (for Gas) available on address" "min_amount"="0.0003750000017" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
```

Take note of the lines:

```bash
"time"="2024-09-24 18:15:34.520716" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
```

and

```bash
"time"="2024-09-24 18:15:38.728534" "level"="warning" "logger"="node/chequebook" "msg"="cannot continue until there is at least min xDAI (for Gas) available on address" "min_amount"="0.0003750000017" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
```

The address referred to in both of these lines is your node's Gnosis Chain address. The second one indicates that the address does not have enough xDAI in order to deploy your node's chequebook contract which is used to pay for bandwidth incentives. You will see this warning if you have configured your node to run as a `full` or `light` node, but it should be absent for `ultra-light` nodes. 

## Fund Node

Depending on your chosen node type, (full, light, or ultra-light), you will want to fund your node with differing amounts of xBZZ and xDAI. See [this section](/docs/bee/installation/fund-your-node) for more information on how to fund your node. 

### Restart and Wait for Initialisation

After funding your node, use the appropriate command for your system below and wait for it to initialize:

<Tabs
defaultValue="linux"
values={[
{label: 'Linux', value: 'linux'},
{label: 'MacOS', value: 'macos'},
]}>
<TabItem value="linux">

```bash
sudo systemctl start bee
```

</TabItem>

<TabItem value="macos">

```bash
brew services start swarm-bee
```

</TabItem>
</Tabs>

When first started in full or light mode, Bee must deploy a chequebook to the Gnosis Chain blockchain, and sync the postage stamp batch store so that it can check chunks for validity when storing or forwarding them. This can take a while, so please be patient! Once this is complete, you will see Bee starting to add peers and connect to the network.

You can keep an eye on progress by watching the logs while this is taking place.

<Tabs
defaultValue="linux"
values={[
{label: 'Linux', value: 'linux'},
{label: 'MacOS arm64 (Apple Silicon)', value: 'macos-arm64'},
{label: 'MacOS amd64 (Intel)', value: 'macos-amd64'},
]}>
<TabItem value="linux">

```bash
sudo journalctl --lines=100 --follow --unit bee
```

</TabItem>
<TabItem value="macos-arm64">

```bash
tail -f /opt/homebrew/var/log/swarm-bee/bee.log
```

</TabItem>

<TabItem value="macos-amd64">

```bash
tail -f /usr/local/var/log/swarm-bee/bee.log
```
</TabItem>

</Tabs>

*If you've started your node with `bee start`, simply observe the logs printed to your terminal.*

If all goes well, you will see your node automatically begin to connect to other Bee nodes all over the world.

```
INFO[2020-08-29T11:55:16Z] greeting <Hi I am a very buzzy bee bzzzz bzzz bzz. üêù> from peer: b6ae5b22d4dc93ce5ee46a9799ef5975d436eb63a4b085bfc104fcdcbda3b82c
```

Now your node will begin to request chunks of data that fall within your *radius of responsibilty*‚Äîdata that you will then serve to other p2p clients running in the swarm. Your node will then begin to
respond to requests for these chunks from other peers.

:::tip Incentivisation
In Swarm, storing, serving and forwarding chunks of data to other nodes can earn you rewards! Follow [this guide](/docs/bee/working-with-bee/cashing-out) to learn how to regularly cash out cheques other nodes send you in return for your services so that you can get your xBZZ!
:::

Your Bee client has now generated an elliptic curve key pair similar to an Ethereum wallet. These are stored in your [data directory](/docs/bee/working-with-bee/configuration), in the `keys` folder.

:::danger Keep Your Keys and Password Safe!
Your keys and password are very important, back up these files and
store them in a secure place that only you have access to. With great
privacy comes great responsibility - while no-one will ever be able to
guess your key - you will not be able to recover them if you lose them
either, so be sure to look after them well and [keep secure
backups](/docs/bee/working-with-bee/backups).
:::

## Check if Bee is Working

First check that the correct version of Bee is installed:

```bash
bee version
```

```
2.3.0
```

Once the Bee node has been funded, the chequebook deployed, and postage stamp
batch store synced, its HTTP [API](/docs/bee/working-with-bee/bee-api)
will start listening at `localhost:1633` (for `full` or `light` nodes - for an `ultra-light` node, it should be initialized and the API should be available more rapidly).

To check everything is working as expected, send a GET request to localhost port 1633.

```bash
curl localhost:1633
```

```
Ethereum Swarm Bee
```

Success! The Bee API is now listening!

Next, let's see if we have connected with any peers by sending a query to the Bee API (port 1633 by default - `localhost:1633`).

:::info
Here we are using the `jq` [utility](https://stedolan.github.io/jq/) to parse our javascript. Use your package manager to install `jq`, or simply remove everything after and including the first `|` to view the raw json without it.
:::

```bash
curl -s localhost:1633/peers | jq ".peers | length"
```

```
87
```

Perfect! We are accumulating peers, this means you are connected to
the network, and ready to start [using
Bee](/docs/develop/introduction) to [upload and
download](/docs/develop/upload-and-download) content or host
and browse [websites](/docs/develop/host-your-website) hosted
on the Swarm network.

Welcome to the swarm! üêù¬†üêù¬†üêù¬†üêù¬†üêù

## Back Up Keys

Once your node is up and running, make sure to [back up your keys](/docs/bee/working-with-bee/backups). 

## Deposit Stake (Optional)

While depositing stake is not required to run a Bee node, it is required in order for a node to receive rewards for sharing storage with the network. You will need to [deposit xBZZ to the staking contract](/docs/bee/working-with-bee/staking) for your node. To do this, send a minimum of 10 xBZZ to your nodes' wallet and run:

```bash
curl -X POST localhost:1633/stake/100000000000000000
```

This will initiate a transaction on-chain which deposits the specified amount of xBZZ into the staking contract. 

Storage incentive rewards are only available for full nodes which are providing storage capacity to the network.

*Note that SWAP rewards (bandwidth incentives paid for forwarding chunks) are available to all **full** nodes, regardless of whether or not they stake xBZZ in order to participate in the storage incentives system.*

## Next Steps to Consider

### Access the Swarm
If you'd like to start uploading or downloading files to Swarm, [start here](/docs/develop/introduction).

### Explore the API
The [Bee API](/docs/bee/working-with-bee/bee-api) is the primary method for interacting with Bee and getting information about Bee. After installing Bee and getting it up and running, it's a good idea to start getting familiar with the API.

### Run a hive! 
If you would like to run a hive of many Bees, check out the [hive operators](/docs/bee/installation/hive) section for information on how to operate and monitor many Bees at once.

### Start building DAPPs on Swarm
If you would like to start building decentralised applications on Swarm, check out our section for [developing with Bee](/docs/develop/introduction).

---

## Quickstart

This guide will help you install and run a Bee [light node](/docs/bee/working-with-bee/node-types) using the [shell script](/docs/bee/installation/shell-script-install) install method. After explaining how to install and start the node, the guide then explains how to use the [`swarm-cli` command line tool](/docs/bee/working-with-bee/swarm-cli) to find your node's address, fund your node, and fully initialize it so that it is ready interact with the network.

:::tip
A "light" node can download and upload data from Swarm but does not share its disk space with the network and does not earn rewards. [Learn more](/docs/bee/working-with-bee/node-types). 
:::

## Requirements  

- **Linux or macOS** (The shell script installation method does **not** support Windows natively. Windows users can use [WSL](https://learn.microsoft.com/en-us/windows/wsl/install).)  
- [Node.js (v18 or higher)](https://nodejs.org/)
- [npm (Node Package Manager)](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)  
- [`curl`](https://curl.se/) or [`wget`](https://www.gnu.org/software/wget/) (Check with `curl --version` or `wget --version`)
- [~0.20 xBZZ on Gnosis Chain](/docs/bee/installation/fund-your-node#how-to-get-xbzz)
- [~0.01 xDAI on Gnosis Chain](/docs/bee/installation/fund-your-node#how-to-get-xdai)

:::info
Although `BZZ` is the official symbol of the token on both Ethereum and Gnosis Chain, the term `xBZZ` is widely used by the Swarm community and in documentation to indicate that it is BZZ on Gnosis Chain (not Ethereum).
:::

## Install Bee

Run the shell script using `curl` or `wget`:

:::tip
We specify `TAG=v2.6.0` to indicate which Bee version to install. You can find available versions in the ["releases" section](https://github.com/ethersphere/bee/releases) of the Bee GitHub repo.
:::

```bash
curl -s https://raw.githubusercontent.com/ethersphere/bee/master/install.sh | TAG=v2.6.0 bash
```

OR

```bash
wget -q -O - https://raw.githubusercontent.com/ethersphere/bee/master/install.sh | TAG=v2.6.0 bash
```

Verify installation:

```bash
bee version
```

## Install Swarm-CLI

Requires **Node.js 18+**. Install using **npm**:

```bash
npm install --global @ethersphere/swarm-cli
```

Verify installation:

```bash
swarm-cli --version
```

## Start Your Bee Node

Start Bee with a secure password:

```bash
bee start \
  --password YOUR_SECURE_PASSWORD \
  --verbosity 5 \
  --swap-enable \
  --api-addr 127.0.0.1:1633 \
  --blockchain-rpc-endpoint https://xdai.fairdatasociety.org
```

Example output:

```bash
Welcome to Swarm.... Bzzz Bzzzz Bzzzz

"Share the knowledge" - in memory of ldeffenb

                \     /
            \    o ^ o    /
              \ (     ) /
   ____________(%%%%%%%)____________
  (     /   /  )%%%%%%%(  \   \     )
  (___/___/__/           \__\___\___)
     (     /  /(%%%%%%%)\  \     )
      (__/___/ (%%%%%%%) \___\__)
              /(       )\
            /   (%%%%%)   \
                 (%%%)
                   !

DISCLAIMER:
This software is provided to you "as is", use at your own risk and without warranties of any kind.
It is your responsibility to read and understand how Swarm works and the implications of running this software.
The usage of Bee involves various risks, including, but not limited to:
damage to hardware or loss of funds associated with the Ethereum account connected to your node.
No developers or entity involved will be liable for any claims and damages associated with your use,
inability to use, or your interaction with other nodes or the software.

version: 2.6.0-390a402e - planned to be supported until 22 April 2025, please follow https://ethswarm.org/

"time"="2025-03-04 11:13:10.113050" "level"="info" "logger"="node" "msg"="bee version" "version"="2.6.0-390a402e"
"time"="2025-03-04 11:13:10.164801" "level"="info" "logger"="node" "msg"="swarm public key" "public_key"="02b19880b8d024eac3bf8afa3fa85b31b72fcfd491cebc6af78ddd85ff97f65416"
"time"="2025-03-04 11:13:10.216657" "level"="debug" "logger"="node" "msg"="using existing libp2p key"
"time"="2025-03-04 11:13:10.268431" "level"="debug" "logger"="node" "msg"="using existing pss key"
"time"="2025-03-04 11:13:10.268474" "level"="info" "logger"="node" "msg"="pss public key" "public_key"="03a3166e04b749ab3d04fda8a41180598ff2eed01a8096fb72d2c7da393a47c46a"
"time"="2025-03-04 11:13:10.268479" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x003842B26B3dB292Cf84d5969E71c0d1e93F5578"
"time"="2025-03-04 11:13:10.288418" "level"="info" "logger"="node" "msg"="using overlay address" "address"="fe38346dd89e4211c0e60195ee73e38d2c2ee2fe2b914b771d4ad503cfedbd3c"
"time"="2025-03-04 11:13:10.288474" "level"="info" "logger"="node" "msg"="starting with an enabled chain backend"
"time"="2025-03-04 11:13:10.987200" "level"="info" "logger"="node" "msg"="connected to blockchain backend" "version"="Nethermind/v1.30.3+87c86379/linux-x64/dotnet9.0.0"
"time"="2025-03-04 11:13:11.196067" "level"="info" "logger"="node" "msg"="using chain with network network" "chain_id"=100 "network_id"=1
"time"="2025-03-04 11:13:11.211976" "level"="info" "logger"="node" "msg"="starting debug & api server" "address"="127.0.0.1:1633"
"time"="2025-03-04 11:13:11.623998" "level"="info" "logger"="node" "msg"="using default factory address" "chain_id"=100 "factory_address"="0xC2d5A532cf69AA9A1378737D8ccDEF884B6E7420"
"time"="2025-03-04 11:13:11.675186" "level"="info" "logger"="node/chequebook" "msg"="no chequebook found, deploying new one."
"time"="2025-03-04 11:13:11.723451" "level"="warning" "logger"="node/chequebook" "msg"="cannot continue until there is at least min xDAI (for Gas) available on address" "min_amount"="0.000250000002" "address"="0x003842B26B3dB292Cf84d5969E71c0d1e93F5578"
```

üéâ Congratulations! You've just successfully installed and started your first Bee node üêù!

## Get Your Node‚Äôs Address

The final line of the logs in the previous step lets us know that we need to fund our node to continue, and shows our node's Gnosis chain address. Copy the address and save it for the next step:

```bash
"time"="2025-03-04 11:13:11.723451" "level"="warning" "logger"="node/chequebook" "msg"="cannot continue until there is at least min xDAI (for Gas) available on address" "min_amount"="0.000250000002" "address"="0x003842B26B3dB292Cf84d5969E71c0d1e93F5578"
```

You can also view your node's addresses any time using the `swarm-cli addresses` command:

```bash
swarm-cli addresses
```

Example output:

```bash
Node Addresses
-----------------------------------------------------------------------------------------------------------------------------------
Ethereum: 0x003842b26b3db292cf84d5969e71c0d1e93f5578
Overlay: fe38346dd89e4211c0e60195ee73e38d2c2ee2fe2b914b771d4ad503cfedbd3c
PSS Public Key: 03a3166e04b749ab3d04fda8a41180598ff2eed01a8096fb72d2c7da393a47c46a
Public Key: 02b19880b8d024eac3bf8afa3fa85b31b72fcfd491cebc6af78ddd85ff97f65416
Underlay: /ip4/127.0.0.1/tcp/1634/p2p/QmPbXzjN9mzYnpxsMn6ftFvvUuf4VArcmR6oGtpf1mRgWt /ip4/172.25.128.69/tcp/1634/p2p/QmPbXzjN9mzYnpxsMn6ftFvvUuf4VArcmR6oGtpf1mRgWt /ip6/::1/tcp/1634/p2p/QmPbXzjN9mzYnpxsMn6ftFvvUuf4VArcmR6oGtpf1mRgWt
```

## Fund Your Node

Send **xDAI** (to pay for transaction fees on Gnosis Chain) and **xBZZ** (for uploads and staking) to your node‚Äôs Ethereum address on **Gnosis Chain**.

- **xDAI:** 0.01 xDAI is enough to start a light node
- **xBZZ:** 0.20 xBZZ is enough to upload a small amount of data

Learn how to [get xDAI and xBZZ](/docs/bee/installation/fund-your-node#getting-tokens) if you need some.

:::tip
If you wait too long to fund your node it may shut itself down. In that case, simply use the same startup command to start the node again.
:::

## Wait to Sync (~5 Minutes)

After starting and funding a Bee light node for the first time, the node will automatically issue a [transaction](https://gnosisscan.io/tx/0xf8048c4e8020ccef842c9a901e6262e9c06d6f5926ff31bdb7dd9d7274dcf19c) on Gnosis Chain to deploy the node's [chequebook contract](/docs/concepts/incentives/bandwidth-incentives#chequebook-contract).

The node then needs to sync blockchain data before it can buy a postage stamp batch. The process may take **~5 minutes** depending on your RPC provider and network speed. 

You can check your node's syncing progress with the `swarm-cli status` command:

```bash
swarm-cli status
```

```bash
Bee
API: http://localhost:1633 [OK]
Version: 2.6.0-d0aa8b93
Mode: light

Chainsync
Block: 39,566,742 / 41,710,807 (Œî 2,144,065)

Topology
ERROR Request failed with status code 503

There may be additional information in the Bee logs.
```

The `Chainsync` section tells us how many blocks our node has synced so far out of the total Gnosis Chain blocks (and the number after the Œî symbol shows how many blocks still need to be synced):

```bash
Chainsync
Block: 33,515,656 / 38,855,407 (Œî 5,339,751)
```

The `Topology` section will show information about which other nodes your own node is connected with. It will display an `ERROR` until the node is fully initialized.

After several minutes, your node will be fully synced, and can now interact with the Swarm network - we can use `swarm-cli status` again to confirm:

```bash
swarm-cli status
```

```bash
Bee
API: http://localhost:1633 [OK]
Version: 2.6.0-d0aa8b93
Mode: light

Chainsync
Block: 41,710,955 / 41,710,962 (Œî 7)

Topology
Connected Peers: 151
Population: 2257
Depth: 10

Wallet
xBZZ: 0.0000000000000000
xDAI: 0.009787142484816165

Chequebook
Available xBZZ: 0.0000000000000000
Total xBZZ: 0.0000000000000000
noah@NoahM16:~$ swarm-cli status
Bee
API: http://localhost:1633 [OK]
Version: 2.6.0-d0aa8b93
Mode: light

Chainsync
Block: 41,711,260 / 41,711,268 (Œî 8)

Topology
Connected Peers: 156
Population: 2693
Depth: 10

Wallet
xBZZ: 0.0000000000000000
xDAI: 0.009787142484816165

Chequebook
Available xBZZ: 0.0000000000000000
Total xBZZ: 0.0000000000000000
```

## Next Steps

With your node now fully synced, you're ready start start learning how to [develop on Swarm](/docs/develop/introduction).

---

## Set Target Neighborhood

In older versions of Bee, [neighborhood](/docs/concepts/DISC/neighborhoods) assignment was random by default. However, we can maximize a node's chances of winning xBZZ and also strengthen the resiliency of the network by strategically assigning neighborhoods to new nodes (see the [staking section](/docs/bee/working-with-bee/staking) for more details).

Therefore the default Bee configuration now includes the `neighborhood-suggester` option, which is set by default to use the Swarmscan neighborhood suggester (`https://api.swarmscan.io/v1/network/neighborhoods/suggestion`). You can use an alternative suggester URL, but it must return a JSON response in the following format: `{"neighborhood":"101000110101"}`. However, we currently recommend using only the default suggester. 

:::info
The Swarmscan neighborhood selector prioritizes the least populated neighborhood. If a neighborhood contains imbalanced sub-neighborhoods, it will suggest the least populated sub-neighborhood instead. Furthermore, the suggester will temporarily de-prioritize previously suggested neighborhoods based on the assumption that a new node is being created in each suggested neighborhood so that multiple nodes do not simultaneously attempt to join the same neighborhood.
:::

#### Setting Neighborhood Manually

It's recommended to use the default `neighborhood-suggester` configuration for choosing your node's neighborhood, however you may also set your node's neighborhood manually using the `target-neighborhood` option.

To use this option, it's first necessary to identify potential target neighborhoods. You can find underpopulated neighborhoods using the [Swarmscan website](https://swarmscan.io/neighborhoods). It ranks neighborhoods from least to most populated and displays their leading binary bits. Simply copy the leading bits from one of the least populated neighborhoods (for example, `0010100001`) and use it to set `target-neighborhood`. After doing so, an overlay address within that neighborhood will be generated when starting Bee for the first time.

```yaml
## bee.yaml
target-neighborhood: "0010100001"
```

You can also use the [Swarmscan API endpoint](https://api.swarmscan.io/#tag/Network/paths/~1v1~1network~1neighborhoods~1suggestion/get) to programmatically retrieve a suggested neighborhood:

```bash
curl https://api.swarmscan.io/v1/network/neighborhoods/suggestion
```
A suggested neighborhood will be returned:

```bash
{"neighborhood":"1111110101"}
```

---

## Shell Script Install

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Swarm Shell Script Installation Guide

The official [shell script](https://github.com/ethersphere/bee/blob/master/install.sh) provided by Swarm automatically detects your system and installs the correct version of Bee. This installation method is an excellent choice if you're looking for a minimalistic and flexible option for your Bee node installation.

:::warning
Note that we append 127.0.0.1 (localhost) to our Bee API's port (1633 by default), since we do not want to expose our Bee API endpoint to the public internet, as that would allow anyone to control our node. Make sure you do the same. Additionally, it's recommended to use a firewall to restrict access to your node(s).
:::

:::info
This guide uses command line flag options in the node startup commands such as `--blockchain-rpc-endpoint`, however, there are [several other methods available for configuring options](/docs/bee/working-with-bee/configuration). 
:::

## Install and Start Your Node 
Below is a step-by-step guide for installing and setting up your Bee node using the shell script installation method.

### Run Shell Script 

Run the install shell script using either `curl` or `wget`:

:::caution
In the example below, the version is specified using `TAG=v2.6.0`. Check the [latest Bee releases](https://github.com/ethersphere/bee/tags) and if needed, update the command to install the most recent version (note that in tags containing "rc," the abbreviation stands for "release candidate", and these versions should be used for testing purposes only). 
:::

:::info
Note that while this shell script supports many commonly used Unix-like systems, it is not quite a universal installer tool. The architectures it supports include:

**1. Linux:**  
- `linux-386` (32-bit x86)  
- `linux-amd64` (64-bit x86)  
- `linux-arm64` (64-bit ARM)  
- `linux-armv6` (32-bit ARM v6)  

**2. macOS (Darwin):**  
- `darwin-arm64` (Apple Silicon, M1/M2/M3)  
- `darwin-amd64` (Intel-based Mac)  

This means the script works on most modern Linux distributions and macOS versions that match these architectures. Windows users can use [WSL](https://learn.microsoft.com/en-us/windows/wsl/install).
:::

:::caution
You may need to install [`curl`](https://curl.se/) or [`wget`](https://www.gnu.org/software/wget/) if your system doesn't have one of them pre-installed and the shell script command fails to run.
:::

<Tabs
defaultValue="curl"
values={[
{label: 'Curl', value: 'curl'},
{label: 'Wget', value: 'wget'},
]}>

<TabItem value="curl">

```bash
curl -s https://raw.githubusercontent.com/ethersphere/bee/master/install.sh | TAG=v2.6.0 bash
```
</TabItem>
<TabItem value="wget">

**wget**

```bash
wget -q -O - https://raw.githubusercontent.com/ethersphere/bee/master/install.sh | TAG=v2.6.0 bash
```
</TabItem>

</Tabs>
 
Let's check that the script ran properly:

```bash=
bee 
```

If the script ran without any problems you should see this:

```bash=
Ethereum Swarm Bee

Usage:
  bee [command]

Available Commands:
  start       Start a Swarm node
  dev         Start a Swarm node in development mode
  init        Initialise a Swarm node
  deploy      Deploy and fund the chequebook contract
  version     Print version number
  db          Perform basic DB related operations
  split       Split a file into chunks
  printconfig Print default or provided configuration in yaml format
  help        Help about any command
  completion  Generate the autocompletion script for the specified shell

Flags:
      --config string   config file (default is $HOME/.bee.yaml)
  -h, --help            help for bee

Use "bee [command] --help" for more information about a command.
```

### Node Startup Commands
 
Let's try starting up our node for the first time with the command below. Make sure to pick a [strong password](https://xkcd.com/936/) of your own:

Below are startup commands configured for each of the three Bee node types, ***full***, ***light***, and ***ultra-light***. Refer to the [Node Types](/docs/bee/working-with-bee/node-types) page to learn more about each node type and decide which one best suits your needs.

<Tabs
defaultValue="full"
values={[
{label: 'Full', value: 'full'},
{label: 'Light', value: 'light'},
{label: 'Ultra Light', value: 'ultra-light'},
]}>

<TabItem value="full">
For the full node, we have `--full-node` and `--swap-enable` both enabled, and we've used `--blockchain-rpc-endpoint` to set our RPC endpoint as `https://xdai.fairdatasociety.org`. Your RPC endpoint may differ depending on your setup.

```bash
bee start \
  --password flummoxedgranitecarrot \
  --full-node \
  --swap-enable \
  --api-addr 127.0.0.1:1633 \
  --blockchain-rpc-endpoint https://xdai.fairdatasociety.org
```
</TabItem>
<TabItem value="light">
For the light node, we omit `--full-node`, keeping the rest the same as the full node setup.

```bash
bee start \
  --password flummoxedgranitecarrot \
  --swap-enable \
  --api-addr 127.0.0.1:1633 \
  --blockchain-rpc-endpoint https://xdai.fairdatasociety.org
```
</TabItem>
<TabItem value="ultra-light">
 For the ultra-light node, we omit all three of the relevant settings to disable them (since they default to `false`), `--full-node`, `--swap-enable`, and `--blockchain-rpc-endpoint`.

```bash
bee start \
  --password flummoxedgranitecarrot \
  --api-addr 127.0.0.1:1633 
```
</TabItem>

</Tabs>

:::info

Command explained:

1. **`bee start`**: This is the command to start the Bee node.

2. **`--password flummoxedgranitecarrot`**: The password to decrypt the private key associated with the node. Replace "flummoxedgranitecarrot" with your actual password.

3. **`--full-node`**: This option enables the node to run in full mode, sharing its disk with the network, and becoming eligible for staking.

4. **`--swap-enable`**: This flag enables SWAP, which is the bandwidth incentives scheme for Swarm. It will initiate a transaction to set up the SWAP chequebook on Gnosis Chain (required for light and full nodes).

5. **`--api-addr 127.0.0.1:1633`**: Specifies that the Bee API will be accessible locally only via `127.0.0.1` on port `1633` and not accessible to the public. 

6. **`--blockchain-rpc-endpoint https://xdai.fairdatasociety.org`**: Sets the RPC endpoint for interacting with the Gnosis blockchain (required for light and full nodes).
:::

### Example Startup Output

<Tabs
defaultValue="full"
values={[
{label: 'Full', value: 'full'},
{label: 'Light', value: 'light'},
{label: 'Ultra Light', value: 'ultra-light'},
]}>

<TabItem value="full">
The node has successfully started, but it still needs funding with xDAI (for Gnosis Chain transactions) and xBZZ (for uploads, downloads, and staking). 

```bash
Welcome to Swarm.... Bzzz Bzzzz Bzzzz
                \     /
            \    o ^ o    /
              \ (     ) /
   ____________(%%%%%%%)____________
  (     /   /  )%%%%%%%(  \   \     )
  (___/___/__/           \__\___\___)
     (     /  /(%%%%%%%)\  \     )
      (__/___/ (%%%%%%%) \___\__)
              /(       )\
            /   (%%%%%)   \
                 (%%%)
                   !

DISCLAIMER:
This software is provided to you "as is", use at your own risk and without warranties of any kind.
It is your responsibility to read and understand how Swarm works and the implications of running this software.
The usage of Bee involves various risks, including, but not limited to:
damage to hardware or loss of funds associated with the Ethereum account connected to your node.
No developers or entity involved will be liable for any claims and damages associated with your use,
inability to use, or your interaction with other nodes or the software.

version: 2.2.0-06a0aca7 - planned to be supported until 11 December 2024, please follow https://ethswarm.org/

"time"="2024-09-24 18:15:34.383102" "level"="info" "logger"="node" "msg"="bee version" "version"="2.2.0-06a0aca7"
"time"="2024-09-24 18:15:34.428546" "level"="info" "logger"="node" "msg"="swarm public key" "public_key"="0373fe2ab33ab836635fc35864cf708fa0f4a775c0cf76ca851551e7787b58d040"
"time"="2024-09-24 18:15:34.520686" "level"="info" "logger"="node" "msg"="pss public key" "public_key"="03a341032724f1f9bb04f1d9b22607db485cccd74174331c701f3a6957d94d95c1"
"time"="2024-09-24 18:15:34.520716" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
"time"="2024-09-24 18:15:34.533789" "level"="info" "logger"="node" "msg"="fetching target neighborhood from suggester" "url"="https://api.swarmscan.io/v1/network/neighborhoods/suggestion"
"time"="2024-09-24 18:15:36.773501" "level"="info" "logger"="node" "msg"="mining a new overlay address to target the selected neighborhood" "target"="00100010110"
"time"="2024-09-24 18:15:36.776550" "level"="info" "logger"="node" "msg"="using overlay address" "address"="22d502d022de0f8e9d477bc61144d0d842d9d82b8241568c6fe4e41f0b466615"
"time"="2024-09-24 18:15:36.776576" "level"="info" "logger"="node" "msg"="starting with an enabled chain backend"
"time"="2024-09-24 18:15:37.388997" "level"="info" "logger"="node" "msg"="connected to blockchain backend" "version"="erigon/2.60.7/linux-amd64/go1.21.5"
"time"="2024-09-24 18:15:37.577840" "level"="info" "logger"="node" "msg"="using chain with network network" "chain_id"=100 "network_id"=1
"time"="2024-09-24 18:15:37.593747" "level"="info" "logger"="node" "msg"="starting debug & api server" "address"="127.0.0.1:1633"
"time"="2024-09-24 18:15:37.969782" "level"="info" "logger"="node" "msg"="using default factory address" "chain_id"=100 "factory_address"="0xC2d5A532cf69AA9A1378737D8ccDEF884B6E7420"
"time"="2024-09-24 18:15:38.160249" "level"="info" "logger"="node/chequebook" "msg"="no chequebook found, deploying new one."
"time"="2024-09-24 18:15:38.728534" "level"="warning" "logger"="node/chequebook" "msg"="cannot continue until there is at least min xDAI (for Gas) available on address" "min_amount"="0.0003750000017" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
```

  
</TabItem>
<TabItem value="light">

Here you can see that the node has started up successfully, but our node still needs to be funded with xDAI and xBZZ (xDAI for Gnosis Chain transactions and xBZZ for uploads/downloads). Continue to the next section for funding instructions.   

```bash
Welcome to Swarm.... Bzzz Bzzzz Bzzzz
                \     /
            \    o ^ o    /
              \ (     ) /
   ____________(%%%%%%%)____________
  (     /   /  )%%%%%%%(  \   \     )
  (___/___/__/           \__\___\___)
     (     /  /(%%%%%%%)\  \     )
      (__/___/ (%%%%%%%) \___\__)
              /(       )\
            /   (%%%%%)   \
                 (%%%)
                   !

DISCLAIMER:
This software is provided to you "as is", use at your own risk and without warranties of any kind.
It is your responsibility to read and understand how Swarm works and the implications of running this software.
The usage of Bee involves various risks, including, but not limited to:
damage to hardware or loss of funds associated with the Ethereum account connected to your node.
No developers or entity involved will be liable for any claims and damages associated with your use,
inability to use, or your interaction with other nodes or the software.

version: 2.2.0-06a0aca7 - planned to be supported until 11 December 2024, please follow https://ethswarm.org/

"time"="2025-01-24 12:57:21.274657" "level"="info" "logger"="node" "msg"="bee version" "version"="2.2.0-06a0aca7"
"time"="2025-01-24 12:57:21.274854" "level"="warning" "logger"="node" "msg"="your node is outdated, please check for the latest version"
"time"="2025-01-24 12:57:21.449064" "level"="info" "logger"="node" "msg"="swarm public key" "public_key"="03c356839a5570c758e812d0c248b135f0dc8ffa2b8404a97597e456f4fe5f7ee8"
"time"="2025-01-24 12:57:21.805033" "level"="info" "logger"="node" "msg"="pss public key" "public_key"="036c63b7c544ad401a5dbfb463f71cda265eec74c1d0d9cbc9db2abd6b3e4f11e9"
"time"="2025-01-24 12:57:21.805124" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x5c39545873Bd663b0bB0716ED87dE0E399Aae419"
"time"="2025-01-24 12:57:21.815765" "level"="info" "logger"="node" "msg"="using overlay address" "address"="74539eab1dbd5c722bb8ba10cef55f715e38f298b706fb1866af49f4fd15d8d3"
"time"="2025-01-24 12:57:21.815855" "level"="info" "logger"="node" "msg"="starting with an enabled chain backend"
"time"="2025-01-24 12:57:21.861341" "level"="info" "logger"="node" "msg"="connected to blockchain backend" "version"="Nethermind/v1.30.1+2b75a75a/linux-x64/dotnet9.0.0"
"time"="2025-01-24 12:57:21.869117" "level"="info" "logger"="node" "msg"="using chain with network network" "chain_id"=100 "network_id"=1
"time"="2025-01-24 12:57:21.880930" "level"="info" "logger"="node" "msg"="starting debug & api server" "address"="127.0.0.1:1633"
"time"="2025-01-24 12:57:21.897675" "level"="info" "logger"="node" "msg"="using default factory address" "chain_id"=100 "factory_address"="0xC2d5A532cf69AA9A1378737D8ccDEF884B6E7420"
"time"="2025-01-24 12:57:21.911463" "level"="info" "logger"="node/chequebook" "msg"="no chequebook found, deploying new one."
"time"="2025-01-24 12:57:21.938038" "level"="warning" "logger"="node/chequebook" "msg"="cannot continue until there is at least min xDAI (for Gas) available on address" "min_amount"="0.000250000002" "address"="0x5c39545873Bd663b0bB0716ED87dE0E399Aae419"
```

</TabItem>
<TabItem value="ultra-light">
 If you've started in ultra-light mode, you should see output which looks something like this, and you're done! Your node is now successfully running in ultra-light mode. You can now skip down to the final section on this page about logs and monitoring.

```bash
 root@noah-bee:~# bee start \
  --password flummoxedgranitecarrot \
  --api-addr 127.0.0.1:1633

Welcome to Swarm.... Bzzz Bzzzz Bzzzz
                \     /
            \    o ^ o    /
              \ (     ) /
   ____________(%%%%%%%)____________
  (     /   /  )%%%%%%%(  \   \     )
  (___/___/__/           \__\___\___)
     (     /  /(%%%%%%%)\  \     )
      (__/___/ (%%%%%%%) \___\__)
              /(       )\
            /   (%%%%%)   \
                 (%%%)
                   !

DISCLAIMER:
This software is provided to you "as is", use at your own risk and without warranties of any kind.
It is your responsibility to read and understand how Swarm works and the implications of running this software.
The usage of Bee involves various risks, including, but not limited to:
damage to hardware or loss of funds associated with the Ethereum account connected to your node.
No developers or entity involved will be liable for any claims and damages associated with your use,
inability to use, or your interaction with other nodes or the software.

version: 2.2.0-06a0aca7 - planned to be supported until 11 December 2024, please follow https://ethswarm.org/

"time"="2025-01-24 12:51:06.981505" "level"="info" "logger"="node" "msg"="bee version" "version"="2.2.0-06a0aca7"
"time"="2025-01-24 12:51:06.981658" "level"="warning" "logger"="node" "msg"="your node is outdated, please check for the latest version"
"time"="2025-01-24 12:51:07.131555" "level"="info" "logger"="node" "msg"="swarm public key" "public_key"="03c356839a5570c758e812d0c248b135f0dc8ffa2b8404a97597e456f4fe5f7ee8"
"time"="2025-01-24 12:51:07.402847" "level"="info" "logger"="node" "msg"="pss public key" "public_key"="036c63b7c544ad401a5dbfb463f71cda265eec74c1d0d9cbc9db2abd6b3e4f11e9"
"time"="2025-01-24 12:51:07.402915" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x5c39545873Bd663b0bB0716ED87dE0E399Aae419"
"time"="2025-01-24 12:51:07.416074" "level"="info" "logger"="node" "msg"="using overlay address" "address"="74539eab1dbd5c722bb8ba10cef55f715e38f298b706fb1866af49f4fd15d8d3"
"time"="2025-01-24 12:51:07.416149" "level"="info" "logger"="node" "msg"="starting with a disabled chain backend"
"time"="2025-01-24 12:51:07.416242" "level"="info" "logger"="node" "msg"="using chain with network network" "chain_id"=100 "network_id"=1
"time"="2025-01-24 12:51:07.428047" "level"="info" "logger"="node" "msg"="starting debug & api server" "address"="127.0.0.1:1633"
"time"="2025-01-24 12:51:07.464425" "level"="info" "logger"="node" "msg"="using datadir" "path"="/root/.bee"
"time"="2025-01-24 12:51:07.486853" "level"="info" "logger"="migration-RefCountSizeInc" "msg"="starting migration of replacing chunkstore items to increase refCnt capacity"
"time"="2025-01-24 12:51:07.486921" "level"="info" "logger"="migration-RefCountSizeInc" "msg"="migration complete"
"time"="2025-01-24 12:51:07.489133" "level"="info" "logger"="node" "msg"="starting reserve repair tool, do not interrupt or kill the process..."
"time"="2025-01-24 12:51:07.489346" "level"="info" "logger"="node" "msg"="removed all bin index entries"
"time"="2025-01-24 12:51:07.489430" "level"="info" "logger"="node" "msg"="removed all chunk bin items" "total_entries"=0
"time"="2025-01-24 12:51:07.489482" "level"="info" "logger"="node" "msg"="counted all batch radius entries" "total_entries"=0
"time"="2025-01-24 12:51:07.489520" "level"="info" "logger"="node" "msg"="parallel workers" "count"=2
"time"="2025-01-24 12:51:07.489612" "level"="info" "logger"="node" "msg"="migrated all chunk entries" "new_size"=0 "missing_chunks"=0 "invalid_sharky_chunks"=0
"time"="2025-01-24 12:51:07.489659" "level"="info" "logger"="migration-step-04" "msg"="starting sharky recovery"
"time"="2025-01-24 12:51:07.514853" "level"="info" "logger"="migration-step-04" "msg"="finished sharky recovery"
"time"="2025-01-24 12:51:07.515253" "level"="info" "logger"="migration-step-05" "msg"="start removing upload items"
"time"="2025-01-24 12:51:07.515374" "level"="info" "logger"="migration-step-05" "msg"="finished removing upload items"
"time"="2025-01-24 12:51:07.515434" "level"="info" "logger"="migration-step-06" "msg"="start adding stampHash to BatchRadiusItems, ChunkBinItems and StampIndexItems"
"time"="2025-01-24 12:51:07.515571" "level"="info" "logger"="migration-step-06" "msg"="finished migrating items" "seen"=0 "migrated"=0
"time"="2025-01-24 12:51:07.517270" "level"="info" "logger"="node" "msg"="starting in ultra-light mode"
```
</TabItem>

</Tabs>

## Fund and Stake 

Running a full node for the purpose of earning xBZZ by sharing disk space and participating in the redistribution game requires a minimum of 10 xBZZ and a small amount of xDAI (for initializing the chequebook contract and for paying for redistribution-related transactions). 

While running a light node requires a small amount of xDAI to pay for initializing the chequebook contract and a smaller amount of xBZZ to pay for uploads and downloads.

### Fund node 

Check the logs from the previous step. Look for the line which says: 

```
"time"="2024-09-24 18:15:34.520716" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
```
That address is your node's address on Gnosis Chain which needs to be funded with xDAI (and also xBZZ if you plan on doing any uploading or on staking). Copy it and save it for the next step.

You can also use the following command:

```bash
curl -s localhost:1633/addresses | jq .ethereum
```

Which will return your node's address:

```bash
"0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
```

***How Much to Send?***

Only a very small amount of xDAI is needed to get started, 0.1 xDAI is more than enough.
 
For very small short term uploads you can start with ~0.2 xBZZ, but the required amount will scale up with the volume and duration of storage required.

You will also need at least 10 xBZZ if you plan on staking.

### Initialize full node

After sending the required tokens of ~0.1 xDAI and 10 xBZZ (or a smaller amount of xBZZ if you don't plan on staking) to your node's Gnosis Chain address, close the bee process in your terminal (`Ctrl + C`). Then start it again with the same command:

```bash
bee start \
  --password flummoxedgranitecarrot \
  --full-node \
  --swap-enable \
  --api-addr 127.0.0.1:1633 \
  --blockchain-rpc-endpoint https://xdai.fairdatasociety.org
```
After funding and restarting your node, the logs printed to the terminal should look something like this:

```bash
Welcome to Swarm.... Bzzz Bzzzz Bzzzz
                \     /
            \    o ^ o    /
              \ (     ) /
   ____________(%%%%%%%)____________
  (     /   /  )%%%%%%%(  \   \     )
  (___/___/__/           \__\___\___)
     (     /  /(%%%%%%%)\  \     )
      (__/___/ (%%%%%%%) \___\__)
              /(       )\
            /   (%%%%%)   \
                 (%%%)
                   !

DISCLAIMER:
This software is provided to you "as is", use at your own risk and without warranties of any kind.
It is your responsibility to read and understand how Swarm works and the implications of running this software.
The usage of Bee involves various risks, including, but not limited to:
damage to hardware or loss of funds associated with the Ethereum account connected to your node.
No developers or entity involved will be liable for any claims and damages associated with your use,
inability to use, or your interaction with other nodes or the software.

version: 2.2.0-06a0aca7 - planned to be supported until 11 December 2024, please follow https://ethswarm.org/

"time"="2024-09-24 18:57:16.710417" "level"="info" "logger"="node" "msg"="bee version" "version"="2.2.0-06a0aca7"
"time"="2024-09-24 18:57:16.760154" "level"="info" "logger"="node" "msg"="swarm public key" "public_key"="0373fe2ab33ab836635fc35864cf708fa0f4a775c0cf76ca851551e7787b58d040"
"time"="2024-09-24 18:57:16.854594" "level"="info" "logger"="node" "msg"="pss public key" "public_key"="03a341032724f1f9bb04f1d9b22607db485cccd74174331c701f3a6957d94d95c1"
"time"="2024-09-24 18:57:16.854651" "level"="info" "logger"="node" "msg"="using ethereum address" "address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66"
"time"="2024-09-24 18:57:16.866697" "level"="info" "logger"="node" "msg"="using overlay address" "address"="22d502d022de0f8e9d477bc61144d0d842d9d82b8241568c6fe4e41f0b466615"
"time"="2024-09-24 18:57:16.866730" "level"="info" "logger"="node" "msg"="starting with an enabled chain backend"
"time"="2024-09-24 18:57:17.485408" "level"="info" "logger"="node" "msg"="connected to blockchain backend" "version"="erigon/2.60.1/linux-amd64/go1.21.5"
"time"="2024-09-24 18:57:17.672282" "level"="info" "logger"="node" "msg"="using chain with network network" "chain_id"=100 "network_id"=1
"time"="2024-09-24 18:57:17.686479" "level"="info" "logger"="node" "msg"="starting debug & api server" "address"="127.0.0.1:1633"
"time"="2024-09-24 18:57:18.065029" "level"="info" "logger"="node" "msg"="using default factory address" "chain_id"=100 "factory_address"="0xC2d5A532cf69AA9A1378737D8ccDEF884B6E7420"
"time"="2024-09-24 18:57:18.252410" "level"="info" "logger"="node/chequebook" "msg"="no chequebook found, deploying new one."
"time"="2024-09-24 18:57:19.576100" "level"="info" "logger"="node/chequebook" "msg"="deploying new chequebook" "tx"="0xf7bc9c5b04e96954c7f70cecfe717cad9cdc5d64b6ec080b2cbe712166ce262a"
"time"="2024-09-24 18:57:27.619377" "level"="info" "logger"="node/transaction" "msg"="pending transaction confirmed" "sender_address"="0x1A801dd3ec955E905ca424a85C3423599bfb0E66" "tx"="0xf7bc9c5b04e96954c7f70cecfe717cad9cdc5d64b6ec080b2cbe712166ce262a"
"time"="2024-09-24 18:57:27.619437" "level"="info" "logger"="node/chequebook" "msg"="chequebook deployed" "chequebook_address"="0x261a07a63dC1e7200d51106155C8929b432181fb"
```

Here we can see that after our node has been funded, it was able to issue the transactions for deploying the chequebook contract, which is a prerequisite for running a staking node.

Next your node will begin to sync [postage stamp data](/docs/develop/tools-and-features/buy-a-stamp-batch), which can take ~5 to 10 minutes. You will see this log message while your node is syncing postage stamp data:

```bash
"time"="2024-09-24 22:21:19.664897" "level"="info" "logger"="node" "msg"="waiting to sync postage contract data, this may take a while... more info available in Debug loglevel"
```

After your node finishes syncing postage stamp data it will start in full node mode and begin to sync all the chunks of data it is responsible for storing as a full node:

```bash
"time"="2024-09-24 22:30:19.154067" "level"="info" "logger"="node" "msg"="starting in full mode"
"time"="2024-09-24 22:30:19.155320" "level"="info" "logger"="node/multiresolver" "msg"="name resolver: no name resolution service provided"
"time"="2024-09-24 22:30:19.341032" "level"="info" "logger"="node/storageincentives" "msg"="entered new phase" "phase"="reveal" "round"=237974 "block"=36172090
"time"="2024-09-24 22:30:33.610825" "level"="info" "logger"="node/kademlia" "msg"="disconnected peer" "peer_address"="6ceb30c7afc11716f866d19b7eeda9836757031ed056b61961e949f6e705b49e"
```

This process can take a while, even up to several hours depending on your system and network. You can check the progress of your node through the logs which print out to the Bee API:

You check your node's progress with the `/status` endpoint:

:::info
The [`jq` utility](https://jqlang.github.io/jq/) jq utility formats API responses for easier reading:
* Install it using your system‚Äôs package manager.
* If you don't want to use it, remove `| jq` from all commands.
:::

```bash
curl -s  http://localhost:1633/status | jq
```

```bash
{
  "overlay": "22dc155fe072e131449ec7ea2f77de16f4735f06257ebaa5daf2fdcf14267fd9",
  "proximity": 256,
  "beeMode": "full",
  "reserveSize": 686217,
  "reserveSizeWithinRadius": 321888,
  "pullsyncRate": 497.8747754074074,
  "storageRadius": 11,
  "connectedPeers": 148,
  "neighborhoodSize": 4,
  "batchCommitment": 74510761984,
  "isReachable": false,
  "lastSyncedBlock": 36172390
}
```
We can see that our node has not yet finished syncing chunks since the `pullsyncRate` is around 497 chunks per second. Once the node is fully synced, this value will go to zero. However, we do not need to wait until our node is fully synced in order to stake our node, so we can now move immediately to the next step.

### Stake node

Now we're ready to stake. We'll slightly modify our startup command so that it runs in the background instead of taking control of our terminal:

```bash
nohup bee start \
  --password flummoxedgranitecarrot \
  --full-node \
  --swap-enable \
  --api-addr 127.0.0.1:1633 \
  --blockchain-rpc-endpoint https://xdai.fairdatasociety.org > bee.log 2>&1 &
```

:::info
1. **`nohup`**: `nohup` prevents the `bee start` process from stopping when the terminal closes.

2. **`> bee.log 2>&1`**: Redirects both standard output and standard error to a log file called `bee.log`. 

3. **`&`**: This sends the process to the background, allowing the terminal to be used for other commands while the Bee node continues running.
:::

Let's check the Bee API to confirm the node is running:

```
curl localhost:1633
```
If the node is running we should see:
```
Ethereum Swarm Bee
```

Now with our node properly running in the background, we're ready to stake our node. You can use the following command to stake 10 xBZZ:

```bash
curl -XPOST localhost:1633/stake/100000000000000000
```

If the staking transaction is successful a `txHash` will be returned:

```
{"txHash":"0x258d64720fe7abade794f14ef3261534ff823ef3e2e0011c431c31aea75c2dd5"}
```

We can also confirm that our node has been staked with the `/stake` endpoint:

```bash
curl localhost:1633/stake
```

The results will be displayed in PLUR units (1 PLUR is equal to 1e-16 xBZZ). If you have properly staked the minimum 10 xBZZ, you should see the output below:

```bash
{"stakedAmount":"100000000000000000"}
```
 
Congratulations! You have now installed your Bee node and are connected to the network as a full staking node. Your node will now be in the process of syncing chunks from the network. Once the node is fully synced, your node will finally be eligible to earn staking rewards.

### Set Target Neighborhood

When installing your Bee node it will automatically be assigned a neighborhood. However, when running a full node with staking there are benefits to periodically updating your node's neighborhood. Learn more about why and how to set your node's target neighborhood [here](/docs/bee/installation/set-target-neighborhood).

### Logs and monitoring

:::info
You can learn more about Bee logs [here](/docs/bee/working-with-bee/logs-and-files).
:::

With our previously modified command, our Bee node will now be running in the background and the logs will be written to the `bee.log` file. To review our node's logs we can simply view the file contents:

```bash
cat bee.log
```

The file will continue to update with all the latest logs as they are output:

```bash
"time"="2024-09-27 18:05:34.096641" "level"="info" "logger"="node/kademlia" "msg"="connected to peer" "peer_address"="03b48e678938d63c0761c74a805fbe0446684c9c417330c2bec600ecfd6c492f" "proximity_order"=8
"time"="2024-09-27 18:05:35.168425" "level"="info" "logger"="node/kademlia" "msg"="connected to peer" "peer_address"="0e9388fff473a9c74535337c32cc74d8f921514d2635d0c4a49c6e8022f5594e" "proximity_order"=4
"time"="2024-09-27 18:05:35.532723" "level"="info" "logger"="node/kademlia" "msg"="disconnected peer" "peer_address"="3c195cd8882ee537d170e92d959ad6bd72a76a50097a671c72646e83b45a1832"
```

There are many different ways to monitor your Bee node's process, but one convenient way to do so is the [bashtop command line tool](https://github.com/aristocratos/bashtop). The method of [installation](https://github.com/aristocratos/bashtop?tab=readme-ov-file#installation) will vary depending on your system.

After installation, we can launch it with the `bashtop` command:

```bash
bashtop
```

![](/img/bashtop_01.png)

We can use the `f` key to filter for our Bee node's specific process by searching for the `bee` keyword (use the arrow keys to navigate and `enter` to select). From here we can view info about our node's process, or shut it down using the `t` key (for "terminate").

![](/img/bashtop_02.png)

**Checking the Node's status with the Bee API**

To check your node's status as a staking node, we can use the `/redistributionstate` endpoint:

```bash
curl -s http://localhost:1633/redistributionstate | jq
```

Below is the output for a node that has been running for several days:

```bash
{
  "minimumGasFunds": "11080889201250000",
  "hasSufficientFunds": true,
  "isFrozen": false,
  "isFullySynced": true,
  "phase": "claim",
  "round": 212859,
  "lastWonRound": 207391,
  "lastPlayedRound": 210941,
  "lastFrozenRound": 210942,
  "lastSelectedRound": 212553,
  "lastSampleDuration": 491687776653,
  "block": 32354719,
  "reward": "1804537795127017472",
  "fees": "592679945236926714",
  "isHealthy": true
}
```

For a complete breakdown of this output, check out [this section in the Bee docs](/docs/bee/working-with-bee/bee-api#redistributionstate).

You can read more other important endpoints for monitoring your Bee node in the [official Bee docs](/docs/bee/working-with-bee/bee-api), and you can find complete information about all available endpoints in [the API reference docs](/api/).

## Back Up Keys

Once your node is up and running, make sure to [back up your keys](/docs/bee/working-with-bee/backups).

## Getting help

The CLI has built-in documentation. Running `bee` gives you an entry point to the documentation. Running `bee start -h` or `bee start --help` will tell you how you can configure your Bee node via the command line arguments.

You may also check out the [configuration guide](/docs/bee/working-with-bee/configuration), or simply run your Bee terminal command with the `--help` flag, eg. `bee start --help` or `bee --help`.

## Next Steps to Consider

### Access the Swarm
If you'd like to start uploading or downloading files to Swarm, [start here](/docs/develop/introduction).

### Explore the API
The [Bee API](/docs/bee/working-with-bee/bee-api) is the primary method for interacting with Bee and getting information about Bee. After installing Bee and getting it up and running, it's a good idea to start getting familiar with the API.

### Run a hive! 
If you would like to run a hive of many Bees, check out the [hive operators](/docs/bee/installation/hive) section for information on how to operate and monitor many Bees at once.

### Start building DAPPs on Swarm
If you would like to start building decentralised applications on Swarm, check out our section for [developing with Bee](/docs/develop/introduction).

---

## Verify

verify.md

https://github.com/ethersphere/bee/pull/1581

---

## Backups

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Backing up your Bee node involves copying and saving files from the data directory specified in the `dat-dir` configuration option, along with the node's password. The details of where and how this option is specified will vary depending on the type of [configuration method](/docs/bee/working-with-bee/configuration) used (YAML file, command line flag, or environment variable).

:::caution
A node's password may be specified in several different locations. It can be specified either through the `password` option or the `password-file` option. For a backup, you will need to either copy the `password` option value, or copy the file itself from the location specified by the `password-file` option.

Don't forget - it's not a backup until you're sure the backup files work! Make sure to test restoring from backup files and password to prevent loss of assets due to data loss or corruption.
:::

## Bee Files

A full Bee node backup includes the `keys`,  `localstore`, `stamperstore`,  `statestore`, and `password` files. The node should be stopped before taking a backup and not restarted until restoring the node from the backup to prevent the node from getting out of sync with the network.

Key data from the `keys` directory allows access to Bee node's Gnosis account (provided that you have also made sure to back the password for your keys). If your keys and password are lost or stolen it could lead to the loss of all assets in that account. The `stamperstore` contains postage stamp data. If lost, previously purchased postage stamps will become unusable.

### Statestore and Localstore.

The `statestore` retains data related to its operation, and the `localstore` contains chunks locally which are frequently requested, pinned in the node, or are in the node's neighborhood of responsibility.

:::info
As the data in `statestore` and `localstore` continually changes during normal operation of a node, when taking a backup the node should first be stopped and not re-connected to the Swarm network until restoring from the backup (otherwise the `statestore` and `localstore` files will get out of sync with the network). It is possible to restore using out of sync `statestore` and `localstore` files, however it may lead to data loss or unexpected behavior related to chunk uploads, postage stamps, and more. 
:::

### Stamperstore

The `stamperstore` contains postage stamp batch related data, and so is important to include in your backup if you have purchased any postage batches which you wish to continue using.

### Keys

The `keys` directory contains the following key files: 

* `libp2p.key`
* `libp2p_v2.key`
* `pss.key`
* `swarm.key`

These keys are generated during the Bee node's initialisation and are required for maintaining access to your node.

:::danger
The `swarm.key` file grants full control over your node's Gnosis Chain account. If lost, you cannot recover funds. If stolen, your assets can be drained.
:::

:::info
To use `swarm.key` to manage the Gnosis account for a node through Metamask or other wallets, [exportSwarmKeys](https://github.com/ethersphere/exportSwarmKey) can be used to convert `swarm.key` to a compatible format.
:::

### Data Directory Structure

The data directory contains four directories. Its default location depends on the node install method and startup method used.

```
‚îú‚îÄ‚îÄ kademlia-metrics
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ keys
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libp2p.key
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libp2p_v2.key
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pss.key
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ swarm.key
‚îú‚îÄ‚îÄ localstore
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ indexstore
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ sharky
‚îú‚îÄ‚îÄ password
‚îú‚îÄ‚îÄ stamperstore
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ statestore
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ...
```

## Data Directory Locations

The default data directory for your Bee node will depend on the installation method used. 

:::caution
If Bee is installed to run as a service using a package manager such as `apt` or `yum`, then it can be started using your system's services manager such as `systemctl` using a command like `systemctl start bee`. However, after installing with a package manager, Bee can also by started using the `bee start` command used for running Bee with a shell script / binary install. When the `bee start` command is run, it will create a SECOND data directory alongside the default data directory for your package manager at the same directory it would for the shell script installation:

```
/home/<user>/.bee
```

In that case, you would have two separate data directories in two different locations, and the directory used will depend on whether you start your node using a service manager like `systemctl` or the `bee start` command. 

If you installed Bee via a package manager but sometimes start it manually, you may have two separate data directories:

* System service (`systemctl start bee`) ‚Üí Uses `/var/lib/bee`.
* Manual start (`bee start`) ‚Üí Uses `/home/<user>/.bee`.

*The exact directory will differ depending on your system. See [Configuration page](/docs/bee/working-with-bee/configuration#default-data-and-config-directories).*
:::

### *apt* and *yum / rpm* Package Managers

Default `data-dir` location:

```
/var/lib/bee
```

### Homebrew (amd64)

Default `data-dir` location:

```
/usr/local/var/lib/swarm-bee
```

### Homebrew (arm64)

Default `data-dir` location:

```
/opt/homebrew/var/lib/swarm-bee
```

### *scoop* Package Manager

Default `data-dir` location:

```
./data
``` 

### Shell Script & Binary Install 

If you installed Bee using the [automated shell script](/docs/bee/installation/shell-script-install) or by [building Bee from source](/docs/bee/installation/build-from-source), your data directory will typically be located at:

```bash
/home/<user>/.bee
```

### Docker

Default `data-dir` location:

```
/home/bee/.bee
```

## Back-up your node data

Copy entire `bee` data folder to create a full backup. This will do a full backup of `kademlia-metrics`,  `keys`,  `statestore`,  `stamperstore`, `password`, and `localstore`, files into a newly created `/backup` directory. Make sure to save the backup directory to a safe location.

:::tip
For a more lightweight backup, you can remove `localstore` and `localstore`. You can safely restore your node from the remaining files.
:::

```
mkdir backup
sudo cp -r /var/lib/bee/ backup
```
    
## Back-up your password 

Depending on your [configuration](/docs/bee/working-with-bee/configuration) method, your password may be located in a variety of different locations. If you use a `.yaml` file for your configuration, then it might be found directly under the `password` option, or it could be that the location of your password file is recorded by the `password-file` option. In either case, make sure to record the password somewhere safe or include the password file as a part of your backup.   

The same applies to other configuration methods. If you use environment variables for specifying your configuration options, your password itself will likely be specified in a `.env` file somewhere which contains either the password itself in the `BEE_PASSWORD` variable or the location of your password file in the `BEE_PASSWORD_FILE` variable. 

The same again holds true for the command line flag method. Make sure you have the password you use with the `--password` command line flag or the password file specified by the `--password-file` flag saved in your backup. 

## Back-up blockchain keys only

If you only need to export your node's blockchain keys, you need to export the `swarm.key` UTC / JSON keystore file and the `password` file used to encrypt it. First create a directory for your keys and then copy your keys to that directory.

```bash
mkdir keystore
sudo cp -r /var/lib/bee/keys/swarm.key /var/lib/bee/password keystore    
```

## Metamask Import

If you wish to import your Bee node‚Äôs Gnosis Chain account into Metamask, find your `swarm.key` and `password`, then follow these steps:

## View key and password for wallet import 

```bash
sudo cat /var/lib/bee/keys/swarm.key 
sudo cat /var/lib/bee/password
```

:::info
Note that `swarm.key` is in UTC / JSON keystores format and is encrypted by default by your password file inside the `/bee` directory. Make sure to export both the `swarm.key` file and the `password` file in order to secure your wallet. If you need your private key exported from the keystore file, you may use one of a variety of Ethereum wallets which support exporting private keys from UTC files (such as [Metamask](https://metamask.io/), however we offer no guarantees for any software, make sure you trust it completely before using it). 
:::

## Get private key from keystore and password

To import to Metamask:

1. View and copy your `swarm.key` and `password` as shown above
2. Go to Metamask and click "Account 1" --> "Import Account"
3. Choose the "Select Type" dropdown menu and choose "JSON file"
4. Paste the password (Make sure to do this first)
5. Upload exported JSON file  
6. Click "Import"

To export your private key:

1. Go to Metamask and click "Account 1" to view the dropdown menu of all accounts
2. Click the three dots next to the account you want to export
3. Click "Account details"
4. Click "Show private key"
5. Enter your Metamask password (not your keystore password)
6. Copy your private key to a safe location

## Restore from backup

:::danger
Before restoring, make sure to check for any old node data from a previous node which has not yet been backed up, and back it up if needed.
:::

:::tip
The specific directories and commands for restoring will depend on which install method and system is used. The instructions below are for a Linux package manager based installation. See the [configuration section](/docs/bee/working-with-bee/configuration#default-data-and-config-directories) more more details about default file locations.
:::

1. After [uninstalling](/docs/bee/working-with-bee/uninstalling-bee) any existing Bee installations, perform a new [installation](/docs/bee/installation/getting-started#installation-methods).

1. Remove any existing Bee node data before restoring. This prevents conflicts with old files:

    ```
    sudo rm -r /var/lib/bee
    ```

1. Navigate to backup directory and copy files to data folder.

    ```
    sudo cp -r /<path-to-backup>/. /var/lib/bee
    ```
    
1. Revert ownership of the data folder. 
    ```
    sudo chown -R bee:bee /var/lib/bee
    ```
1. Restart `bee` and check logs.
    ```
    sudo systemctl restart bee
    sudo journalctl --lines=100 --follow --unit bee      
    ```

---

## Bcrypt hashing utility

In order to generate a valid admin password hash you can use any available bcrypt compatible tools, both [online](https://bcrypt-generator.com/) and offline (htpasswd).

For convenience Bee also provides a method to generate and validate password hashes:

```sh
$ bee bcrypt super$ecret
$2a$10$eZP5YuhJq2k8DFmj9UJGWOIjDtXu6NcAQMrz7Zj1bgIVBcHA3bU5u
$ bee bcrypt --check super$ecret '$2a$10$eZP5YuhJq2k8DFmj9UJGWOIjDtXu6NcAQMrz7Zj1bgIVBcHA3bU5u'
OK: password hash matches provided plain text
```

:::info
When validating a hash don't forget about quotes - the ($) hash prefix might interfere with your terminal.
:::

---

## Bee API

The Bee HTTP API is the primary interface to a running Bee node. API-endpoints can be queried using familiar HTTP requests, and will respond with semantically accurate [HTTP status and error codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) as well as data payloads in [JSON](https://www.json.org/json-en.html) format where appropriate.

The Bee API provides full access to all core functionalities of a Bee node, including uploading, downloading, staking, postage stamp batch purchasing and management, and node monitoring. By default, it runs on port `:1633`.

:::danger
Make sure that your api-addr (default 1633) is never exposed to the internet. If you do not have a firewall or other security measures in place, manually setting your Bee API address from the default `1633` to `127.0.0.1:1633` is strongly recommended to prevent unauthorized access.

You may also consider using the [Gateway Proxy tool](/docs/develop/tools-and-features/gateway-proxy) to protect your node's API endpoint.
:::

Detailed information about Bee API endpoints can be found in the [API reference docs](/api/).

## Interacting With the API

You can interact with the Bee API using standard HTTP requests, allowing you to programmatically access all of your Bee node's various functions such as [purchasing stamp batches](/docs/develop/tools-and-features/buy-a-stamp-batch), [uploading and downloading](/docs/develop/upload-and-download), [staking](/docs/bee/working-with-bee/staking), and more.

### Alternatives for Working with the API

For developers, the [Bee JS library](/docs/develop/tools-and-features/bee-js) offers a more convenient way to interact with the API in a NodeJS environment.

For many other common use cases, you may prefer to make use of the [Swarm CLI](/docs/bee/working-with-bee/swarm-cli) tool, as it offers a convenient command line based interface for interacting with your node's API.

## Exploring Node Status

After installing and starting up your node, we can begin to understand the node's status by interacting with the API.

For example, to determine how many nodes your Bee node is currently connected to, run:

```bash
curl -s http://localhost:1633/peers | jq '.peers | length'
```

```
23
```

Great! We can see that we are currently connected with 23 other nodes!

:::info
Here we are using the `jq` command line utility to count the amount of objects in the `peers` array in the JSON response we have received from our API, learn more about how to install and use `jq` [here](https://stedolan.github.io/jq/).
:::

Let's review a handful of endpoints which will provide you with important information relevant to detecting and diagnosing problems with your nodes.

### _/status_

The `/status` endpoint returns a quick summary of some important metrics for your node.

```bash
  curl -s  http://localhost:1633/status | jq
```

```bash
{
  "overlay": "1e2054bec3e681aeb0b365a1f9a574a03782176bd3ec0bcf810ebcaf551e4070",
  "proximity": 256,
  "beeMode": "full",
  "reserveSize": 3215597,
  "reserveSizeWithinRadius": 3215806,
  "pullsyncRate": 1.5622222222222222,
  "storageRadius": 10,
  "connectedPeers": 89,
  "neighborhoodSize": 12,
  "batchCommitment": 11615207424,
  "isReachable": true,
  "lastSyncedBlock": 41786200,
  "committedDepth": 10,
  "isWarmingUp": false
}
```

- `"overlay"` - Your node's overlay address.
- `"proximity"` - The proximity order (PO), representing how closely related this node is to your own node in Swarm's Kademlia network.
- `"beeMode"` - The mode of your node, can be `"full"`, `"light"`, or `"ultraLight"`.
- `"reserveSize"` - The number of chunks your node is currently storing in its reserve. This value should be roughly similar across nodes in the network. It should be identical for nodes within the same neighborhood.
- `"reserveSizeWithinRadius"` - The number of chunks your node is currently storing which fall within its storage radius.
- `"pullsyncRate"` - The rate at which your node is currently syncing chunks from other nodes in the network.
- `"storageRadius"` - The storage radius (radius of responsibility ) is the proximity order of chunks for which your node is responsible for storing. It should generally match the radius shown on [Swarmscan](https://swarmscan.io/neighborhoods).
- `"connectedPeers"` - The number of peers your node is connected to.
- `"neighborhoodSize"` - The number of total neighbors in your neighborhood, not including your own node. The more nodes in your neighborhood, the lower your chance of winning rewards as a staking node.
- `"batchCommitment"` - The total number of chunks which would be stored on the Swarm network if 100% of all postage batches were fully utilised.
- `"isReachable"` - Whether or not your node is reachable on the p2p API by other nodes on the Swarm network (port 1634 by default).
- `"lastSyncedBlock"` - The last block number from the connected blockchain that your node has synced up to.
- `"committedDepth"` - The storage depth currently committed by your node, which defines how much of your reserve is actually being used to store chunks. Is equal to `"storageRadius"` plus the [doubling factor](/docs/bee/working-with-bee/staking/#reserve-doubling) specified in the `reserve-capacity-doubling` option (which is zero by default).
- `"isWarmingUp"` - Indicates whether your node is still in the warm-up phase (building up its reserve and syncing with the network) or has reached normal operation.

### _/status/peers_

The `/status/peers` endpoint returns information about all the peers of the node making the request. The type of the object returned is the same as that returned from the `/status` endpoint. This endpoint is useful for diagnosing syncing / availability issues with your node.

The list is sorted by Kademlia proximity, not geographical distance. Nodes with lower PO values are further away, while higher PO values indicate closer neighbors. The most distant nodes with PO (proximity order) of zero are at the top of the list and the closest nodes with higher POs at the bottom of the list. The nodes at the bottom of the list with a PO equal or greater than the storage depth make up the nodes in your own node's neighborhood. It's possible that not all nodes in your neighborhood will appear in this list each time you call the endpoint if the connection between your nodes and the rest of the nodes in the neighborhood is not stable.

Here are the last few entries:

```bash
 curl -s http://localhost:1633/status/peers | jq
```

```bash
 ...
  {
      "overlay": "1e1547d0d629469ff0d8fd2cbb6435df8fd913f2e948f177d733356d784b7ea4",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3217677,
      "reserveSizeWithinRadius": 3215613,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 153,
      "neighborhoodSize": 12,
      "batchCommitment": 11615207424,
      "isReachable": true,
      "lastSyncedBlock": 41786375,
      "committedDepth": 10,
      "isWarmingUp": false
    },
    {
      "overlay": "1e09531ee3d8031b130b1c7d530dac26f57d2b9cfd368a979ef227331deb2ae5",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3215934,
      "reserveSizeWithinRadius": 3215613,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 166,
      "neighborhoodSize": 12,
      "batchCommitment": 11615207424,
      "isReachable": true,
      "lastSyncedBlock": 41786375,
      "committedDepth": 10,
      "isWarmingUp": false
    },
    {
      "overlay": "1e1ad7975d88430b8ede359ca231e73aaffaeefe35d6f32e709ff37dc3028eaa",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3215364,
      "reserveSizeWithinRadius": 3215344,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 150,
      "neighborhoodSize": 12,
      "batchCommitment": 11614158848,
      "isReachable": true,
      "lastSyncedBlock": 41786375,
      "committedDepth": 10,
      "isWarmingUp": false
    },
    {
      "overlay": "1e30c8fe93339f8637a339b5d4d85ec42731a193be8987c6457f4ea72c93cfb7",
      "proximity": 11,
      "beeMode": "full",
      "reserveSize": 3218783,
      "reserveSizeWithinRadius": 3215613,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 165,
      "neighborhoodSize": 12,
      "batchCommitment": 11615207424,
      "isReachable": true,
      "lastSyncedBlock": 41786375,
      "committedDepth": 10,
      "isWarmingUp": false
    },
    {
      "overlay": "1e3c168d12e0f590640454c01e2825522ca60eb0a1c7dfaac9da2329e9d87300",
      "proximity": 11,
      "beeMode": "full",
      "reserveSize": 3215635,
      "reserveSizeWithinRadius": 3215613,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 169,
      "neighborhoodSize": 12,
      "batchCommitment": 11615207424,
      "isReachable": true,
      "lastSyncedBlock": 41786375,
      "committedDepth": 10,
      "isWarmingUp": false
    },
    {
      "overlay": "1e3f2e9b0f6d45aa1fd710e7fca4a7890d2cbde829cd2722674ab120544e3772",
      "proximity": 11,
      "beeMode": "full",
      "reserveSize": 3215645,
      "reserveSizeWithinRadius": 3215613,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 163,
      "neighborhoodSize": 12,
      "batchCommitment": 11615207424,
      "isReachable": true,
      "lastSyncedBlock": 41786375,
      "committedDepth": 10,
      "isWarmingUp": false
    },
    {
      "overlay": "1e288371f2e3c3325c1a3af5008d7c81fa4ab1d176e1c6bbb3f9ace4655dc05d",
      "proximity": 12,
      "beeMode": "full",
      "reserveSize": 3215644,
      "reserveSizeWithinRadius": 3215613,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 165,
      "neighborhoodSize": 12,
      "batchCommitment": 11615207424,
      "isReachable": true,
      "lastSyncedBlock": 41786375,
      "committedDepth": 10,
      "isWarmingUp": false
    },
    {
      "overlay": "1e2c2b11a118a0be240af19421a8a323610869247625fa28a7590d765a21c566",
      "proximity": 12,
      "beeMode": "full",
      "reserveSize": 3215633,
      "reserveSizeWithinRadius": 3215613,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 172,
      "neighborhoodSize": 12,
      "batchCommitment": 11615207424,
      "isReachable": true,
      "lastSyncedBlock": 41786370,
      "committedDepth": 10,
      "isWarmingUp": false
    },
    {
      "overlay": "1e20ef01ddab9112a9a26618d901c761f20d8bcb8328c143ab13e9846be9ad82",
      "proximity": 16,
      "beeMode": "full",
      "reserveSize": 3215652,
      "reserveSizeWithinRadius": 3215613,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 164,
      "neighborhoodSize": 12,
      "batchCommitment": 11615207424,
      "isReachable": true,
      "lastSyncedBlock": 41786375,
      "committedDepth": 10,
      "isWarmingUp": false
    }
  ]
}
```

And we can compare these entries to our own node's `/status` results for diagnostic purposes:

```bash
 curl -s http://localhost:1633/status | jq
```

```bash
{
  "overlay": "1e2054bec3e681aeb0b365a1f9a574a03782176bd3ec0bcf810ebcaf551e4070",
  "proximity": 256,
  "beeMode": "full",
  "reserveSize": 3215597,
  "reserveSizeWithinRadius": 3215806,
  "pullsyncRate": 1.5622222222222222,
  "storageRadius": 10,
  "connectedPeers": 89,
  "neighborhoodSize": 12,
  "batchCommitment": 11615207424,
  "isReachable": true,
  "lastSyncedBlock": 41786200,
  "committedDepth": 10,
  "isWarmingUp": false
}
```

From the results we can see that our node's neighborhood size and batch commitment are generally in line with other nodes in the same neighborhood. Any significant discrepancy may indicate a problem with your node.

### _/redistributionstate_

    This endpoint provides an overview of values related to storage fee redistribution game (in other words, staking rewards). You can use this endpoint to check whether or not your node is participating properly in the redistribution game.

    ```bash
    curl -s http://localhost:1633/redistributionstate | jq
    ```

    ```bash
    {
      "minimumGasFunds": "11080889201250000",
      "hasSufficientFunds": true,
      "isFrozen": false,
      "isFullySynced": true,
      "phase": "claim",
      "round": 212859,
      "lastWonRound": 207391,
      "lastPlayedRound": 210941,
      "lastFrozenRound": 210942,
      "lastSelectedRound": 212553,
      "lastSampleDuration": 491687776653,
      "block": 32354719,
      "reward": "1804537795127017472",
      "fees": "592679945236926714",
      "isHealthy": true
    }
    ```

    * `"minimumGasFunds"` - The minimum required xDAI denominated in wei (1 xDAI = 10^18 wei) required for a node to participate in the redistribution game.
    * `"hasSufficientFunds"` - Whether your node has at least the `"minimumGasFunds"` amount of xDAI.
    * `"isFrozen"` - Indicates if your node is frozen, which may occur for [several reasons](/docs/bee/working-with-bee/staking/#diagnosing-freezing-issues).
    * `"isFullySynced"` - Whether your node has fully synced all the chunks in its `"storageRadius"` (the value returned from the `/reservestate` endpoint.)
    * `"phase"` - The current phase of the redistribution game (this does not indicate whether or not your node is participating in the current phase).
    * `"round"` - The current number of the round of the redistribution game.
    * `"lastWonRound"` - The last round number in which your node won the redistribution game.
    * `"lastPlayedRound"` - The last round number in which your node participating in the redistribution game. If this number matches the number of the current round shown in `"round"`, then your node is participating in the current round.
    * `"lastFrozenRound"` - The last round in which your node was frozen.
    * `"lastSelectedRound"` - The last round in which your node's neighborhood was selected. Note that it is possible for your node's neighborhood to be selected without your node playing in the redistribution game. This may potentially indicate your node's hardware is not sufficient to calculate the commitment hash fast enough. See [section on the `/rchash` endpoint](#) for more information.
    * `"lastSampleDuration"` - The time it took for your node to calculate the sample commitment hash in nanoseconds.
    * `"block"` - current Gnosis block number
    * `"reward"` - The total all-time reward in PLUR earned by your node.
    * `"fees"` - The total amount in fees paid by your node denominated in xDAI wei.
    * `"isHealthy"` - a check of whether your node‚Äôs storage radius is the same as the most common radius from among its peer nodes

### _/reservestate_

    This endpoint shows key information about the reserve state of your node. You can use it to identify problems with your node related to its reserve (whether it is syncing chunks properly into its reserve for example).

    ```bash
        curl -s  http://localhost:1633/reservestate | jq
    ```
    ```bash
        {
          "radius": 15,
          "storageRadius": 10,
          "commitment": 134121783296
        }
    ```

    Let's take a look at each of these values:
    * `"radius"` - Represents the maximum storage radius assuming all postage stamp batches are fully utilized.
    * `"storageRadius"` - The radius of responsibility - the proximity order of chunks for which your node is responsible for storing. It should generally match the radius shown on [Swarmscan](https://swarmscan.io/neighborhoods).
    * `"commitment"` - The total number of chunks which would be stored on the Swarm network if 100% of all postage batches were fully utilised.

### _/chainstate_

    This endpoint relates to your node's interactions with the Swarm Smart contracts on the Gnosis Chain.

    ```bash
     curl -s http://localhost:1633/chainstate | jq

    {
      "chainTip": 41786513,
      "block": 41786505,
      "totalAmount": "293796491451",
      "currentPrice": "56774"
    }
    ```
    * `"chainTip"` - The latest Gnosis Chain block number. Should be as high as or almost as high as the block number shown at [GnosisScan](https://gnosisscan.io/).
    * `"block"` -  The latest block your node has fully synced from Gnosis Chain. If significantly behind `"chainTip"`, your node may still be catching up. Should be very close to `"chainTip"` if your node has already been operating for a while.
    * `"totalAmount"` - Cumulative value of all prices per chunk in PLUR for each block.
    * `"currentPrice"` - The price in PLUR to store a single chunk for each Gnosis Chain block.

### _/topology_

    This endpoint allows you to explore the topology of your node within the Kademlia network. The results are split into 32 bins from bin_0 to bin_32. Each bin represents the nodes in the same neighborhood as your node at each proximity order from PO 0 to PO 32.

    As the output of this file can be very large, we save it to the `topology.json` file for easier inspection:

    ```bash
     curl -s http://localhost:1633/topology | jq '.' > topology.json
    ```
    We open the file in vim for inspection:
    ```bash
    vim topology.json
    ```

    The `/topology` endpoint provides insights into how your node is positioned within the Swarm network. The response starts with global network statistics, followed by detailed bin-by-bin peer connections (for 32 bins). Lets first look at the global stats:

    ```json
      "baseAddr": "da7e5cc3ed9a46b6e7491d3bf738535d98112641380cbed2e9ddfe4cf4fc01c4",
      "population": 20514,
      "connected": 176,
      "timestamp": "2024-02-08T20:57:03.815537925Z",
      "nnLowWatermark": 3,
      "depth": 10,
      "reachability": "Public",
      "networkAvailability": "Available",
        ...
    ```

- `"baseAddr"` - Your node's overlay address.
- `"population"` - The total number of nodes your node has collected information about. This number should be around ####. If it is far higher or lower it likely indicates a problem.
- `"connected"` - The total number of nodes your node is currently connected to.
- `"timestamp"` - The time at which this topology snapshot was taken.
- `"nnLowWatermark"` - ???
- `"depth"` -
- `"reachability"`
- `"networkAvailability"`
  After the first section are 32 sections, one for each bin. At the front of each of these sections is a summary of information about the respective bin followed two list, one of disconnected peers and the other of connected peers. Let's take a look at bin_10 as an example:

```json
...
  "bin_10": {
      "population": 3, // The total number of peers in this bin including both connected and disconnected peers.
      "connected": 2, // Number of connected peers
      "disconnectedPeers": [ //List of all disconnected peers
        {
          "address": "3e06e4667260c761f1b6a8539a99621c1af1f945e97667376c13b5f84984bcbc",
          "metrics": {
            "lastSeenTimestamp": 1707426772,
            "sessionConnectionRetry": 2,
            "connectionTotalDuration": 104619,
            "sessionConnectionDuration": 72,
            "sessionConnectionDirection": "outbound",
            "latencyEWMA": 849,
            "reachability": "Public",
            "healthy": true
          }
        }
      ],
      "connectedPeers": [ // List of all connected peers
        {
          "address": "3e09deca28d24a4c6dab9350dd0fb27a2333f03120b9f92f0ac0fd245707c9e3",
          "metrics": {
            "lastSeenTimestamp": 1707426766,
            "sessionConnectionRetry": 2,
            "connectionTotalDuration": 105059,
            "sessionConnectionDuration": 33,
            "sessionConnectionDirection": "outbound",
            "latencyEWMA": 899,
            "reachability": "Public",
            "healthy": true
          }
        },
        {
          "address": "3e1cdf7b1072fcde264c75f70635b9c1e9c1623eab2de55a0380f17b07751955",
          "metrics": {
            "lastSeenTimestamp": 1707426741,
            "sessionConnectionRetry": 1,
            "connectionTotalDuration": 109216,
            "sessionConnectionDuration": 59,
            "sessionConnectionDirection": "outbound",
            "latencyEWMA": 948,
            "reachability": "Public",
            "healthy": true
          }
        }
      ]
    },
```

### _/node_

    This endpoint returns info about options related to your node type and also displays your current node type.

    ```bash
    curl -s http://localhost:1633/node | jq
    ```
    ```bash
    {
      "beeMode": "full",
      "chequebookEnabled": true,
      "swapEnabled": true
    }
    ```
    * `"beeMode"` - The mode of your node, can be `"full"`, `"light"`, or `"ultraLight"`.
    * `"chequebookEnabled"` - Whether or not your node's `chequebook-enable` option is set to `true`.
    * `"swapEnabled"` - Whether or not your node's `swap-enable` option is set to `true`.

    If your node is not operating in the correct mode, this can help you to diagnose whether you have set your options correctly.

### _/rchash_

Calling the `/rchash` endpoint triggers the generation of a reserve commitment hash, which is used in the [redistribution game](/docs/concepts/incentives/redistribution-game), and will report the amount of time it took to generate the hash. This is useful for getting a performance benchmark to ensure that your node's hardware is sufficient.

The `/rchash` endpoint has 3 parameters: `depth` and `anchor_01` and `anchor_02`. For both of the anchor parameters, you should use the first 4 digits from your node's overlay address (which you can find from the `/addresses` endpoint). For depth, you should use the current storage depth of your node which you can find using the `/status` endpoint (storage depth is equivalent to the `storageRadius` value returned from `/status`):

```
/rchash/{depth}/{anchor_01}/{anchor_02}
```

:::info anchor parameter details

- The anchor parameters must match the prefix bits of the node's overlay address up to at least the current storage depth (with each hex digit equal to 4 bits).
- The anchor parameters also must have an even number of digits.

Therefore you can use the first four digits of your node's overlay address since it will work for depths up to depth 16, which will not be approached unless the depth increases up to depth 17, which is not likely to happen in the near future. If it does increase to depth 17, then the first 6 overlay digits should be used.
:::

```bash
sudo curl -sX GET http://localhost:1633/rchash/10/1e20/1e20 | jq
```

It should not take much longer than 6 minutes at most for results to be returned:

```bash
{
  "hash": "a1d6e1700dff0c5259029c8a58904251855911eb298b45fab4b0c26d4de0fa5f",
  "proofs": {
    "proof1": {
      "proofSegments": [
        "00001099542c8958b2a3a0f7803ee0c07a34de91dd34d4567bf16b367686a387",
        "75b51d98c9346a9cb2120bd3a10c0febb43a1d900ff1e4247a29e46974cd4e02",
        "d3e289937aa2e2a49e27c9b525580002cbcab4e4d4a972a97ef8e272e0bafebb",
        "2215b68c492114e3b4f78080f924bf9cdb3b7da9d5f4e811631f50d2ac45af14",
        "c115c46c632fcefea88def9a241fb37acaf222bbbd74a4435d3cde48b6210436",
        "0eb01ebfc9ed27500cd4dfc979272d1f0913cc9f66540d7e8005811109e1cf2d",
        "887c22bd8750d34016ac3c66b5ff102dacdd73f6b014e710b51e8022af9a1968"
      ],
      "proveSegment": "1e09adea90a0935fdcd03b5b8193b95767f2f40308d4ea29323845de1270cdd7",
      "proofSegments2": [
        "74656e656666656b74000c4469676974616c20636f707900052e63616c6c000d",
        "b5d06699c843bc0cc39abd993553865a01c1366174e9e87539ac491dce0a3b77",
        "1513dd5a4a3633ecf370ef6f27af99b0f23d73d4773e504314bbf2c690887999",
        "a16bdb02905d76eb17965eca8a32f303f0a6129a125c30767a097f80a9e30ded",
        "ec01379e1a2ae10fd4e0ee4e4fc4b0d3f6f9ac55038ced386fc7f4294d4bb103",
        "456189653d7fdcd3f1ef6b27d00cb8700cd3abb59a1d60b08b61af1d77c74a13",
        "4f5384191e60b983ee26b2d42bdcce3fba7041f4cde80b78d808a893d2dc8e79"
      ],
      "proveSegment2": "436f6465706167652039353000054f72626f7400064d50432d4843000c536569",
      "chunkSpan": 4096,
      "proofSegments3": [
        "74656e656666656b74000c4469676974616c20636f707900052e63616c6c000d",
        "d99a7bf301af141ce9e1acb909c631975c2f7bc60984bbfb9d4097415d2ea723",
        "f77855426ebacca7833d31b559f8ef0f5909849e8604856ef7c2344e536252bd",
        "273b259e9fb51852d8bbfe409c648605f60fcdaec7fff4ec20120d6f5898f412",
        "7d0e379980bf75f76e9a128eb05fdfb8d9db7da40060eff06629ee48465b3a70",
        "1be5155660d125ee125cceaf3ea81313321b9ac36aefb6b7b3b35c4a4c0fd5b2",
        "8b1a6990327bfb0738025c57b6f06d7255e49b89a2d5f4bed1aed202816843ae"
      ],
      "postageProof": {
        "signature": "d13f85223e1fe47f3689e31471f322d61b10b25231db5319819c8f68548fdc907acf17a0341ba81114d1667693b1e6f35f37c50b4365474baa03f8b233566fa51b",
        "postageId": "9af1b37f38d4af75dbee9ba713b95bcc6d03e1c759ac774ec8bd4864e68d2c03",
        "index": "1e0900000e13",
        "timeStamp": "17ca57481e839b79"
      },
      "socProof": null
    },
    "proof2": {
      "proofSegments": [
        "0000336f98de7901000b056246b5b105611a56a9e45c452fe65288751f90de2e",
        "55f65e720d9d53fc7f6d73c87e9a7fd1ad73343b96bd4fc679fdb0d188fe872c",
        "7a753e4aa045cc1ce5834a669755bd43f6344ade1890f0080e5c50c23489f098",
        "4081e7d623e62c35a17a1167f7f295a26cbf7705a443ed06dfb8fcc416dfc2e4",
        "306564763d673ca314aaa072d79ac4b888b69aad80da3ca387aafd2752afce8e",
        "0eb01ebfc9ed27500cd4dfc979272d1f0913cc9f66540d7e8005811109e1cf2d",
        "887c22bd8750d34016ac3c66b5ff102dacdd73f6b014e710b51e8022af9a1968"
      ],
      "proveSegment": "1e1b2adb768e9d05021c514914292acc56a8273a4f2ea196f50add7c3e5bff45",
      "proofSegments2": [
        "cf19af651fdc9ae2aab3c240860b11156691c6561e0291633cc00aea719c3db5",
        "91afb2735a5f002134c31fbf4f1b0c2b99b47ea1d84da75a3d8da597e1d27034",
        "9b9e75f236b13ae250a9cf3ae59655054583166c6f6379871c5f2c9b1fdcdb1d",
        "4cad5229b21c2f1ca0e5dcf34e61253709dcef1d11e9346c9daf6d6a7a7d6ea2",
        "ca49b26daaaffb849045d55f25f3a05c942f8b5844c71a8fcc2172c1ba5e2f86",
        "3bbf39b295aa2a01a8b6de929d85cb3fc375bddeaf70ac0ad9ff17f64bca3892",
        "3a8343e42db633b929473ccbd90e36320cf0bb6a19474c8a3a01dbbc9ebc99ad"
      ],
      "proveSegment2": "4cb106406558d74c99ed246e22f73f55abd96f633c1e00025511b98930faafb4",
      "chunkSpan": 4096,
      "proofSegments3": [
        "cf19af651fdc9ae2aab3c240860b11156691c6561e0291633cc00aea719c3db5",
        "8d4665ad0c7ba6a24fe285613057e6289c3b341887fedba9a4190d236f62f31e",
        "5fdf57b41fb119b29275edcb55a00b7a0ea5119e1ad78dc97ea615236f9112cb",
        "32bc2d3b400b6837117f7720af512913d85388cafe365e06f96172531a59622f",
        "d393a441a2dcabfe6ae7a5da201badcd1da877f33092e1f823b816fa826879ac",
        "557d9f4ca3655f6bc06f4d13b87da1ddc5c2f7b6eb60b043a7a5a989bb4b2b5f",
        "f81cf8329547445e4427e91f8682ef05e482c7b8219adc34066bd9942aec6d05"
      ],
      "postageProof": {
        "signature": "efae95247cae78db875194de1851a6866456f9cb881079676289bd3b72f27af53d783441bc1992f5439e1815179afb05b1a12c7a86b36b41fd411fa29529f1981c",
        "postageId": "922b7387276adeea51df5aeb80f597a62c7c236b4387f7f06ced2883762f8fef",
        "index": "1e1b0000006a",
        "timeStamp": "17ad450de2b34084"
      },
      "socProof": null
    },
    "proofLast": {
      "proofSegments": [
        "00003dbfed587dc866be862e65ebf4da79b54397c6dee4cefa2754e8e0ad2f37",
        "0b6fa766b60f03b9efec530413243318f277adccdd974d316cf7b52b5f072681",
        "1629dad7244d3efcfb5bddcc794cbd42eb5d6e81ecd47711a7ef08ea9c80c405",
        "4081e7d623e62c35a17a1167f7f295a26cbf7705a443ed06dfb8fcc416dfc2e4",
        "306564763d673ca314aaa072d79ac4b888b69aad80da3ca387aafd2752afce8e",
        "0eb01ebfc9ed27500cd4dfc979272d1f0913cc9f66540d7e8005811109e1cf2d",
        "887c22bd8750d34016ac3c66b5ff102dacdd73f6b014e710b51e8022af9a1968"
      ],
      "proveSegment": "1e2fbcf6d9a79786ce93970536eb4f18ec148cefd2f5e7e15a203f99ad2b1d0a",
      "proofSegments2": [
        "1a00fb8c44cf0f65d12d2aed8fcde9517684dd7afc6e3a488cc5a882a92e8cbf",
        "5936e75eea7818850651a3e50fab30826766f9777c9099b045c1e6454355111c",
        "0ed1fbde9703139dcf9aa8c008c64ac8a85fea8fcfb237986c4652844b49c318",
        "f1904237d00d889455695fe711b3f4c7a6e0c40b5e85ae66eefcbf34631b9802",
        "19a4dcef98735175b96c675fb1ffd5c59a2fa1a3515724d3d3c2849fa2b75672",
        "1449ca5c3b4a701dde9a4dd38f8db6216a5fe8c72e8701dd20263622b3375c77",
        "1160eaff3f42ad8c90f8d95fdee20daa5f1fece84947e265878be9db543c4651"
      ],
      "proveSegment2": "57b3f11f2fe2ba52d0897d936901db164d04fa5a9684009a17a39aa718b2b1c8",
      "chunkSpan": 4096,
      "proofSegments3": [
        "1a00fb8c44cf0f65d12d2aed8fcde9517684dd7afc6e3a488cc5a882a92e8cbf",
        "5f99524dc31e7bf8ca75011c74f739b30bcb855c3ad7e46701d30dc6901cb563",
        "0fc1e41481afc47ae70056c780149a50ab9c48630eb44607b9100dfda0ba9dfe",
        "e4d7e158b9a446e8f2bad51acf26fc1874013dc23352471ba945fbec0a97d910",
        "dddc32e1f2ac86aa4bb711bb6b812afa5d6ca9405d04816e2dcfefd303eb8294",
        "e5384c29f838e85b71bc2eb5cb14d59201e8e1997a9779d193a6a9b327637693",
        "6f08720c155d4a53fd94ed403fb4476a4c7e6058bf021df23efdfdca1746027d"
      ],
      "postageProof": {
        "signature": "8039da66b4b6e45d5559d0d060f968ccfa11b052234b5645d90d396a0069b6ff40aecb62e9c45d13a68dda0b1fd68dee1de575e28f72c3766d64e824e7c9f9211b",
        "postageId": "9af1b37f38d4af75dbee9ba713b95bcc6d03e1c759ac774ec8bd4864e68d2c03",
        "index": "1e2f000038a3",
        "timeStamp": "17dd336464f4f0bc"
      },
      "socProof": null
    }
  },
  "durationSeconds": 1191.652640831
}
```

If the `Time` value is much longer than 6 minutes then it likely means that the node's hardware performance is not sufficient. In the example above, it took almost 20 minutes to complete, indicating that the hardware is not sufficient. In such cases, consider upgrading to use faster memory or processor.

If while running the `/rchash` command there is an evictions related error such as the one below, try running the call to the `/rchash` endpoint again.

```
error: "level"="error" "logger"="node/storageincentives" "msg"="make sample" "error"="sampler: failed creating sample: sampler stopped due to ongoing evictions"
```

While evictions are a normal part of Bee's standard operation, the event of an eviction will interrupt the sampler process.

### _/health_

The `/health` endpoint provides a quick status check for your Bee node which simply indicates whether the node is operating or not. It is often used in tools like Docker and Kubernetes.

```bash
    curl -s http://localhost:1633/health | jq
```

```bash
    {
      "status": "ok",
      "version": "2.6.0-d0aa8b93",
      "apiVersion": "7.3.0"
    }
```

- `"status"` - "ok" if the server is responsive.
- `"version"` - The version of your Bee node. You can find latest version by checking the [Bee github repo](https://github.com/ethersphere/bee).
- `"apiVersion"`

---

## Cashing Out

There are two different types of cashing out. The first type is cashing out xBZZ rewards earned from staking and providing storage services (this method also allows for withdrawal of the native xDAI token). The second type is for the withdrawal of xBZZ earned through bandwidth incentives (SWAP). Both types are explained below: 

## Withdrawing xBZZ Rewards and Native xDAI

You can withdraw xBZZ rewards or native xDAI tokens using the `/wallet/withdraw/` endpoint. The endpoint allows you to withdraw tokens to any address which you have whitelisted using the `withdrawal-addresses-whitelist` option. 

You can specify either a single address:

```yaml
# withdrawal target addresses
withdrawal-addresses-whitelist: 0x62d04588e282849d391ebff1b9884cb921b9b94a
```

Or an array of addresses:

```yaml
# withdrawal target addresses
withdrawal-addresses-whitelist: [ 0x62d04588e282849d391ebff1b9884cb921b9b94a, 0x71a5aae026e2ab87612a5824d492a095e7d790bf ]
```

The token you desire to withdraw is specified in the path directly:

```bash
http://localhost:1633/wallet/withdraw/{coin}
```
For `coin`, you can use the value `NativeToken` for xDAI or `BZZ` for xBZZ. 

The `amount` query parameter is used to specify how much of the token you wish to withdraw. The value should be specified in the lowest denomination for each token (wei for xDAI and PLUR for xBZZ).

The `address` query parameter is used to specify the target address to withdraw to. This address must be specified using the `withdrawal-addresses-whitelist` option in your configuration.

The following command will withdraw a single PLUR of xBZZ to address 0x62d04588e282849d391ebff1b9884cb921b9b94a:

```bash
curl -X POST "http://localhost:1633/wallet/withdraw/bzz?amount=1&address=0x62d04588e282849d391ebff1b9884cb921b9b94a"
```

## Cashing out Cheques (SWAP)

As your Bee forwards and serves chunks to its peers, it is rewarded in
xBZZ in the form of cheques. Once these cheques accumulate sufficient
value, you may _cash them out_ using Bee's API. This process transfers
money from your peer's chequebooks into your own, which you can then
withdraw to your wallet to do with as you please!

:::important
Do **not** cash out your cheques too regularly! Once a week is more
than sufficient! Besides the transaction costs, this prevents and
relieves unnecessary congestion on the blockchain. üí©
:::

:::info
Learn more about how SWAP and other accounting protocols work by reading
[The Book of Swarm](https://www.ethswarm.org/the-book-of-swarm-2.pdf).
:::

Bee contains a rich set of features to enable you to query the current accounting state of your node. First, let's query our node's current balance by sending a POST request to the balances endpoint.

```bash
curl localhost:1633/chequebook/balance | jq
```

```json
{
  "totalBalance": "10000000",
  "availableBalance": "9640360"
}
```

It is also possible to examine your per-peer balances.

```bash
curl localhost:1633/balances | jq
```

```json
{
  "balances": [
    //...
    {
      "peer": "d0bf001e05014fa036af97f3d226bee253d2b147f540b6c2210947e5b7b409af",
      "balance": "-85420"
    },
    {
      "peer": "f1e2872581de18bdc68060dc8edd3aa96368eb341e915aba86b450486b105a47",
      "balance": "-75990"
    }
    //...
  ]
}
```

In Swarm, these per-peer balances represent trustful agreements between nodes. Tokens only actually change hands when a node settles a cheque. This can either be triggered manually or when a certain threshold is reached with a peer. In this case, a settlement takes place. You may view these using the settlements endpoint.

More info can be found by using the chequebook API.

```bash
curl localhost:1633/settlements| jq
```

```json
{
  "totalreceived": "718030",
  "totalsent": "0",
  "settlements": [
    //...
    {
      "peer": "dce1833609db868e7611145b48224c061ea57fd14e784a278f2469f355292ca6",
      "received": "8987000000000",
      "sent": "0"
    }
    //...
  ]
}
```

More information about the current received or sent cheques can also be found using the chequebook api.

```bash
curl localhost:1633/chequebook/cheque | jq
```

```json
{
  "lastcheques": [
    {
      "peer": "dce1833609db868e7611145b48224c061ea57fd14e784a278f2469f355292ca6",
      "lastreceived": {
        "beneficiary": "0x21b26864067deb88e2d5cdca512167815f2910d3",
        "chequebook": "0x4A373Db93ba54cab999e2C757bF5ca0356B42a3f",
        "payout": "8987000000000"
      },
      "lastsent": null
    }
    //...
  ]
}
```

As our node's participation in the network increases, we will begin to see more and more of these balances arriving. In the case that we have _received_ a settlement from another peer, we can ask our node to perform the relevant transactions on the blockchain, and cash our earnings out.

To do this, we simply POST the relevant peer's address to the `cashout` endpoint.

```bash
curl -X POST http://localhost:1633/chequebook/cashout/d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b
```

```json
{
  "transactionHash": "0xba7b500e21fc0dc0d7163c13bb5fea235d4eb769d342e9c007f51ab8512a9a82"
}
```

You may check the status of your transaction using the [xDAI
Blockscout](https://blockscout.com/xdai/mainnet).

Finally, we can now see the status of the cashout transaction by sending a GET request to the same URL.

```bash
curl http://localhost:1633/chequebook/cashout/d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b | jq
```

```json
{
  "peer": "d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b",
  "chequebook": "0xae315a9adf0920ba4f3353e2f011031ca701d247",
  "cumulativePayout": "179160",
  "beneficiary": "0x21b26864067deb88e2d5cdca512167815f2910d3",
  "transactionHash": "0xba7b500e21fc0dc0d7163c13bb5fea235d4eb769d342e9c007f51ab8512a9a82",
  "result": {
    "recipient": "0x312fe7fde9e0768337c9b3e3462189ea6f9f9066",
    "lastPayout": "179160",
    "bounced": false
  }
}
```

Success, we earned our first xBZZ! üêù

Now we have earned tokens, to withdraw our xBZZ from the chequebook contract back into our node's own wallet, we simply POST a request to the chequebook withdraw endpoint.

```bash
curl -X POST http://localhost:1633/chequebook/withdraw\?amount\=1000 | jq
```

And conversely, if we have used more services than we have provided, we may deposit extra xBZZ into the chequebook contract by sending a POST request to the deposit endpoint.

```bash
curl -X POST http://localhost:1633/chequebook/deposit\?amount\=1000 | jq
```

```json
{
  "transactionHash": "0xedc80ebc89e6d719e617a50c6900c3dd5dc2f283e1b8c447b9065d7c8280484a"
}
```

You may then use [Blockscout](https://blockscout.com/xdai/mainnet) to
track your transaction and make sure it completed successfully.

## Managing uncashed cheques

For the Bee process, the final step of earning xBZZ is cashing a
cheque. It is worth noting that a cheque is not yet actual xBZZ. In
Bee, a cheque, just like a real cheque, is a promise to hand over
money upon request. In real life, you would present the cheque to a
bank. In swarm life, we present the cheque to a smart-contract.

Holding on to a swap-cheque is risky; it is possible that the owner of
the chequebook has issued cheques worth more xBZZ than is contained in
their chequebook contract. For this reason, it is important to cash
out your cheques every so often.

With the set of API endpoints, as offered by Bee, it is possible to
develop a script that fully manages the uncashed cheques for you. As
an example, we offer you a [very basic
script](https://gist.github.com/ralph-pichler/3b5ccd7a5c5cd0500e6428752b37e975#file-cashout-sh),
where you can manually cash out all cheques with a worth above a
certain value. To use the script:

1. Download and save the script:

```bash
wget -O cashout.sh https://gist.githubusercontent.com/ralph-pichler/3b5ccd7a5c5cd0500e6428752b37e975/raw/cashout.sh
```

2. Make the file executable:

```bash
chmod +x cashout.sh
```

3. List all uncashed cheques and cash out your cheques above a certain value:

- List:

  ```bash
  ./cashout.sh
  ```

  :::info
  If running ./cashout.sh returns nothing, you currently have no uncashed cheques.
  :::

- Cashout all cheques:

  ```bash
  ./cashout.sh cashout-all
  ```

:::info
Are you a Windows-user who is willing to help us? We are currently
missing a simple cashout script for Windows. Please see the
[issue](https://github.com/ethersphere/bee/issues/1092).
:::

:::info
You can find the officially deployed smart-contract by the Swarm team
in the [swap-swear-and-swindle
repository](https://github.com/ethersphere/swap-swear-and-swindle).
:::

---

## Configuration

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

## Configuration Methods and Priority

There are three configuration methods, each with a different priority level. Configuration is processed in the following ascending order of preference:

1. Command Line Flags 
2. Environment Variables
3. YAML Configuration File

:::info
All three methods may be used when running Bee using `bee start`. 

However when Bee is started as a service with tools like `systemctl` or `brew services`, only the YAML configuration file is supported by default.
:::

### Command Line Arguments

Run `bee start --help` in your terminal to list the available command-line arguments:

```bash
Ethereum Swarm Bee

Usage:
  bee [command]

Available Commands:
  start       Start a Swarm node
  dev         Start a Swarm node in development mode
  init        Initialise a Swarm node
  deploy      Deploy and fund the chequebook contract
  version     Print version number
  db          Perform basic DB related operations
  split       Split a file into chunks
  printconfig Print default or provided configuration in yaml format
  help        Help about any command
  completion  Generate the autocompletion script for the specified shell

Flags:
      --config string   config file (default is $HOME/.bee.yaml)
  -h, --help            help for bee

Use "bee [command] --help" for more information about a command.
root@noah-bee:~# docker exec bee-1 bee start --help
Start a Swarm node

Usage:
  bee start [flags]

Flags:
      --allow-private-cidrs                      allow to advertise private CIDRs to the public network
      --api-addr string                          HTTP API listen address (default "127.0.0.1:1633")
      --block-time uint                          chain block time (default 5)
      --blockchain-rpc-endpoint string           rpc blockchain endpoint
      --bootnode strings                         initial nodes to connect to (default [/dnsaddr/mainnet.ethswarm.org])
      --bootnode-mode                            cause the node to always accept incoming connections
      --cache-capacity uint                      cache capacity in chunks, multiply by 4096 to get approximate capacity in bytes (default 1000000)
      --cache-retrieval                          enable forwarded content caching (default true)
      --chequebook-enable                        enable chequebook (default true)
      --cors-allowed-origins strings             origins with CORS headers enabled
      --data-dir string                          data directory (default "/home/bee/.bee")
      --db-block-cache-capacity uint             size of block cache of the database in bytes (default 33554432)
      --db-disable-seeks-compaction              disables db compactions triggered by seeks (default true)
      --db-open-files-limit uint                 number of open files allowed by database (default 200)
      --db-write-buffer-size uint                size of the database write buffer in bytes (default 33554432)
      --full-node                                cause the node to start in full mode
  -h, --help                                     help for start
      --mainnet                                  triggers connect to main net bootnodes. (default true)
      --minimum-storage-radius uint              minimum radius storage threshold
      --nat-addr string                          NAT exposed address
      --neighborhood-suggester string            suggester for target neighborhood (default "https://api.swarmscan.io/v1/network/neighborhoods/suggestion")
      --network-id uint                          ID of the Swarm network (default 1)
      --p2p-addr string                          P2P listen address (default ":1634")
      --p2p-ws-enable                            enable P2P WebSocket transport
      --password string                          password for decrypting keys
      --password-file string                     path to a file that contains password for decrypting keys
      --payment-early-percent int                percentage below the peers payment threshold when we initiate settlement (default 50)
      --payment-threshold string                 threshold in BZZ where you expect to get paid from your peers (default "13500000")
      --payment-tolerance-percent int            excess debt above payment threshold in percentages where you disconnect from your peer (default 25)
      --postage-stamp-address string             postage stamp contract address
      --postage-stamp-start-block uint           postage stamp contract start block number
      --pprof-mutex                              enable pprof mutex profile
      --pprof-profile                            enable pprof block profile
      --price-oracle-address string              price oracle contract address
      --redistribution-address string            redistribution contract address
      --reserve-capacity-doubling int            reserve capacity doubling
      --resolver-options strings                 ENS compatible API endpoint for a TLD and with contract address, can be repeated, format [tld:][contract-addr@]url
      --resync                                   forces the node to resync postage contract data
      --skip-postage-snapshot                    skip postage snapshot
      --staking-address string                   staking contract address
      --statestore-cache-capacity uint           lru memory caching capacity in number of statestore entries (default 100000)
      --static-nodes strings                     protect nodes from getting kicked out on bootnode
      --storage-incentives-enable                enable storage incentives feature (default true)
      --swap-enable                              enable swap
      --swap-factory-address string              swap factory addresses
      --swap-initial-deposit string              initial deposit if deploying a new chequebook (default "0")
      --target-neighborhood string               neighborhood to target in binary format (ex: 111111001) for mining the initial overlay
      --tracing-enable                           enable tracing
      --tracing-endpoint string                  endpoint to send tracing data (default "127.0.0.1:6831")
      --tracing-host string                      host to send tracing data
      --tracing-port string                      port to send tracing data
      --tracing-service-name string              service name identifier for tracing (default "bee")
      --transaction-debug-mode                   skips the gas estimate step for contract transactions
      --use-postage-snapshot                     bootstrap node using postage snapshot from the network
      --verbosity string                         log verbosity level 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace (default "info")
      --warmup-time duration                     maximum node warmup duration; proceeds when stable or after this time (default 5m0s)
      --welcome-message string                   send a welcome message string during handshakes
      --withdrawal-addresses-whitelist strings   withdrawal target addresses

Global Flags:
      --config string   config file (default is $HOME/.bee.yaml)
```

### Environment variables

Bee configuration can also be set using environment variables.

Environment variables are set as variables in your operating system's
session or systemd configuration file. To set an environment variable,
type the following in your terminal session.

```bash
export VARIABLE_NAME=variableValue
```

Verify that it is correctly set by running `echo $VARIABLE_NAME`.

All available configuration options are available as `BEE` prefixed,
capitalised, and underscored environment variables, e.g. `--api-addr` becomes `BEE_API_ADDR`.

### YAML configuration file

You can view the default contents of the `bee.yaml` configuration file using the `bee printconfig` command:

```bash
bee printconfig
```

```yaml
# allow to advertise private CIDRs to the public network
allow-private-cidrs: false
# HTTP API listen address
api-addr: :1633
# chain block time
block-time: "5"
# rpc blockchain endpoint
blockchain-rpc-endpoint: https://xdai.fairdatasociety.org
# initial nodes to connect to
bootnode:
- /dnsaddr/mainnet.ethswarm.org
# cause the node to always accept incoming connections
bootnode-mode: false
# cache capacity in chunks, multiply by 4096 to get approximate capacity in bytes
cache-capacity: "1000000"
# enable forwarded content caching
cache-retrieval: true
# enable chequebook
chequebook-enable: true
# config file (default is $HOME/.bee.yaml)
config: /home/bee/.bee.yaml
# origins with CORS headers enabled
cors-allowed-origins: []
# data directory
data-dir: /home/bee/.bee
# size of block cache of the database in bytes
db-block-cache-capacity: "33554432"
# disables db compactions triggered by seeks
db-disable-seeks-compaction: true
# number of open files allowed by database
db-open-files-limit: "200"
# size of the database write buffer in bytes
db-write-buffer-size: "33554432"
# cause the node to start in full mode
full-node: "true"
# help for printconfig
help: false
# triggers connect to main net bootnodes.
mainnet: "true"
# minimum radius storage threshold
minimum-storage-radius: "0"
# NAT exposed address
nat-addr: ""
# suggester for target neighborhood
neighborhood-suggester: https://api.swarmscan.io/v1/network/neighborhoods/suggestion
# ID of the Swarm network
network-id: "1"
# P2P listen address
p2p-addr: :1634
# enable P2P WebSocket transport
p2p-ws-enable: false
# password for decrypting keys
password: 427067e9514e93613b861fef5561c6
# path to a file that contains password for decrypting keys
password-file: ""
# percentage below the peers payment threshold when we initiate settlement
payment-early-percent: 50
# threshold in BZZ where you expect to get paid from your peers
payment-threshold: "13500000"
# excess debt above payment threshold in percentages where you disconnect from your peer
payment-tolerance-percent: 25
# postage stamp contract address
postage-stamp-address: ""
# postage stamp contract start block number
postage-stamp-start-block: "0"
# enable pprof mutex profile
pprof-mutex: false
# enable pprof block profile
pprof-profile: false
# price oracle contract address
price-oracle-address: ""
# redistribution contract address
redistribution-address: ""
# reserve capacity doubling
reserve-capacity-doubling: "1"
# ENS compatible API endpoint for a TLD and with contract address, can be repeated, format [tld:][contract-addr@]url
resolver-options: []
# forces the node to resync postage contract data
resync: false
# skip postage snapshot
skip-postage-snapshot: false
# staking contract address
staking-address: ""
# lru memory caching capacity in number of statestore entries
statestore-cache-capacity: "100000"
# protect nodes from getting kicked out on bootnode
static-nodes: []
# enable storage incentives feature
storage-incentives-enable: true
# enable swap
swap-enable: "true"
# swap factory addresses
swap-factory-address: ""
# initial deposit if deploying a new chequebook
swap-initial-deposit: "0"
# neighborhood to target in binary format (ex: 111111001) for mining the initial overlay
target-neighborhood: ""
# enable tracing
tracing-enable: false
# endpoint to send tracing data
tracing-endpoint: 127.0.0.1:6831
# host to send tracing data
tracing-host: ""
# port to send tracing data
tracing-port: ""
# service name identifier for tracing
tracing-service-name: bee
# skips the gas estimate step for contract transactions
transaction-debug-mode: false
# bootstrap node using postage snapshot from the network
use-postage-snapshot: false
# log verbosity level 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace
verbosity: "4"
# maximum node warmup duration; proceeds when stable or after this time
warmup-time: 5m0s
# send a welcome message string during handshakes
welcome-message: ""
# withdrawal target addresses
withdrawal-addresses-whitelist: []
```

:::info
Note that depending on whether Bee is started directly with the `bee start` command or started as a service with `systemctl` / `brew services`, the default directory for the YAML configuration file (shown in the `config` option above) [will be different](/docs/bee/working-with-bee/configuration). 
:::

To change your node's configuration, simply edit the YAML file and restart Bee: 

<Tabs
defaultValue="linux"
values={[
{label: 'Linux', value: 'linux'},
{label: 'MacOS arm64 (Apple Silicon)', value: 'macos-arm64'},
{label: 'MacOS amd64 (Intel)', value: 'macos-amd64'},
]}>
<TabItem value="linux">

Open the config file for editing:

```bash
sudo vi /etc/bee/bee.yaml
```
After saving your changes, restart your node:

```bash
sudo systemctl restart bee
```

</TabItem>

<TabItem value="macos-arm64">

Open the config file for editing:

```bash
sudo vi /opt/homebrew/etc/swarm-bee/bee.yaml
```

After saving your changes, restart your node:

```bash
brew services restart swarm-bee
```

</TabItem>

<TabItem value="macos-amd64">

Open the config file for editing:

```bash
sudo vi /usr/local/etc/swarm-bee/bee.yaml
```

After saving your changes, restart your node:

```bash
brew services restart swarm-bee
```
</TabItem>

</Tabs>

## Manually generating YAML config file for *bee start*

No YAML file is generated during installation when using the [shell script install method](/docs/bee/installation/shell-script-install), so you must generate one if you wish to use a YAML file to specify your configuration options. To do this you can use the `bee printconfig` command to print out a set of default options and save it to a new file in the default location:

```bash
bee printconfig &> $HOME/.bee.yaml
```

:::info
Note that `bee printconfig` prints the default configuration for your node, not the current configuration including any changes.
:::

When using `bee.yaml` with the `bee start` command, make sure to use the `--config` flag to specify the location of your configuration file.

## Node Types

There are three node types which each offer varying levels of functionality - ***full***, ***light***, and ***ultra-light***. You can configure your node to run as any of these three types by setting the related options within your configuration. 

For a deeper dive into each node type and its features and limitations, refer to the [Node Types](/docs/bee/working-with-bee/node-types) page.

### How to Set Node Type

There are three relevant options which are used to set your node type: `full-node`, `swap-enable`, and `blockchain-rpc-endpoint`. The required option values for each node type are outlined below:

| Node Type        | `full-node` | `swap-enable` | `blockchain-rpc-endpoint` | Functionality                                                                      |
| ---------------- | ----------- | ------------- | ------------------------- | ---------------------------------------------------------------------------------- |
| Full Node        | `true`      | `true`        | Required                  | Full functionality, including uploads, downloads, and Swarm network participation. |
| Light Node       | `false`     | `true`        | Required                  | Supports uploading and downloading only.                                               |
| Ultra-Light Node | `false`     | `false`       | Not required              | Free-tier downloads only.                                                          |

## Configuration Examples

Bee nodes can be configured using command-line flags, environment variables, or a YAML configuration file:

<Tabs
  defaultValue="full"
  values={[ 
    {label: 'Full', value: 'full'},
    {label: 'Light', value: 'light'},
    {label: 'Ultra-Light', value: 'ultra-light'},
  ]}>

  <TabItem value="full">

  ### Full Node Configuration

  #### Using Command-Line Arguments
  ```bash
  bee start \
    --password mypassword \
    --full-node \
    --swap-enable \
    --blockchain-rpc-endpoint https://xdai.fairdatasociety.org
  ```

  #### Using Environment Variables
  ```bash
  export BEE_PASSWORD="mypassword"
  export BEE_FULL_NODE="true"
  export BEE_SWAP_ENABLE="true"
  export BEE_BLOCKCHAIN_RPC_ENDPOINT="https://xdai.fairdatasociety.org"
  bee start
  ```

  #### Using YAML Configuration
  ```yaml
  password: mypassword
  full-node: true
  swap-enable: true
  blockchain-rpc-endpoint: "https://xdai.fairdatasociety.org"
  ```
  </TabItem>

  <TabItem value="light">

  ### Light Node Configuration

  #### Using Command-Line Arguments
  ```bash
  bee start \
    --password mypassword \
    --swap-enable \
    --blockchain-rpc-endpoint https://xdai.fairdatasociety.org
  ```

  #### Using Environment Variables
  ```bash
  export BEE_PASSWORD="mypassword"
  export BEE_SWAP_ENABLE="true"
  export BEE_BLOCKCHAIN_RPC_ENDPOINT="https://xdai.fairdatasociety.org"
  bee start
  ```

  #### Using YAML Configuration
  ```yaml
  password: mypassword
  swap-enable: true
  blockchain-rpc-endpoint: "https://xdai.fairdatasociety.org"
  ```
  </TabItem>

  <TabItem value="ultra-light">

  ### Ultra-Light Node Configuration

  #### Using Command-Line Arguments
  ```bash
  bee start \
    --password mypassword
  ```

  #### Using Environment Variables
  ```bash
  export BEE_PASSWORD="mypassword"
  bee start
  ```

  #### Using YAML Configuration
  ```yaml
  password: mypassword
  ```
  </TabItem>

</Tabs>

## Default Data and Config Directories

Depending on the operating system and startup method used, the default directories for your node will differ:

### Bee Service Default Directories (Package Manager Install)

When installed using a package manager, Bee is set up to run as a service with default data and configuration  directories set up automatically during the installation. The examples below include default directories for Linux and macOS. You can find the complete details of default directories for different operating systems in the `bee.yaml` files included in the [packaging folder of the Bee repo](https://github.com/ethersphere/bee/tree/master/packaging). 

<Tabs
defaultValue="linux"
values={[
{label: 'Linux', value: 'linux'},
{label: 'MacOS arm64 (Apple Silicon)', value: 'macos-arm64'},
{label: 'MacOS amd64 (Intel)', value: 'macos-amd64'},
]}>
<TabItem value="linux">

The default data folder and config file locations:

```yaml
data-dir: /var/lib/bee
config: /etc/bee/bee.yaml
```

</TabItem>

<TabItem value="macos-arm64">

The default data folder and config file locations:

```yaml
data-dir: /opt/homebrew/var/lib/swarm-bee
config: /opt/homebrew/etc/swarm-bee/bee.yaml
```

</TabItem>

<TabItem value="macos-amd64">

The default data folder and config file locations:

```yaml
data-dir: /usr/local/var/lib/swarm-bee/
config: /usr/local/etc/swarm-bee/bee.yaml
```

</TabItem>
</Tabs>

### Shell Script Install Default Directories

For all operating systems, the default data and config directories for the `bee start` startup method can be found using the `bee printconfig` command:

This will print out a complete default Bee node configuration file to the terminal, the `config` and `data-dir` values show the default directories for your system: 

```yaml
config: /root/.bee.yaml
data-dir: /root/.bee
```

:::info
The default directories for your system may differ from the example above, so make sure to run the `bee printconfig` command to view the default directories for your system.
:::

## Create Password

A password is required for all modes, and can either be set directly in text through the `password` configuration option or alternatively a file can be used by setting the `password-file` option to the path where your password file is located.

## Setting Blockchain RPC endpoint

:::warning
A RPC endpoint for *a full archival Gnosis Chain node is required* since a Bee node must sync all data starting from when the [postage stamp smart contract was created](https://gnosisscan.io/tx/0x3427deb106b30a7d23f7ce9d2465f2d83945948c5aeddba55337c318fb56ec25). 

The free RPC endpoint offered by the Fair Data Society (https://xdai.fairdatasociety.org) will work since it is a full archival node, but running Bee with other public free RPC endpoints from non-archive nodes will result in the `storage: not found` error.

If you do encounter the `storage: not found` error, update your RPC endpoint to one for a full archival node, and restart your node with the `resync` option set to `true`. 
:::

Full and light Bee nodes require a Gnosis Chain RPC endpoint in order to sync blockchain data and issue transactions (not required for ultra-light nodes). 

To set your RPC endpoint, specify it with the `blockchain-rpc-endpoint` value, which is set to an empty string by default.

```yaml
## bee.yaml
blockchain-rpc-endpoint: https://xdai.fairdatasociety.org
```

We recommend you [run your own Gnosis Chain node](https://docs.gnosischain.com/node/), but you may also consider using a paid RPC endpoint provider such as [GetBlock](https://getblock.io/).

### RPC Providers

While we recommend running your own Gnosis Chain node for your RPC endpoint, you may wish to use a third party provider instead.

For a comprehensive list of RPC providers, refer to the [Gnosis Chain documentation](https://docs.gnosischain.com/tools/RPC%20Providers/). The list includes both free and paid RPC providers (refer to [warning above](#setting-blockchain-rpc-endpoint) about free RPC providers).

For running a light node or for testing out a single full node you can use the free RPC endpoint provided by the Fair Data Society: `https://xdai.fairdatasociety.org`.

## Configuring Swap Initial Deposit (Optional)

When running your Bee node with SWAP enabled for the first time, your node will deploy a 'chequebook' contract using the canonical factory contract which is deployed by Swarm. Once the chequebook is deployed, Bee will (optionally) deposit a certain amount of xBZZ in the chequebook contract so that it can pay other nodes in return for their services. The amount of xBZZ transferred to the chequebook is set by the `swap-initial-deposit` configuration setting (it may be left at the default value of zero or commented out). 

## NAT address

Swarm is all about sharing and storing chunks of data. To enable other Bees (also known as _peers_) to connect to your Bee, you must
broadcast your public IP address in order to ensure that Bee is reachable on the correct p2p port (default `1634`). We recommend that you [manually configure your external IP and check
connectivity](/docs/bee/installation/connectivity) to ensure your Bee is able to receive connections from other peers.

First, determine your public IP address:

```bash
curl icanhazip.com
```

```bash
123.123.123.123
```

Then configure your node, including your p2p port (default 1634).

```yaml
## bee.yaml
nat-addr: "123.123.123.123:1634"
```

## ENS Resolution (Optional)

The [ENS](https://ens.domains/) domain resolution system is used to host websites on Bee, and in order to use this your Bee must be connected to a mainnet Ethereum blockchain node. We recommend you run your own ethereum node. An option for resource restricted devices is geth+nimbus and a guide can be found [here](https://ethereum-on-arm-documentation.readthedocs.io/en/latest/). Other options include [dappnode](https://dappnode.io/), [nicenode](https://www.nicenode.xyz/), [stereum](https://stereum.net/) and [avado](https://ava.do/). 

If you do not wish to run your own Ethereum node, you may use a blockchain RPC service provider such as [Infura](https://infura.io). After signing up for Infura, simply set your `--resolver-options` to `https://mainnet.infura.io/v3/your-api-key`.

```yaml
## bee.yaml
resolver-options: ["https://mainnet.infura.io/v3/<<your-api-key>>"]
```

## Sepolia Testnet Configuration 

In order to operate a Bee node on the Sepolia testnet, you need to change `mainnet` to `false`, and provide a valid Sepolia testnet RPC endpoint through the `blockchain-rpc-endpoint` option.

Here is an example of a full configuration for a testnet full node:

```yaml
data-dir: /home/username/bee/sepolia # Specified an alternate "data-dir" for our testnet node data
full-node: true
mainnet: false # Changed to "false"
password: password
blockchain-rpc-endpoint: wss://sepolia.infura.io/ws/v3/<API-KEY> # Replaced the Gnosis Chain RPC with a Sepolia testnet RPC endpoint
swap-enable: true
verbosity: 5
welcome-message: "welcome-from-the-hive"
warmup-time: 30s
```

### Funding Testnet Node

Make sure to fund your node with Sepolia ETH rather than xDAI to pay for gas on the Sepolia testnet. There are many public faucets you can use to obtain Sepolia ETH, such as [this one from Infura](https://www.infura.io/faucet/sepolia). 

To get Sepolia BZZ (sBZZ) you can use [this Uniswap market](https://app.uniswap.org/swap?outputCurrency=0x543dDb01Ba47acB11de34891cD86B675F04840db&inputCurrency=ETH), just make sure that you've switched to the Sepolia network in your browser wallet.

---

## Introduction

In this section we cover everything a node operator needs to know about working with Bee:

## Configuration

Learn how to [configure your node](/docs/bee/working-with-bee/configuration), and the details behind all the configuration options Bee provides.

## Bee API

Access the HTTP API directly for [detailed information about your Bee](/docs/bee/working-with-bee/bee-api).

## Logs and Files

Find out where Bee stores your [logs and files](/docs/bee/working-with-bee/logs-and-files).

## Swarm CLI

You can use the [`swarm-cli`command line tool](/docs/bee/working-with-bee/swarm-cli) to monitor your Bee's status, cash out your cheques, upload data to the swarm and more!

## Cashing Out

Get your cheques cashed and bank your xBZZ. [See this guide](/docs/bee/working-with-bee/cashing-out) to receiving payments from your peers.

## Monitoring and Metrics

There is a lot going on inside Bee, we provide tools and metrics to help you [find out what's going on](/docs/bee/working-with-bee/monitoring).

## Backups

[Keep your important data safe](/docs/bee/working-with-bee/backups), Bee stores important state and key information on your hardrive, make sure you keep a secure copy in case of disaster.

## Upgrading

Find out how to [keep your Bee up to date](/docs/bee/working-with-bee/upgrading-bee) with the latest and greatest releases, and make sure you're tuned into our release announcements.

## Uninstalling Bee

We hope you won't need to remove Bee. If you do, please let us know if you had issues so we can help resolve them for our beloved network. Here's the guide to [removing Bee from your system](/docs/bee/working-with-bee/uninstalling-bee).

---

## Logging in Bee


This section provides an overview of logging in Bee, including log locations, exporting logs, managing verbosity levels, and using fine-grained control for specific loggers.

:::info
Bee uses a structured logging format compatible with popular tools such as [Grafana](https://grafana.com/) and [Elasticsearch](https://www.elastic.co/elasticsearch). Structured logging helps streamline log analysis and management by organizing data into machine-readable formats, enabling easy integration with monitoring and debugging tools.
:::

:::warning  
Bee logs can be verbose by default, potentially consuming significant disk space over time. Consider implementing [log rotation](https://en.wikipedia.org/wiki/Log_rotation) to prevent excessive disk utilization.
:::

## Log Locations

### **Linux (Package Manager Installation)**
When installed via a package manager (e.g., `APT`, `RPM`), Bee runs as a **systemd service**, and logs are managed by the system journal, **journalctl**.

View logs with:
```bash
journalctl --lines=100 --follow --unit bee
```

Export all logs as JSON:
```bash
journalctl --unit bee --output=json > bee-logs.json
```

Export logs for a specific time range:
```bash
journalctl --since "1 hour ago" --output=json --unit bee > bee-logs.json
```

Learn more about `journalctl` usage and filtering logs in this [tutorial](https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs) from DigitalOcean.

### **macOS (Homebrew Installation)**

For a Homebrew installation on macOS, logs are saved to:
```bash
/usr/local/var/log/swarm-bee/bee.log
```

View logs in real-time:
```bash
tail -f /usr/local/var/log/swarm-bee/bee.log
```

### **Docker**

Docker saves **stdout** and **stderr** output as JSON files by default. Logs are stored in:

```
/var/lib/docker/containers/<container-id>/<container-id>-json.log
```

View logs in real time:
```bash
docker logs -f <container-name>
```

Export logs to a file:
```bash
docker logs <container-name> > bee-logs.json
```

Export logs for a specific time range:
```bash
docker logs --since "30m" <container-name> > bee-logs.json
```

See [Docker documentation](https://docs.docker.com/reference/cli/docker/container/logs/) for additional options.

### **Shell Script**

For a shell script-installed Bee started using `bee start`, logs are sent to **stdout** and **stderr** by default, which means they will appear in the terminal. They are **not saved to disk by default**.

To save logs to a file, redirect **stdout** and **stderr**:

```bash
bee start --password <password> > bee.log 2>&1 &
```

View recent logs and follow for updates:
```bash
tail -f bee.log
```

## Logging Levels

Bee supports the following log levels:

| Level       | Description                        |
|-------------|------------------------------------|
| `0=silent` | No logs.                           |
| `1=error`  | Critical errors only.              |
| `2=warn`   | Warnings and errors.               |
| `3=info`   | General operational logs (default).|
| `4=debug`  | Detailed diagnostic logs.          |
| `5=trace`  | Highly granular logs for debugging.|

### Behavior of Log Levels

Log levels are cumulative: setting a higher verbosity includes all lower levels.  
For example, `debug` will output logs at `debug`, `info`, `warn`, and `error` levels.

## Setting Verbosity

The general verbosity level can be set using the `verbosity` configuration option in order to display all log messages up to the selected level of verbosity. 

### **YAML Config File**
Set the `verbosity` parameter in `config.yaml`:

```yaml
# Log verbosity: 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace
verbosity: debug
```

### **Command Line Flag**
Set the verbosity level (0-5) when starting Bee:

```bash
bee start --verbosity debug
```

### **Environment Variable**
Set `BEE_VERBOSITY` before starting Bee:

```bash
export BEE_VERBOSITY=debug
bee start
```

## Fine-Grained Logging Control

Bee allows fine-grained control of logging levels for specific subsystems using the **`/loggers` API endpoint**. This enables adjustments without restarting the node.

### **1. Retrieving Loggers List**

Retrieve a list of active loggers and their verbosity levels:

```bash
curl http://localhost:1633/loggers | jq
```

The list of loggers includes detailed entries for each subsystem. Below is an example for the `node/api` logger:

```json
{
  "logger": "node/api",
  "verbosity": "info",
  "subsystem": "node/api[0][]>>824634474528",
  "id": "bm9kZS9hcGlbMF1bXT4-ODI0NjM0NDc0NTI4"
}
```

- **`id`**: The Base64-encoded identifier used to adjust the logger‚Äôs verbosity.
- **`verbosity`**: The current log level.

### **2. Adjusting Logger Verbosity**

You can dynamically adjust the log level for any logger without restarting Bee.

**Syntax**:
```bash
curl -X PUT http://localhost:1633/loggers/<id>/<verbosity>
```

- **`<id>`**: The Base64-encoded logger name retrieved from `/loggers`.
- **`<verbosity>`**: Desired log level (`none`, `error`, `warn`, `info`, `debug`, `trace`).

**Example**: Set `node/api` to `debug`:
```bash
curl -X PUT http://localhost:1633/loggers/bm9kZS9hcGlbMF1bXT4-ODI0NjM0NDc0NTI4/debug
```

### Log Level Behavior Note

Log levels are cumulative. When a logger is set to a specific level, it will include all log messages at that level and below.  

For example:
- Setting a logger to `info` will show logs at `info`, `warn`, and `error`.
- Logs at higher levels (`debug` and `trace`) will **not** be displayed.

---

## Monitoring Your Node

Your Bee node is equipped with tools to help you understand what your Bee has been up to!

Navigate to `http://localhost:1633/metrics`.

This is the current state of Bee's metrics as they stand at this moment.

In order to use these metrics and view, we need to keep a record of these metrics over time.

To do this we will use [Prometheus](https://prometheus.io/docs/introduction/overview/). Simply install, configure as follows, and restart!

For Ubuntu and other Debian based Linux distributions install using `apt`:

```bash
sudo apt install prometheus
```

And configure `localhost:1633` as a `target` in the `static_configs`.

```bash
sudo vim /etc/prometheus/prometheus.yml
```

```yaml
static_configs:
  - targets: ["localhost:9090", "localhost:1633"]
```

Navigate to [http://localhost:9090](http://localhost:9090) to see the Prometheus user interface.

Now that our metrics are being scraped into Prometheus' database, we can use it as a data source which is used by [Grafana](https://grafana.com/oss/grafana/) to display the metrics as a time series graph on the dashboard.

Type `bee_` in the 'expression' or 'metrics' field in Prometheus or Grafana respectively to see the list of metrics available. Here's a few to get you started!

```
rate(bee_swap_cheques_received[1d])
rate(bee_swap_cheques_sent[1d])
rate(bee_swap_cheques_rejected[1d])
```

Share your creations in the [#node-operators](https://discord.gg/X3ph5yGRFU) channel of our Discord server!

---

## Node Types

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Bee nodes can operate in three different modes depending on the user's needs, ranging from full-featured nodes that contribute to the network and earn incentives to lightweight modes that allow for basic interaction with minimal resource requirements. This guide outlines the three primary node types ‚Äî **_Full_**, **_Light_**, and **_Ultra-Light_** ‚Äî along with their configurations, capabilities, and limitations.

Choosing the right node type depends on your goals, whether it's participating in the Swarm network as a storage provider, developing applications that use Swarm's decentralized storage and messaging, or simply exploring the technology with minimal setup.

## Node Types Overview

Bee can operate in different modes, each tailored to specific use cases:

| Feature                                                                 | Full Node | Light Node | Ultra-Light Node |
| ----------------------------------------------------------------------- | --------- | ---------- | ---------------- |
| Free tier [downloads](/docs/develop/upload-and-download)                                                     | ‚úÖ        | ‚úÖ         | ‚úÖ               |
| [Uploading](/docs/develop/upload-and-download) (Can purchase [postage stamp batches](/docs/develop/tools-and-features/buy-a-stamp-batch))                          | ‚úÖ        | ‚úÖ         | ‚ùå               |
| Can exceed free tier downloads                                          | ‚úÖ        | ‚úÖ         | ‚ùå               |
| Storage sharing                                                         | ‚úÖ        | ‚ùå         | ‚ùå               |
| [Storage incentives](/docs/bee/working-with-bee/staking/)               | ‚úÖ        | ‚ùå         | ‚ùå               |
| [Bandwidth incentives](/docs/concepts/incentives/bandwidth-incentives/) | ‚úÖ        | ‚ùå         | ‚ùå               |
| [PSS messaging](/docs/develop/tools-and-features/pss)                   | ‚úÖ        | ‚ùå         | ‚ùå               |

## Full Node

Full nodes are the most feature-rich nodes in the Swarm network. They provide full upload and download capabilities, store and serve data, and participate in storage and bandwidth incentives. Running a full node requires more system resources, but it allows users to fully engage with and support the network.

Full nodes are ideal for users who want to contribute to the Swarm network and earn incentives, as well as developers who require access to all Bee features including messaging features such as PSS and GSOC.

### Recommended Specifications

:::warning
Full nodes require significant system resources, including storage and bandwidth. Additionally, they must be connected to the blockchain to participate in incentives. 

If you intend to participate in the redistribution game to earn storage incentives, you should test your setup using [the `/rchash` endpoint](/docs/bee/working-with-bee/bee-api/#rchash) in order to ensure that your hardware is sufficient. Participation in the redistribution game requires a process with high computational and memory requirements, along with significant bandwidth usage.
:::

Requires significant storage and processing power compared to other node types:

- **Processor**: Recent 2 GHz dual-core.
- **RAM**: 8 GB.
- **Storage**: 30 GB SSD (HDD not recommended).
- **Internet**: High-speed and stable connection.

### Configuration

To run Bee as a full node, set:

- `full-node: true`
- `swap-enable: true`
- `blockchain-rpc-endpoint` to a valid Gnosis Chain RPC URL

**Key characteristics:**

- Can upload and download data.
- Can purchase and manage postage stamp batches in order to pay for uploading data.
- Can share disk space with the network and store chunks from Swarm uploaders.
- Can participate in the storage incentives system by sharing disk space for a chance to earn xBZZ.
- Can participate in the bandwidth incentives system and earn xBZZ by forwarding chunks for other nodes.
- Requires a Gnosis Chain RPC endpoint for blockchain connectivity.
- Supports full PSS messaging and GSOC.

## Light Node

Light nodes provide a balance between functionality and resource efficiency. They can upload and download data but do not participate in chunk forwarding or storage for other nodes.

Light nodes are suited for users who want to interact with Swarm without the overhead of running a full node. They can serve the needs of developers who need to access Swarm's download / upload features but do not need advanced messaging features such as PSS and GSOC which are available only in full nodes.

Light node operators cannot earn xBZZ by participating in Swarm's incentives systems, as they do not participate in chunk forwarding or storage but only consume services, paying xBZZ for downloading data from full nodes and buying postage stamp batches for uploading data.

:::info
Light nodes do not benefit from plausible deniability when requesting data from the network. They are always the originator of requests.
:::

### Recommended Specifications

No specific hardware is required to run a light node. It can run well on practically any commercially available computer released in recent years, including lightweight single-board computers such as [Raspberry Pi](https://en.wikipedia.org/wiki/Raspberry_Pi). Your downloads / uploads may be limited by your network speed, however, so if you plan on interacting extensively with the Swarm network, you should take your connection speed into consideration.

### Configuration

To run Bee as a light node, set:

- `full-node: false`
- `swap-enable: true`
- `blockchain-rpc-endpoint` to a valid Gnosis Chain RPC URL

**Key characteristics:**

- Can upload and download data.
- Can purchase and manage postage stamp batches in order to pay for uploading data.
- Requires a Gnosis Chain RPC endpoint for blockchain connectivity.

**Limitations:**

- Cannot share disk space with the network and store chunks from Swarm uploaders.
- Cannot earn xBZZ by staking xBZZ and participating in the storage incentive system.
- Cannot earn xBZZ by participating in the bandwidth incentives system.
- Can send PSS messages but ***cannot*** receive them.
- Can send outgoing GSOC updates but ***cannot*** receive them.

## Ultra-Light Node

Ultra-light nodes allow users to run a node without requiring a blockchain RPC endpoint. These nodes can download data within the free consumption threshold set by full nodes (this threshold may vary since it is [configurable](/docs/bee/working-with-bee/configuration) by full node operators using the `payment-tolerance-percent` and `payment-threshold` options).

Ultra-light nodes are designed for users who want to access the Swarm network with minimal resource requirements. These nodes can download data within the free consumption threshold but do not support uploads and cannot earn xBZZ by participating in Swarm's incentives systems. 

As with light nodes, your node's download speed will be limited by your network speed (however this may be less important of a consideration given that an ultra-light node is restricted to downloading within free tier limits anyway).

:::warning
As with light nodes, ultra-light nodes do not benefit from plausible deniability when requesting data from the network.

When running without a blockchain connection, [bandwidth incentive payments (SWAP)](/docs/concepts/incentives/bandwidth-incentives/) cannot be made, increasing the risk of being blocklisted by other peers for exceeding their free-tier download limits.
:::

### Recommended Specifications

As with the light node, there are no specific requirements to run an ultra-light node, and it will run on practically any commercially available hardware from recent years.   

### Configuration

Bee will start in ultra-light mode by default, but in order to explicitly configure your node to run as an ultra-light node, use the following options:

- Set `full-node: false`
- Set `blockchain-rpc-endpoint` to an empty string "" (or comment it out / remove it).
- Set `swap-enable: false`

**Key characteristics:**

- Can download limited amounts of data.

**Limitations:**
- Cannot upload data.
- Cannot purchase postage stamps.
- Cannot share disk space with the network and store chunks from Swarm uploaders.
- Cannot earn xBZZ by staking xBZZ and participating in the storage incentive system.
- Cannot earn xBZZ by participating in the bandwidth incentives system.
- Cannot use PSS or GSOC for sending or receiving.

---

## Staking

## Quickstart Guide

This guide will walk you through **staking xBZZ** and participating in the **redistribution game** to earn storage incentives. 

:::warning
Staking requires a fully synced full node and a minimum of 10 xBZZ. See detailed [staking requirements](/docs/bee/working-with-bee/staking#requirements) below.
:::

### Prerequisites   

- A small amount of xDAI to pay transaction fees - ~0.01 xDAI is enough to start
- At least 10 xBZZ to deposit as non-refundable stake
- A fully synced full Bee node

:::tip
If you don't already have xDAI or xBZZ, you will need to [get some](/docs/bee/installation/fund-your-node#getting-tokens).
:::

### Step 1: Fund Your Node with xDAI and xBZZ

Your node needs **xDAI** to pay for transaction fees on Gnosis Chain, and also needs **xBZZ** to deposit as stake. 

First, find your node's address using:

```bash
swarm-cli addresses
```

This will print your node's various addresses. The one you need to fund is `Ethereum`

:::tip
The `Ethereum` term here refers to an Ethereum style address on Gnosis Chain. Do not send funds to the address on the Ethereum chain itself.
:::

```bash
Node Addresses
------------------------------------------------------------------------------------------------------------------
Ethereum: 9a73f283cd9211b96b5ec63f7a81a0ddc847cd93
...
```

Then, use the following command to check how much is required:

```bash
swarm-cli status
```

At the bottom of the results printed to the terminal you will find the `Redistribution` section. From there you will see the `Minimum gas funds` item. That value is the minimum amount required to participate in a *single redistribution round*.

:::tip
If you plan on operating your node for an extended period, you will want to deposit quite a bit more than the minimum. You can start with **0.01 xDAI** to cover fees for the next few weeks/months of active staking, and then monitor actual usage and top-up when needed.
:::

```
Redistribution
Reward: 0.0000000000000000
Has sufficient funds: true
Fully synced: true
Frozen: false
Last selected round: 263526
Last played round: 0
Last won round: 0
Minimum gas funds: 0.000000000326250000
```

Finally, send the required xDAI and xBZZ to the address you got from `swarm-cli addresses`. 

You will need to send at least 10 xBZZ to get started staking.

:::tip
Send **20 xBZZ** if using the [reserve doubling](#reserve-doubling) feature.
:::

### Step 2: Stake xBZZ 

Once your node has xDAI, stake **at least 10 xBZZ** (this is non-refundable).

You can use the following `swarm-cli` command to stake 10 xBZZ:

:::info
The deposit amount is specified in [PLUR](/docs/references/glossary/#plur)
:::

```bash
swarm-cli stake --deposit 100000000000000000
```

After a moment, the staking transaction will complete. Then you can check that the transaction was successful:

```bash
swarm-cli stake
```

```bash
Staked xBZZ: 10
```

:::tip
**Optional:** Stake **20 xBZZ** if using the [reserve doubling](#reserve-doubling) feature.
:::

### Step 3: Check Status

After staking you should [check your node's status](/docs/bee/working-with-bee/staking#check-status) to make sure it is fully synced, fully funded, and operating properly. 

### Step 4: Monitor & Maximize Rewards

‚úÖ Make sure you are using a stable Gnosis Chain [RPC endpoint](/docs/bee/working-with-bee/configuration#setting-blockchain-rpc-endpoint).  
‚úÖ [Check your node's status](/docs/bee/working-with-bee/staking#check-redistribution-status) to ensure it's operating properly.
‚úÖ [Check `/rchash`](/docs/bee/working-with-bee/bee-api#rchash) to ensure your node's performance is sufficient.

## Staking Overview

To earn storage incentives by participating in the [redistribution game](/docs/concepts/incentives/redistribution-game), full nodes must first deposit a minimum of 10 xBZZ as ***non-refundable*** stake. xDAI is also required to pay for ongoing Gnosis Chain transactions related to the redistribution game.

:::danger
Only stake your xBZZ if you intend to participate as a full node, as withdrawals are not possible.
:::

### Requirements

- A [full node](/docs/bee/working-with-bee/node-types) - see full node [recommend specs](/docs/bee/working-with-bee/node-types#recommended-specifications).  
- A [high-performance RPC endpoint](/docs/bee/working-with-bee/configuration#setting-blockchain-rpc-endpoint) connection to Gnosis Chain.
- A minimum of 10 xBZZ to be used as ***non-refundable*** stake (the requirement is increased if [reserve doubling](/docs/bee/working-with-bee/staking#reserve-doubling) is used).

 

### Check Status

Use the `/redistributionstate` endpoint of the API to get more information about the redistribution status of the node.

```bash
curl -X GET http://localhost:1633/redistributionstate | jq
```

```bash
{ 
  "minimumFunds": "18750000000000000",
  "hasSufficientFunds": true,
  "isFrozen": false,
  "isFullySynced": true,
  "phase": "commit",
  "round": 176319,
  "lastWonRound": 176024,
  "lastPlayedRound": 176182,
  "lastFrozenRound": 0,
  "block": 26800488,
  "reward": "10479124611072000",
  "fees": "30166618102500000"
}
```
* `"minimumFunds": <integer>` - The minimum xDAI needed to play a single round of the redistribution game (the unit is 1e-18 xDAI).
* `"hasSufficientFunds": <bool>` - Shows whether the node has enough xDAI balance to submit at least five storage incentives redistribution related transactions.  If `false` the node will not be permitted to participate in next round.
* `"isFrozen": <bool>` - Shows node frozen status.
* `"isFullySynced": <bool>` - Shows whether node's localstore has completed full historical syncing with all connected peers.
* `"phase": <string>` - Current phase of [redistribution game](/docs/concepts/incentives/redistribution-game) (`commit`, `reveal`, or `claim`).
* `"round": <integer>` - Current round of redistribution game. The round number is determined by dividing the current Gnosis Chain block height by the number of blocks in one round. One round takes 152 blocks, so using the "block" output from the example above we can confirm that the round number is 176319 (block 26800488 / 152 blocks = round 176319).   
* `"lastWonRound": <integer>` - Number of round last won by this node.
* `"lastPlayedRound": <integer>` - Number of the last round where node's neighborhood was selected to participate in redistribution game.
* `"lastFrozenRound": <integer>` The number the round when node was last frozen. 
* `"block": <integer>` - Gnosis block of the current redistribution game.
* `"reward": <string (BigInt)>` - Record of total reward received in [PLUR](/docs/references/glossary#plur).
* `"fees": <string (BigInt)>` - Record of total spent in 1E-18 xDAI on all redistribution related transactions.

:::warning
Do not shut down or update your node during an active redistribution round as it may cause them to lose out on winnings or become frozen. To see if your node is playing the current round, check if `lastPlayedRound` equals `round` in the output from the [`/redistributionstate` endpoint](/api/#tag/RedistributionState/paths/~1redistributionstate/get).
:::

You should also check the [`/status`](/api/#tag/Node-Status/paths/~1status/get) endpoint: 

```bash
curl -s  http://localhost:1633/status | jq
```

```bash
{
  "peer": "da7e5cc3ed9a46b6e7491d3bf738535d98112641380cbed2e9ddfe4cf4fc01c4",
  "proximity": 0,
  "beeMode": "full",
  "reserveSize": 3747532,
  "pullsyncRate": 0,
  "storageRadius": 10,
  "connectedPeers": 183,
  "neighborhoodSize": 12,
  "batchCommitment": 133828050944,
  "isReachable": true
}
```
**Expected values for a healthy staking node:** 

* `"beeMode": full`
* `"pullsyncRate": 0`
* `"isReachable": true`

:::info
If your node is not operating properly such as getting frozen or not participating in any rounds, see the [troubleshooting section](#troubleshooting).
:::

## Partial Stake Withdrawals

If the price of xBZZ rises significantly and provides excess collateral, a partial withdrawal will be allowed down to the minimum required stake:

### Check for withdrawable stake

```bash
curl http://localhost:1633/stake/withdrawable | jq
```
If there is any stake available for withdrawal, the amount will be displayed in PLUR:

```bash
{
  "withdrawableStake": "18411"
}
```

### Withdraw available stake

If there is any stake available for withdrawal, you can withdraw it using the `DELETE` method on `/stake/withdrawable`:

```bash
curl -X DELETE http://localhost:1633/stake/withdrawable
```

## Reserve Doubling

The reserve doubling feature enables nodes to store chunks from a neighboring "sister" area, effectively increasing their reserve capacity twofold. By maintaining chunks from this sister neighborhood, a node becomes eligible to join the redistribution game whenever the sister neighborhood is chosen, effectively doubling its chances of participating.

Although reserve doubling demands twice the disk storage and increases bandwidth usage for chunk syncing (with no additional bandwidth needed for chunk forwarding), its effect on CPU and RAM consumption remains minimal. This feature provides node operators with greater flexibility to optimize their nodes, aiming to achieve a higher reward-to-resource usage ratio.

### Step by Step Guide

In order to double a node's reserve which has previously been operating without doubling, the `reserve-capacity-doubling` option must be updated from the default of `0` to `1` and restarted. There is also an increase in the xBZZ stake requirement from the minimum of 10 xBZZ to 20 xBZZ. 

#### **Step 1**: Set `reserve-capacity-doubling` to `1`.

The reserve doubling feature can be enabled by setting the new `reserve-capacity-doubling` config option to `1`  using the [configuration method](/docs/bee/working-with-bee/configuration#configuration-methods-and-priority) of your choice. 

#### **Step 2**: Stake at least 20 xBZZ 

For doubling the reserve of a node which was previously operating which already has 10 xBZZ staked, simply stake an additional 10 xBZZ for a total of 20 xBZZ stake:

:::info
As always, ensure you properly convert the stake parameter to PLUR, where 1 PLUR equals 1e-16 xBZZ.
:::

```bash
curl -X POST localhost:1633/stake/100000000000000000
```

Or for a new node with zero staked xBZZ, the entire 20 xBZZ can be staked at once:

```bash
curl -X POST localhost:1633/stake/200000000000000000
```

We can use the `GET /stake` endpoint to confirm the total stake for our node:

```bash
curl -s  http://localhost:1633/stake | jq
```

```bash
{
  "stakedAmount": "200000000000000000"
}
```

#### **Step 3**: Restart node 

After ensuring the node has at least 20 xBZZ staked and the `reserve-capacity-doubling` option has been set to `1`, restart the node.

After restarting your node, it should then begin syncing chunks from its sister neighborhood. 

The `/status/neighborhoods` endpoint can be used to confirm that the node has doubled its reserve and is now syncing with its sister neighborhood:

```bash
{
  "neighborhoods": [
    {
      "neighborhood": "01111101011",
      "reserveSizeWithinRadius": 1148351,
      "proximity": 10
    },
    {
      "neighborhood": "01111101010",
      "reserveSizeWithinRadius": 1147423,
      "proximity": 11
    }
  ]
}
```

The output should list both your original and sister neighborhood.

We can also check the `/status` endpoint to confirm our node is syncing new chunks:

```bash
curl -s  http://localhost:1633/status | jq
```

```bash
{
  "overlay": "be177e61b13b1caa20690311a909bd674a3c1ef5f00d60414f261856a8ad5c30",
  "proximity": 256,
  "beeMode": "full",
  "reserveSize": 4192792,
  "reserveSizeWithinRadius": 2295023,
  "pullsyncRate": 1.3033333333333332,
  "storageRadius": 10,
  "connectedPeers": 18,
  "neighborhoodSize": 1,
  "batchCommitment": 388104192,
  "isReachable": true,
  "lastSyncedBlock": 6982430
}
```

We can see that the `pullsyncRate` value is above zero, meaning that our node is currently syncing chunks, as expected.
 
## Maximize rewards

There are two main factors which determine the chances for a staking node to win a reward ‚Äî neighborhood selection and stake density. Both of these should be considered together before starting up a Bee node for the first time. See the [incentives page](/docs/concepts/incentives/redistribution-game) for more context.

### Neighborhood selection 

By default when running a Bee node for the first time the node will use the [neighborhood suggestion tool](https://api.swarmscan.io/v1/network/neighborhoods/suggestion) from Swarmscan to find an optimal [neighborhood](/docs/concepts/DISC/neighborhoods). While it is possible to manually choose a neighborhood using the `target-neighborhood` config option, we recommend not to do so as the suggestion tool will pick neighborhoods in order to maximize node earnings and network health. [Learn more](/docs/bee/installation/set-target-neighborhood). 

### Stake density

Stake density is defined as:

$$
\text{stake density} = \text{staked xBZZ} \times {2}^\text{storageDepth}
$$
  
*To learn more about stake density and the mechanics of the incentives system, see the [incentives page](/docs/concepts/incentives/redistribution-game).*

Stake density determines the weighted chances of nodes within a neighborhood of winning rewards. The chance of winning within a neighborhood corresponds to stake density. Stake density can be increased by depositing more xBZZ as stake (note that stake withdrawals are not currently possible, so any staked xBZZ is not currently recoverable). 

Generally speaking, the minimum required stake of 10 xBZZ is sufficient, and rewards can be better maximized by operating more nodes over a greater range of neighborhoods rather than increasing stake. However this may not be true for all node operators depending on how many different neighborhoods they operate nodes in, and it also may change as network dynamics continue to evolve (join the `#node-operators` [Discord channel](https://discord.com/channels/799027393297514537/811553590170353685) to stay up to date with the latest discussions about staking and network dynamics).

## Neighborhood Hopping

:::warning 
There is a 2 round delay (with 152 Gnosis Chain blocks per redistribution game round) every time a node's neighborhood or stake is changed before it can participate in the redistribution game, moreover a node must fully sync the chunks from its new neighborhood before it can participate in the redistribution game, so hopping too frequently is not advised. 
:::

You can use the config option `target-neighborhood` to switch your node over to a new neighborhood. You may wish to use this option if your node's neighborhood becomes overpopulated. 

### Checking neighborhood population

For a quick check of your node's neighborhood population, we can use the `/status` endpoint: 

```bash
curl -s http://localhost:1633/status | jq
{
  "peer": "e7b5c1aac67693268fdec98d097a8ccee1aabcf58e26c4512ea888256d0e6dff",
  "proximity": 0,
  "beeMode": "full",
  "reserveSize": 1055543,
  "reserveSizeWithinRadius": 1039749,
  "pullsyncRate": 42.67013868148148,
  "storageRadius": 11,
  "connectedPeers": 140,
  "neighborhoodSize": 6,
  "batchCommitment": 74463051776,
  "isReachable": false
}
```

Here we can see that at the current `storageRadius` of 11, our node is in a neighborhood with size 6 from the `neighborhoodSize` value.

Using the [Swarmscan neighborhoods tool](https://swarmscan.io/neighborhoods) we can see there are many neighborhoods with fewer nodes, so it would benefit us to move to less populated neighborhood:

![](/img/staking-swarmscan.png)
 
While you might be tempted to simply pick one of these less populated neighborhoods, it is best practice to use the neighborhood suggester API instead, since it will help to prevent too many node operators rapidly moving to the same underpopulated neighborhoods, and also since the suggester takes a look at the next depth down to make sure that even in case of a neighborhood split, your node will end up in the smaller neighborhood. 

```bash
curl -s https://api.swarmscan.io/v1/network/neighborhoods/suggestion
```

Copy the binary number returned from the API:

```bash
{"neighborhood":"01100011110"}
```

Use the binary number you just copied and set it as a string value for the `target-neighborhood` option in your config. 

```bash
## bee.yaml
target-neighborhood: "01100011110"
```

## Stake Migration

If a new Bee release includes an updated staking contract, then you will be required to migrate your node's stake in order to continue normal operation. The stake migration process consists of the following steps:

1. Withdraw xBZZ
2. Stop node 
3. Update and restart
4. Re-stake to the new contract

### Step 1: Withdraw xBZZ 

When a new version of Bee is released with an updated staking contract, the previous staking contract will be disabled, and stake withdrawals will be enabled. 

Once the contract is disabled, stake can be withdrawn by calling the `/stake` endpoint with the `DELETE` method:

```bash
curl -X DELETE http://localhost:1633/stake
```

This command will withdraw all stake from the node to the node‚Äôs Gnosis Chain address.

Confirm that the stake was withdrawn:

```bash
 curl -s  http://localhost:1633/stake | jq
```
The value for `stakedAmount` should now be zero:
```
{
  "stakedAmount": "0"
}
```
### Step 2: Stop node

This step will vary depending on how the node was set up:

```bash
sudo systemctl stop bee
```

or

```
docker compose down
```

or

```
docker stop <container_name_or_id>
```

etc.

### Step 3: Update and restart

:::danger
Before every Bee client upgrade, it is best practice to ALWAYS take a full [backup](/docs/bee/working-with-bee/backups) of your node.
:::

After withdrawing stake and stopping the node, update to the newest version of Bee. After updating, restart the node.

You can use the `/health` endpoint to confirm your current Bee version:

```bash
curl -s http://localhost:1633/health | jq
```

To confirm a successful update, check that the value for the `"version"` field in the results corresponds to the version number of the [latest](https://github.com/ethersphere/bee/releases/latest) Bee release. 

For example, if the latest version was 2.6.0, it would look like this:

```bash
{
  "status": "ok",
  "version": "2.6.0-d0aa8b93",
  "apiVersion": "7.3.0"
}
```

*Make sure to check the [latest](https://github.com/ethersphere/bee/releases/latest) version number yourself, as the versions shown in examples in this guide may not always be up to date with the latest.*

### Step 4: Re-stake xBZZ

After upgrading to the latest version and restarting, xBZZ should be re-staked into the new staking contract so that the node can continue to participate in the redistribution game. 

To stake the minimum required 10 xBZZ:

:::tip
Make sure to modify to the correct staking amount in case your node is using [reserve doubling](/docs/bee/working-with-bee/staking#reserve-doubling).
:::

```bash
curl -X POST localhost:1633/stake/100000000000000000
```

Confirm that the staking transaction was successful:

```bash
curl -s  http://localhost:1633/stake | jq
```
The expected output after staking the minimum of 10 xBZZ:

```bash
{
  "stakedAmount": "100000000000000000"
}
```

Congratulations! You have performed a successful stake migration and your node will now continue to operate as normal.

## Troubleshooting

In this section we cover several commonly seen issues encountered for staking nodes participating in the redistribution game. If you don't see your issue covered here or require additional guidance, check out the `#node-operators` [Discord channel](https://discord.com/channels/799027393297514537/811553590170353685) where you will find support from other node operators and community members.

### Frozen node

A node will be frozen when the reserve commitment hash it submits in its [`commit` transaction](/docs/concepts/incentives/redistribution-game) does not match the correct hash. The reserve commitment hash is used as proof that a node is storing the chunks it is responsible for. It will not be able to play in the redistribution game during the freezing period. See the  [penalties](/docs/concepts/incentives/redistribution-game) section for more information.

#### Check frozen status

You can check your node's frozen status using the `/redistributionstate` endpoint:

```bash
curl -X GET http://localhost:1633/redistributionstate | jq
```

```bash
{ 
  "minimumFunds": "18750000000000000",
  "hasSufficientFunds": true,
  "isFrozen": false,
  "isFullySynced": true,
  "phase": "commit",
  "round": 176319,
  "lastWonRound": 176024,
  "lastPlayedRound": 176182,
  "lastFrozenRound": 0,
  "block": 26800488,
  "reward": "10479124611072000",
  "fees": "30166618102500000"
}
```

The relevant fields here are `isFrozen` and `lastFrozenRound`, which respectively indicate whether the node is currently frozen and the last round in which the node was frozen. 

#### Diagnosing freezing issues

In order to diagnose the cause of freezing issues we must compare our own node's status to that of other nodes within the same neighborhood by comparing the results from our own node returned from the `/status` endpoint to the other nodes in the same neighborhood which can be found from the `/status/peers` endpoint.

First we check our own node's status: 

  ```bash
   curl -s localhost:1633/status | jq
  ```
  ```bash
    {
      "peer": "da7e5cc3ed9a46b6e7491d3bf738535d98112641380cbed2e9ddfe4cf4fc01c4",
      "proximity": 0,
      "beeMode": "full",
      "reserveSize": 3747532,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 183,
      "neighborhoodSize": 12,
      "batchCommitment": 133828050944,
      "isReachable": true 
    }
  ```
  And next we will find the status for all the other nodes in the same neighborhood as our own.  
 
  ```bash
   curl -s  localhost:1633/status/peers | jq
  ```

  The `/status/peers` endpoint returns all the peers of our node, but we are only concerned with peers in the same neighborhood as our own node. Nodes whose `proximity` value is equal to or greater than our own node's `storageRadius` value all fall into the same neighborhood as our node, so the rest have been omitted in the example output below:

  ```bash
  { 
    ...
     {
      "peer": "da33f7a504a74094242d3e542475b49847d1d0f375e0c86bac1c9d7f0937acc0",
      "proximity": 9,
      "beeMode": "full",
      "reserveSize": 3782924,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 188,
      "neighborhoodSize": 11,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da4b529cc1aedc62e31849cf7f8ab8c1866d9d86038b857d6cf2f590604387fe",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3719593,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 176,
      "neighborhoodSize": 11,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da5d39a5508fadf66c8665d5e51617f0e9e5fd501e429c38471b861f104c1504",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3777241,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 198,
      "neighborhoodSize": 12,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da4cb0d125bba638def55c0061b00d7c01ed4033fa193d6e53a67183c5488d73",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3849125,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 181,
      "neighborhoodSize": 13,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da4b1cd5d15e061fdd474003b5602ab1cff939b4b9e30d60f8ff693141ede810",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3778452,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 183,
      "neighborhoodSize": 12,
      "batchCommitment": 133827002368,
      "isReachable": true
    },
    {
      "peer": "da49e6c6174e3410edad2e0f05d704bbc33e9996bc0ead310d55372677316593",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3779560,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 185,
      "neighborhoodSize": 12,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da4cdab480f323d5791d3ab8d22d99147f110841e44a8991a169f0ab1f47d8e5",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3778518,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 189,
      "neighborhoodSize": 11,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da4ccec79bc34b502c802415b0008c4cee161faf3cee0f572bb019b117c89b2f",
      "proximity": 10,
      "beeMode": "full",
      "reserveSize": 3779003,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 179,
      "neighborhoodSize": 10,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da69d412b79358f84b7928d2f6b7ccdaf165a21313608e16edd317a5355ba250",
      "proximity": 11,
      "beeMode": "full",
      "reserveSize": 3712586,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 189,
      "neighborhoodSize": 12,
      "batchCommitment": 133827002368,
      "isReachable": true
    },
    {
      "peer": "da61967b1bd614a69e5e83f73cc98a63a70ebe20454ca9aafea6b57493e00a34",
      "proximity": 11,
      "beeMode": "full",
      "reserveSize": 3780190,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 182,
      "neighborhoodSize": 13,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da7b6a268637cfd6799a9923129347fc3d564496ea79aea119e89c09c5d9efed",
      "proximity": 13,
      "beeMode": "full",
      "reserveSize": 3721494,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 188,
      "neighborhoodSize": 14,
      "batchCommitment": 133828050944,
      "isReachable": true
    },
    {
      "peer": "da7a974149543df1b459831286b42b302f22393a20e9b3dd9a7bb5a7aa5af263",
      "proximity": 13,
      "beeMode": "full",
      "reserveSize": 3852986,
      "pullsyncRate": 0,
      "storageRadius": 10,
      "connectedPeers": 186,
      "neighborhoodSize": 12,
      "batchCommitment": 133828050944,
      "isReachable": true
    }
  ]
}
```

Now that we have the status for our own node and all its neighborhood peers we can begin to diagnose the issue through a series of checks outlined below:

:::info
If you are able to identify and fix a problem with your node from the checklist below, it's possible that your node's reserve has become corrupted. Therefore, after fixing the problem, stop your node, and repair your node according to the instructions in the section following the checklist.
:::

1. Compare `reserveSize` with peers

    The `reserveSize` value is the number of chunks stored by a node in its reserve. The value for `reserveSize` for a healthy node should be around +/- 1% the size of most other nodes in the neighborhood. In our example, for our node's `reserveSize` of 3747532, it falls within that normal range. This does not guarantee our node has no missing or corrupted chunks, but it does indicate that it is generally storing the same chunks as its neighbors. If it falls outside this range, see the next section for instructions on repairing reserves.

2. Compare `batchCommitment` with peers 

    The `batchCommitment` value shows how many chunks would be stored if all postage batches were fully utilised. It also represents whether the node has fully synced postage batch data from on-chain. If your node's `batchCommitment` value falls below that of its peers in the same neighborhood, it could indicate an issue with your blockchain RPC endpoint that is preventing it from properly syncing on-chain data. If you are running your own node, check your setup to make sure it is functioning properly, or check with your provider if you are using a 3rd party service for your RPC endpoint. 

3. Check `pullsyncRate`

    The `pullsyncRate` value measures the speed at which a node is syncing chunks from its peers. Once a node is fully synced, `pullsyncRate` should go to zero. If `pullsyncRate` is above zero it indicates that your node is still syncing chunks, so you should wait until it goes to zero before doing any other checks. If `pullsyncRate` is at zero but your node's `reserveSize` does not match its peers, you should check whether your network connection and RPC endpoint are stable and functioning properly. A node should be fully synced after several hours at most.

4. Check most recent `block` number

    The `block` value returned from the `/redistributionstate` endpoint shows the most recent block a node has synced. If this number is far behind the actual more recent block then it indicates an issue with your RPC endpoint or network. If you are running your own node, check your setup to make sure it is functioning properly, or check with your provider if you are using a 3rd party service for your RPC endpoint. 

    ```bash
    curl -X GET http://localhost:1633/redistributionstate | jq
    ```

    ```bash
    { 
      "minimumFunds": "18750000000000000",
      "hasSufficientFunds": true,
      "isFrozen": false,
      "isFullySynced": true,
      "phase": "commit",
      "round": 176319,
      "lastWonRound": 176024,
      "lastPlayedRound": 176182,
      "lastFrozenRound": 0,
      "block": 26800488,
      "reward": "10479124611072000",
      "fees": "30166618102500000"
    }
    ```
5. Check peer connectivity

    Compare the value of your node's `neighborhoodSize` from the `/status` endpoint and the `neighborhoodSize` of its peers in the same neighborhood from the `/status/peers` endpoint. The figure should be generally the same (although it may fluctuate slightly up or down at any one point in time). If your node's `neighborhoodSize` value is significantly different and remains so over time then your node likely has a connectivity problem. Make sure to [check your network environment](/docs/bee/installation/connectivity) to ensure your node is able to communicate with the network.

If no problems are identified during these checks it likely indicates that your node was frozen in error and there are no additional steps you need to take. 

### Repairing corrupt reserve 

If you have identified and fixed a problem causing your node to become frozen or have other reason to believe that your node's reserves are corrupted then you should repair your node's reserve using the `db repair-reserve` command.

First stop your node, and then run the following command:

:::caution
Make sure to replace `/home/bee/.bee` with your node‚Äôs data directory if it differs from the one shown in the example. Make sure that the directory you specify is the root directory for your node‚Äôs data files, not the localstore directory itself. This is the same directory specified using the `data-dir` option in your node‚Äôs [configuration](/docs/bee/working-with-bee/configuration/).
:::

```bash
bee db repair-reserve --data-dir=/home/bee/.bee
```

After the command has finished running, you may restart your node.

### Node occupies unusually large space on disk

During normal operation of a Bee node, it should not take up more than ~30 GB of disk space. In the rare cases when the node's occupied disk space grows larger, you may need to use the compaction `db compact` command.

:::danger
To prevent any data loss, operators should run the compaction on a copy of the localstore directory and, if successful, replace the original localstore with the compacted copy. 
:::

The command is available as a sub-command under db as such (make sure to replace the value for `--data-dir` with the correct path to your bee node's data folder if it differs from the path shown in the example):

```bash
bee db compact --data-dir=/home/bee/.bee
```
### Node not participating in redistribution

First check that the node is fully synced, is not frozen, and has sufficient funds to participate in staking. To check node sync status, call the `redistributionstate` endpoint:

```
curl -X GET http://localhost:1633/redistributionstate | jq
```
Response:

```bash
{ 
  "minimumFunds": "18750000000000000",
  "hasSufficientFunds": true,
  "isFrozen": false,
  "isFullySynced": true,
  "phase": "commit",
  "round": 176319,
  "lastWonRound": 176024,
  "lastPlayedRound": 176182,
  "lastFrozenRound": 0,
  "block": 26800488,
  "reward": "10479124611072000",
  "fees": "30166618102500000"
}
```
Confirm that `hasSufficientFunds` is `true`, and `isFullySynced` is `true` before moving to the next step. If `hasSufficientFunds` is `false`, make sure to add at least the amount of xDAI shown in `minimumFunds` (unit of 1e-18 xDAI). If the node was recently installed and `isFullySynced` is `false`, wait for the node to fully sync before continuing. After confirming the node's status, continue to the next step.

#### Run sampler process to benchmark performance

One of the most common issues affecting staking is the `sampler` process failing. The sampler is a resource intensive process which is run by nodes which are selected to take part in redistribution. The process may fail or time out if the node's hardware specifications aren't high enough. To check a node's performance the `/rchash` endpoint of the API may be used. See the `/rchash` section of the [Bee API page for usage details](/docs/bee/working-with-bee/bee-api/). 

If you are still experiencing problems, you can find more help in the [node-operators](https://discord.gg/kHRyMNpw7t) Discord channel (for your safety, do not accept advice from anyone sending a private message on Discord).

---

## Swarm CLI

**Swarm‚ÄëCLI** is a command‚Äëline tool powered by `bee-js` that makes it easy to interact with your Bee node directly from the command line. It‚Äôs friendlier than working with the raw Bee HTTP API and faster than writing a custom `bee-js` script when you just want to perform an action from the terminal.

:::tip
`swarm-cli` is the recommended method for interaction with your Bee node from the command line. Unless you have explicit need to use the Bee API directly, `swarm-cli` is generally the better option.
:::

Common uses:

* Check your node: `swarm-cli status`
* Add stake: 
* Upload files or a static site: `swarm-cli upload <path>` (will prompt to pick or create a postage batch)
* Download content: `swarm-cli download <reference> -o <output>`
* Inspect and manage postage batches: `swarm-cli ...` (use `--help` to see stamp-related commands)

**Why use it?**

* **No scaffolding needed** ‚Äî run direct commands without creating a project
* **Interactive prompts** ‚Äî it guides you through common tasks such as stamp purchasing and selection using interactive prompts 
* **Smart option inference** ‚Äî it infers options based on your input (e.g., batch selection, index page, content type) so you don‚Äôt need deep Bee API knowledge
* **Powered by `bee-js`** ‚Äî stays aligned with the latest Bee features

It also greatly simplifies certain more complex tasks, such as as the management of [feeds](/docs/develop/tools-and-features/feeds).  

For installation and usage instructions, [see the README](https://github.com/ethersphere/swarm-cli/blob/master/README.md).

To check the latest version, see the Swarm CLI [releases page](https://github.com/ethersphere/swarm-cli/releases).

For further support and information, [join the Swarm Discord server](https://discord.com/invite/GU22h2utj6).

---

## Uninstalling Bee

Choose the appropriate uninstallation method based on how Bee was installed:

## Package Manager  

This method can be used for package manager based [installs](/docs/bee/installation/package-manager-install) of the official Debian, RPM, and Homebrew packages.

:::danger
Uninstalling Bee will permanently delete your keyfiles and configuration. Ensure you have a [full backup](/docs/bee/working-with-bee/backups) before proceeding.
:::

### Debian

To uninstall Bee and completely remove all associated files including keys and configuration, run: 

```bash
sudo apt-get purge bee
```

### RPM

```bash
sudo yum remove bee
```

## Shell Script / Binary Install

If Bee was installed using the [automated shell script](/docs/bee/installation/shell-script-install) or as a binary by [building from source](/docs/bee/installation/build-from-source), it can be uninstalled by manually removing the installed binary, configuration files, and data directories.

### Identify Data and Config Locations

The shell script install method may result in slightly different default data and configuration locations based on your system. The easiest way to find these locations is to check the default configuration using the `bee printconfig` command:

```bash
bee printconfig
```

The output from this command contains several dozen default configuration values, however we only include the two we need in the example output below, `config` and `data-dir`. These will reveal the default locations for the configuration files and data directory according to our specific system.

Your output should look similar to this:

```bash
# config file (default is $HOME/.bee.yaml)
config: /home/noah/.bee.yaml
# data directory
data-dir: /home/noah/.bee
```

## Remove Configuration Files

Bee does not automatically generate a configuration file, but it looks for one at **`$HOME/.bee.yaml`** by default. 

### Check for Configuration Files 

**Default location for shell script installs:**  
```bash
ls -l $HOME/.bee.yaml
```

### Remove Configuration Files  
If the files exist, remove them:

```bash
rm -f $HOME/.bee.yaml
```

### Verify Removal 
Run the following commands to ensure the configuration files have been deleted:

```bash
ls -l $HOME/.bee.yaml
```

If the command returns **"No such file or directory"**, the configuration file has been successfully removed.

:::caution
If you have generated a config file and saved it to a non default location which you specify when starting your node using a command line flag (`--config`) or environment variable (`BEE_CONFIG`), then it is up to you to keep track of where you saved it and remove it yourself.
:::

## Remove Data Files

Bee stores its **node data, blockchain state, and other persistent files** in a data directory. If you want to fully remove Bee, this directory must be deleted. The data directory [default location](/docs/bee/working-with-bee/configuration#default-data-and-config-directories) differs based on install method and system type. 

## Verify Uninstallation

To confirm that Bee has been fully uninstalled, run:

```bash
command -v bee
```

If Bee is still installed, this command will return the binary path (e.g., /usr/bin/bee). If it returns nothing, Bee has been successfully uninstalled.

---

## Upgrading Bee

It's very important to keep Bee up to date to benefit from security updates and ensure you are able to properly interact with the Swarm network. The [#node-operators](https://discord.com/channels/799027393297514537/811553590170353685) channel is an excellent resource for any of your questions regarding node operation. 

:::warning
Bee sure to [back up](/docs/bee/working-with-bee/backups) your keys and [cash out your cheques](/docs/bee/working-with-bee/cashing-out) to ensure your xBZZ is safe before applying updates.
:::

:::warning
Nodes should not be shut down or updated in the middle of a round they are playing in as it may cause them to lose out on winnings or become frozen. To see if your node is playing the current round, check if `lastPlayedRound` equals `round` in the output from the [`/redistributionstate` endpoint](/api/#tag/RedistributionState/paths/~1redistributionstate/get). See [staking section](/docs/bee/working-with-bee/staking/) for more information on staking and troubleshooting.
:::

### Ubuntu / Debian 

To upgrade Bee, first stop the Bee service: 

```bash
sudo systemctl stop bee
```

Next, upgrade the `bee` package:

```bash
sudo apt-get update
sudo apt-get upgrade bee
```

And will see output like this after a successful upgrade:
```
Reading package lists... Done
Building dependency tree
Reading state information... Done
Calculating upgrade... Done
The following packages will be upgraded:
  bee
1 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
Need to get 0 B/27.2 MB of archives.
After this operation, 73.7 kB of additional disk space will be used.
Do you want to continue? [Y/n] Y
(Reading database ... 103686 files and directories currently installed.)
Preparing to unpack .../archives/bee_2.0.0_amd64.deb ...
Unpacking bee (2.0.0) over (1.17.3) ...
Setting up bee (2.0.0) ...
Installing new version of config file /etc/default/bee ...
```

Make sure to pay attention to any prompts, read them carefully, and respond to them with your preference.

You may now start your node again:

```bash
sudo systemctl start bee
```

### Manual Installations

To upgrade your manual installation, simply stop Bee, replace the Bee binary and restart.

### Docker

To upgrade your Docker installation, simply increment the version number in your configuration and restart.

---

## DISC

import bos_fig_2_7 from '/static/img/bos_fig_2_7.jpg';

DISC (Distributed Immutable Storage of Chunks) is a storage solution developed by Swarm based on a modified implementation of a [Kademlia DHT](/docs/concepts/DISC/kademlia) which has been specialized for data storage. Swarm's implementation of a DHT differs significantly in that it stores the content in the DHT directly, rather than just storing a list of seeders who are able to serve the content. This approach allows for much faster and more efficient retrieval of data.

### Kademlia Topology and Routing

[Kademlia](/docs/concepts/DISC/kademlia) is a distributed hash table (DHT) widely used in peer-to-peer networks such as Ethereum and Bittorent. It serves as the routing and topology foundation for communication between nodes in the Swarm network. It organizes nodes based on their overlay addresses and ensures that messages are relayed efficiently, even in a dynamic, decentralized environment. 

One of the advantages of using Kademlia as a model for network topology is that both the number of forwarding "hops" required to route a chunk to its destination and the number of peer connections required to maintain Kademlia topology are logarithmic to the size of the network (a minimum of two connections is required in order to maintain Kademlia topology in case of network churn - nodes dropping in and out of the network). This makes Swarm a highly scalable system which is efficient even at very large scales. 

### Neighborhoods

[Neighborhoods](/docs/concepts/DISC/neighborhoods) are groups of nodes which are responsible for sharing the same chunks. The chunks which each neighborhood is responsible for storing are defined by the proximity order of the nodes and the chunks. In other words, each node is responsible for storing chunks with which their overlay addresses share a certain number of prefix bits, and together with other nodes which share the same prefix bits, make up neighborhoods which share the responsibility for storing the same chunks. 

Neighborhoods play a key role in providing data redundancy for chunks stored on Swarm since each node in a neighborhood will keep copies of the same chunks. The optional [erasure coding](/docs/concepts/DISC/erasure-coding) feature can also be enabled for added redundancy and greater data protection.

### Chunks 

In the DISC model, chunks are the canonical unit of data. When a file is uploaded to Swarm, it gets broken down into 4kb pieces with attached metadata. The pieces then get distributed amongst nodes in the Swarm network based on their [overlay addresses](/docs/references/glossary#overlay). There are two fundamental chunk types: content-addressed chunks and single-owner chunks. 

#### Content-Addressed Chunks and Single-Owner Chunks

Content-addressed chunks are chunks whose address is based on the hash digest of their data. Using a hash as the chunk address makes it possible to verify the integrity of chunk data. Swarm uses the BMT hash function based on a binary Merkle tree over small segments of the chunk data. A content-addressed chunk has an at most 4KB payload, and its address is calculated as the hash of the span (chunk metadata) and the Binary Merkle Tree hash of the payload.

  
  
    Source: The Book of Swarm - Figure 2.7 - "Content addressed chunk"
  

For single-owner chunks on the other hand, the address is calculated as the hash of a unique id and the owner's overlay address. The content consists of an arbitrary data payload along with required headers. Unlike a content-addressed chunk, the contents of a single-owner chunk may be updated while the address remains unchanged. Single owner chunks form the basis for feeds, which are data structures that allow for mutable content with a static address.

### Push-Sync, Pull-Sync, and Retrieval Protocols

When a file is first uploaded to Swarm, it gets broken down by the uploading Bee node chunks which are then distributed amongst other Bee nodes in the Swarm network. Chunks get distributed to the target neighborhood by the ***push-sync*** protocol. Once a chunk reaches its destination, it will then be duplicated and synced to other nodes in order to achieve data redundancy through the ***pull-sync*** protocol. The pull-sync protocol operates continuously as nodes enter or exit the network ‚Äì ensuring that data redundancy is always maintained. When a client node requests a file for download, its request gets forwarded by the ***retrieval-protocol*** to all the nodes storing the relevant chunks, and then those chunks get returned to the requesting node and the file gets reconstructed from its constituent chunks.

---

## Erasure Coding

Erasure coding (also known as erasure code) is an efficient and flexible approach to data protection which is an optional feature for Swarm uploads. It is a technique that increases data protection by enabling the recovery of original data even when some encoded chunks are lost or corrupted. When used, it ensures that data on Swarm can always be accessed reliably, even if some nodes or entire neighborhoods go offline. Refer to the [official erasure coding paper](https://papers.ethswarm.org/p/erasure/) for more in depth details. 

## How It Works

Erasure coding enhances data protection by dividing the source data into "chunks" and adding additional redundant chunks.

Specifically, data is divided into **m** chunks, and **k** additional chunks are generated, resulting in **m + k** total chunks. The data is encoded across these chunks such that as long as **m** chunks are intact, the original data can be fully reconstructed. Chunks are then distributed across the network as with a standard upload. This approach provides a robust method for data recovery in distributed storage networks like Swarm.

### Example

For an 8KB image, if we set **m = 2** and **k = 1**, we create 3 chunks (2 original + 1 redundant). As long as any 2 of these 3 chunks are available, we can reconstruct the original data. By increasing **k** to 4, we can tolerate the loss of up to 4 chunks while still recovering the original data.

![Erasure Code Example](https://www.ethswarm.org/uploads/erasure-coding-01.png)

### Levels of Protection

In Swarm's implementation of erasure coding, there are four named levels of protection, Medium, Strong, Insane, and Paranoid. For each level, the **m** and **k** values have been adjusted in order to meet a certain level of data protection:

***Table A:***
| Redundancy Level Value | Level Name | Chunk Loss Tolerance         |      
| ---------------- | --------- | ----------------------------------- |
| 1                | Medium    | 1%                                  | 
| 2                | Strong    | 5%                                  |
| 3                | Insane    | 10%                                 | 
| 4                | Paranoid  | 50%                                 | 

The "Redundancy Level" is a numeric value for each level of protection, the "Level Name" is the official name for each level, and the "Chunk Loss Tolerance" column corresponds to the exact level of data protection for each level. For each redundancy level, the original data is retrievable with >=99.9999% statistical certainty given a percent chunk loss equal or less than the percent shown in the "Chunk Loss Tolerance" column. 

Note that this guarantee of retrievability is for each 128 chunk segment, and therefore does not correspond to retrievability of a whole file. The retrievability failure rate for any individual file depends on the size of the file, and increases with the size of the file. For a detailed explanation of how to calculate the retrievability of any sized file refer to [section 3 in the erasure coding paper](https://papers.ethswarm.org/erasure-coding.pdf).    

## Usage

For usage instructions, see the [erasure coding page in the "Develop" section](/docs/develop/tools-and-features/erasure-coding).

## Cost Calculation

In Swarm's implementation of erasure coding, there are four levels of protection: Medium, Strong, Insane, and Paranoid. Each level adds additional parity chunks for a corresponding increase in data protection (and also cost).

The table below shows the number of parities and data chunks for each level, as well as the percent increase in cost vs a non-erasure coded upload. 

***Table B:***
| Redundancy | Parities | Data Chunks | Percent | Chunks Encrypted | Percent Encrypted |
|----------|----------|--------|---------|------------------|-------------------|
| Medium   | 9        | 119 | *7.6%*  | 59            | 15%           |
| Strong   | 21       | 107 | *19.6%* | 53            | 40%           |
| Insane   | 31       | 97   | 32%  | 48            | 65%            |
| Paranoid | 90       | 38      | 240.5%    | 18               | 494%              |

For each redundancy level, there are **m + k** = 128 chunks, where **m** are the data chunks (shown in column "Data Chunks") and **k** are the parity chunks (shown in column "Parities"). The "Percent" and "Percent Encrypted" columns show percent of "parity overhead" cost increase from using erasure coding for normal and encrypted uploads respectively. 

### Cost Calculation for Smaller Uploads

To find the percent increase in cost for uploads of less than 128 chunks, refer to the table below:

***Table C:***
| Security | Parities | Chunks | Percent     | Chunks Encrypted | Percent Encrypted |
|----------|----------|--------|-------------|------------------|-------------------|
| Medium   | 2        | 1      | 200%        |                  |                   |
| Medium   | 3        | 2-5    | 150% - 60%  | 1-2              | 300% - 150%       |
| Medium   | 4        | 6-14   | 66.7% - 28.6%| 3-7             | 133.3% - 57.1%    |
| Medium   | 5        | 15-28  | 33.3% - 17.9%| 7-14            | 71.4% - 35.7%     |
| Medium   | 6        | 29-46  | 20.7% - 13% | 14-23            | 42.9% - 26.1%     |
| Medium   | 7        | 47-68  | 14.9% - 10.3%| 23-34           | 30.4% - 20.6%     |
| Medium   | 8        | 69-94  | 11.6% - 8.5%| 34-47            | 23.5% - 17%       |
| Medium   | 9        | 95-119 | 9.5% - 7.6% | 47-59            | 19.1% - 15.3%     |
| Strong   | 4        | 1      | 400%        |                  |                   |
| Strong   | 5        | 2-3    | 250% - 166.7%| 1               | 500%              |
| Strong   | 6        | 4-6    | 150% - 100% | 2-3              | 300% - 200%       |
| Strong   | 7        | 7-10   | 100% - 70%  | 3-5              | 233.3% - 140%     |
| Strong   | 8        | 11-15  | 72.7% - 53.3%| 5-7             | 160% - 114.3%     |
| Strong   | 9        | 16-20  | 56.2% - 45% | 8-10             | 112.5% - 90%      |
| Strong   | 10       | 21-26  | 47.6% - 38.5%| 10-13           | 100% - 76.9%      |
| Strong   | 11       | 27-32  | 40.7% - 34.4%| 13-16           | 84.6% - 68.8%     |
| Strong   | 12       | 33-39  | 36.4% - 30.8%| 16-19           | 75% - 63.2%       |
| Strong   | 13       | 40-46  | 32.5% - 28.3%| 20-23           | 65% - 56.5%       |
| Strong   | 14       | 47-53  | 29.8% - 26.4%| 23-26           | 60.9% - 53.8%     |
| Strong   | 15       | 54-61  | 27.8% - 24.6%| 27-30           | 55.6% - 50%       |
| Strong   | 16       | 62-69  | 25.8% - 23.2%| 31-34           | 51.6% - 47.1%     |
| Strong   | 17       | 70-77  | 24.3% - 22.1%| 35-38           | 48.6% - 44.7%     |
| Strong   | 18       | 78-86  | 23.1% - 20.9%| 39-43           | 46.2% - 41.9%     |
| Strong   | 19       | 87-95  | 21.8% - 20%  | 43-47           | 44.2% - 40.4%     |
| Strong   | 20       | 96-104 | 20.8% - 19.2%| 48-52           | 41.7% - 38.5%     |
| Strong   | 21       | 105-107| 20% - 19.6%  | 52-53           | 40.4% - 39.6%     |
| Insane   | 5        | 1      | 500%        |                  |                   |
| Insane   | 6        | 2      | 300%        | 1                | 600%              |
| Insane   | 7        | 3      | 233.3%      | 1                | 700%              |
| Insane   | 8        | 4-5    | 200% - 160% | 2                | 400%              |
| Insane   | 9        | 6-8    | 150% - 112.5%| 3-4             | 300% - 225%       |
| Insane   | 10       | 9-10   | 111.1% - 100%| 4-5             | 250% - 200%       |
| Insane   | 11       | 11-13  | 100% - 84.6%| 5-6              | 220% - 183.3%     |
| Insane   | 12       | 14-16  | 85.7% - 75% | 7-8              | 171.4% - 150%     |
| Insane   | 13       | 17-19  | 76.5% - 68.4%| 8-9             | 162.5% - 144.4%   |
| Insane   | 14       | 20-22  | 70% - 63.6% | 10-11            | 140% - 127.3%     |
| Insane   | 15       | 23-26  | 65.2% - 57.7%| 11-13           | 136.4% - 115.4%   |
| Insane   | 16       | 27-29  | 59.3% - 55.2%| 13-14           | 123.1% - 114.3%   |
| Insane   | 17       | 30-33  | 56.7% - 51.5%| 15-16           | 113.3% - 106.2%   |
| Insane   | 18       | 34-37  | 52.9% - 48.6%| 17-18           | 105.9% - 100%     |
| Insane   | 19       | 38-41  | 50% - 46.3% | 19-20            | 100% - 95%        |
| Insane   | 20       | 42-45  | 47.6% - 44.4%| 21-22           | 95.2% - 90.9%     |
| Insane   | 21       | 46-50  | 45.7% - 42% | 23-25            | 91.3% - 84%       |
| Insane   | 22       | 51-54  | 43.1% - 40.7%| 25-27           | 88% - 81.5%       |
| Insane   | 23       | 55-59  | 41.8% - 39% | 27-29            | 85.2% - 79.3%     |
| Insane   | 24       | 60-63  | 40% - 38.1% | 30-31            | 80% - 77.4%       |
| Insane   | 25       | 64-68  | 39.1% - 36.8%| 32-34           | 78.1% - 73.5%     |
| Insane   | 26       | 69-73  | 37.7% - 35.6%| 34-36           | 76.5% - 72.2%     |
| Insane   | 27       | 74-77  | 36.5% - 35.1%| 37-38           | 73% - 71.1%       |
| Insane   | 28       | 78-82  | 35.9% - 34.1%| 39-41           | 71.8% - 68.3%     |
| Insane   | 29       | 83-87  | 34.9% - 33.3%| 41-43           | 70.7% - 67.4%     |
| Insane   | 30       | 88-92  | 34.1% - 32.6%| 44-46           | 68.2% - 65.2%     |
| Insane   | 31       | 93-97  | 33.3% - 32% | 46-48            | 67.4% - 64.6%     |
| Paranoid | 19       | 1      | 1900%       |                  |                   |
| Paranoid | 23       | 2      | 1150%       | 1                | 2300%             |
| Paranoid | 26       | 3      | 866.7%      | 1                | 2600%             |
| Paranoid | 29       | 4      | 725%        | 2                | 1450%             |
| Paranoid | 31       | 5      | 620%        | 2                | 1550%             |
| Paranoid | 34       | 6      | 566.7%      | 3                | 1133.3%           |
| Paranoid | 36       | 7      | 514.3%      | 3                | 1200%             |
| Paranoid | 38       | 8      | 475%        | 4                | 950%              |
| Paranoid | 40       | 9      | 444.4%      | 4                | 1000%             |
| Paranoid | 43       | 10     | 430%        | 5                | 860%              |
| Paranoid | 45       | 11     | 409.1%      | 5                | 900%              |
| Paranoid | 47       | 12     | 391.7%      | 6                | 783.3%            |
| Paranoid | 48       | 13     | 369.2%      | 6                | 800%              |
| Paranoid | 50       | 14     | 357.1%      | 7                | 714.3%            |
| Paranoid | 52       | 15     | 346.7%      | 7                | 742.9%            |
| Paranoid | 54       | 16     | 337.5%      | 8                | 675%              |
| Paranoid | 56       | 17     | 329.4%      | 8                | 700%              |
| Paranoid | 58       | 18     | 322.2%      | 9                | 644.4%            |
| Paranoid | 59       | 19     | 310.5%      | 9                | 655.6%            |
| Paranoid | 61       | 20     | 305%        | 10               | 610%              |
| Paranoid | 63       | 21     | 300%        | 10               | 630%              |
| Paranoid | 65       | 22     | 295.5%      | 11               | 590.9%            |
| Paranoid | 66       | 23     | 287%        | 11               | 600%              |
| Paranoid | 68       | 24     | 283.3%      | 12               | 566.7%            |
| Paranoid | 70       | 25     | 280%        | 12               | 583.3%            |
| Paranoid | 71       | 26     | 273.1%      | 13               | 546.2%            |
| Paranoid | 73       | 27     | 270.4%      | 13               | 561.5%            |
| Paranoid | 75       | 28     | 267.9%      | 14               | 535.7%            |
| Paranoid | 76       | 29     | 262.1%      | 14               | 542.9%            |
| Paranoid  | 78      | 30      | 260%       | 15               | 520%              |               
| Paranoid  | 80      | 31      | 258.1%     | 15               | 533.3%            |               
| Paranoid  | 81      | 32      | 253.1%     | 16               | 506.2%            |               
| Paranoid  | 83      | 33      | 251.5%     | 16               | 518.8%            |               
| Paranoid  | 84      | 34      | 247.1%     | 17               | 494.1%            |               
| Paranoid  | 86      | 35      | 245.7%     | 17               | 505.9%            |               
| Paranoid  | 87      | 36      | 241.7%     | 18               | 483.3%            |               
| Paranoid  | 89      | 37      | 240.5%     | 18               | 494.4%            |  

### Example Cost Calculation

For each redundancy level, there are m + k = 128 chunks, where m are the data chunks (shown in column "Data Chunks") and k are the parity chunks. If the number of chunks in the data being uploaded are an exact multiple of m, then the percent cost of the upload will simply equal the one shown in table B from the section above in the "Percent" column for the corresponding redundancy level.

#### Exact Multiples

For example, if we are uploading with the Strong redundancy level, and our source data consists of 321 (3 * 107) chunks, then we can simply use the percentage from the "Percent" column for the Strong level - 19.6% (63 parities / 321 data chunks).

#### With Remainders

However, generally speaking uploads will not come in exact multiples of m, so we need to adjust our calculations. To do so we need to use table C from the section above which shows the number of parities for sets of chunks starting at a single chunk for each redundancy level up to the maximum number of data chunks for that level. Then we simply sum up the total parities and data chunks for the entire upload and calculate the resulting percentage.

Let's say for example we have a source file of 340 chunks which we want to upload with the Strong level of protection. Referring to table B, we see for the Strong level there are 21 parity chunks for each 107 data chunks. 340 / 107 = ~3.177, meaning our upload will have three full sets of 128 chunks where m = 107 and k = 21. The remainder can be calculated from the modulus of 340 % 107 = 19

Looking at our chart, we can see that at the Strong level for 19 data chunks we need 9 parity chunks. From this we can calculate the final percentage price: 72 / 340 = 21.17%.

---

## Kademlia

import bos_fig_2_3 from '/static/img/bos_fig_2_3.jpg';

Kademlia is a distributed hash table (DHT) algorithm used in peer-to-peer networks to efficiently store and retrieve data without relying on centralized servers. It organizes nodes into an overlay network that ensures efficient routing using a binary tree structure.

## Kademlia Key Concepts

### **XOR Distance Metric**
Kademlia uses a distance metric based on the XOR (exclusive OR) between any addresses. This allows nodes to calculate "distance" from each other. Lookups are made by recursively querying nodes that are progressively closer to the target. 

### **Routing Table**
Each node in a Kademlia network maintains a routing table containing information about other nodes, organized by the XOR distance between node IDs. 

## Kademlia Advantages 

### **Efficient Lookups**

To retrieve a specific chunk, a node uses Kademlia's lookup process to find and fetch the chunk from a node in the neighborhood where it is stored. The number of hops required for a chunk to be retrieved is logarithmic to the number of nodes in the network, meaning lookups remain efficient even as the network grows larger and larger.

### **Fault Tolerance**

Because nodes' peer lists are regularly refreshed through lookups and interactions, and because redundant copies of data are replicated within the network, the network remains functional even when individual nodes leave or fail.

### **Scalability**

Kademlia's design allows it to scale to large networks, as each node only needs to keep track of a small subset of the total nodes in the network. The required set of connected peers grows logarithmically with the number of nodes, making it efficient even in large networks.

## Kademlia in Swarm 

As mentioned above, Swarm's version of Kademlia differs from commonly used implementations of Kademlia in several important ways:

### Proximity Order & Neighborhoods

Swarm introduces the concept of [proximity order (PO)](/docs/references/glossary#proximity-order-po) as a discrete measure of node relatedness between two addresses. In contrast with Kademlia distance which is an exact measure of relatedness, PO is used to measure the relatedness between two addresses on a discrete scale based on the number of shared leading bits. Since this metric ignores all the bits after the shared leading bits, it is not an exact measure of distance between any two addresses.

In Swarm's version of Kademlia, nodes are grouped into [neighborhoods](/docs/concepts/DISC/neighborhoods) of nodes based on PO (ie., neighborhood are composed of nodes which all share the same leading binary prefix bits). Each neighborhood of nodes is responsible for storing the same set of chunks. 

Neighborhoods are important for ensuring data redundancy, and they also play a role in the incentives system which guarantees nodes are rewarded for contributing resources to the network.

### Forwarding Kademlia

Kademlia comes in two flavors, iterative and forwarding. In iterative Kademlia, the requesting node directly queries each node it contacts for nodes that are progressively closer to the target until the node with the requested chunk is found. The chunk is then sent directly from the storer node to the node which initiated the request.

In contrast, Swarm makes use of forwarding Kademlia. Here each node forwards the query to the next closest node in the network, and this process continues until a node with the requested chunk is found. Once the chunk is found, it is sent back along the same chain of nodes rather than sent directly to the initiator of the request.

The main advantage of forwarding Kademlia is that it maintains the anonymity of the node which initiated the request.

  
  
    Source: The Book of Swarm - Figure 2.3 - "Iterative and Forwarding Kademlia routing"
  

### Neighborhood Based Storage Incentives

Swarm introduces a storage incentives layer on top of its Kademlia implementation in order to reward nodes for continuing to provide resources to the network. Neighborhoods play a key role in the storage incentives mechanism. Storage incentives take the role of a "game" in which nodes play to win a reward for storing the correct data. Each round in the game, one neighborhood is chosen to play, and all nodes within the same neighborhood participate as a group. The nodes each compare the data they are storing with each other to make sure they are all storing the data they are responsible for, and one node is chosen to win from among the group. You can read more about how storage incentives work in the dedicated page for storage incentives.

---

## Neighborhoods

In Swarm, a neighborhood refers to an area of responsibility within the network, where nodes in proximity to one another share the task of storing and maintaining data chunks. Nodes within a neighborhood replicate chunks to ensure that if one node goes offline, other nodes in the neighborhood can still retrieve and serve the content.

:::info
To see current neighborhood populations and the current storage depth / storage radius navigate to the ["Neighborhoods" page of Swarmscan.io](https://swarmscan.io/neighborhoods).

The terms "depth" and "radius" are often used interchangeably when discussing neighborhoods. Both refer to number of shared leading bits of node and chunk addresses used to determine the nodes and chunks which fall into which neighborhoods.
:::

## Key Concepts

### Proximity Order (PO)
The PO measures how close a node is to a particular chunk of data or another node. It is defined as the number of shared leading bits between two addresses. Proximity order plays a role in how neighborhoods are defined, as a node‚Äôs neighborhood extends up to its storage depth, covering all nodes within that proximity‚Äã.

### Reserve Depth

The reserve depth is the shallowest PO at which neighborhoods are able to store all of the chunks which have been paid for through [postage stamp batch](/docs/concepts/incentives/overview#postage-stamps) purchases.

### Storage Depth

Storage depth is the shallowest PO at which neighborhoods are able to store all the chunks which have been *uploaded*. If 100% of all all chunks which have been paid for have been stamped and uploaded to the network, then storage depth will equal reserve depth. However, it is common that stamp batches are not always fully utilized, meaning that it is possible for the storage depth to be shallower than the reserve depth.

Storage depth is the proximity order of chunks for which a node must synchronize and store chunks, and it is determined by nodes' reserve sizes in combination with the amount of chunks actually uploaded. 

### Neighborhood Depth

Neighborhood depth for a node is the highest (deepest) PO *`d`* where the node has at least 3 peers which share the same *`d`* number of leading binary prefix bits in their addresses.

### Neighborhood

A neighborhood is a set of nodes in close proximity to each other based on their proximity order (PO). Each node within a storage-depth-defined neighborhood interacts with other nodes to store and replicate data chunks, ensuring availability and redundancy.

## Example neighborhood

Let's take a closer look at an example. Below is a neighborhood of six nodes at depth 10. Each node is identified by its Swarm address, which is a 256 bit hexadecimal number derived from the node's Gnosis Chain address, the Swarm network id, and a random nonce.  

> da4cb0d125bba638def55c0061b00d7c01ed4033fa193d6e53a67183c5488d73
> da5d39a5508fadf66c8665d5e51617f0e9e5fd501e429c38471b861f104c1504
> da7a974149543df1b459831286b42b302f22393a20e9b3dd9a7bb5a7aa5af263
> da76f8fccc3267b589d822f1c601b21b525fdc2598df97856191f9063029d21e
> da7b6439c8d3803286b773a56c4b9a38776b5cd0beb8fd628b6007df235cf35c
> da7fd412b79358f84b7928d2f6b7ccdaf165a21313608e16edd317a5355ba250

Since we are only concerned with the leading binary bits close to the neighborhood depth, for the rest of this example we will abbreviate the addresses to the first four prefixed hexadecimal digits only. Below are listed the hex prefixes and their binary representation, with the first ten leading bits underlined:

| Hex prefix | Binary Bits     |
|------------|-----------------|
| da4c       | 1101101001001100|
| da5d       | 1101101001011101|
| da76       | 1101101001110110|
| da7a       | 1101101001111010|
| da7b       | 1101101001111011|
| da7f       | 1101101001111111|

### Area of Responsibility

Storer nodes are responsible for storing chunks with addresses whose leading bits match their own up to the storage depth. Here are two example chunks which fall within our example neighborhood:

> Chunk A address: `da49a42926015cd1e2bc552147c567b1ca13e8d4302c9e6026e79a24de328b65`   
> Chunk B address: `da696a3dfb0f7f952872eb33e0e2a1435c61f111ff361e64203b5348cc06dc8a`   

As the address of the chunk shown above shares the same ten leading binary bits as the nodes in our example neighborhood, it falls into that neighborhood's [area of responsibility](/docs/references/glossary#2-area-of-responsibility-related-depths), and all the nodes in that neighborhood are required to store that chunk:

> da49 --> 1101101001001001  
> da69 --> 1101101001101001 

*As with the example for nodes, we've abbreviated the chunk addresses to their leading four hexadecimal digits only and converted them to binary digits.*

### Neighborhood Doubling 

As more and more chunks are assigned to neighborhoods, the chunk reserves of the nodes in that neighborhood will begin to fill up. Once the nodes' reserves in a neighborhood become full and can no longer store additional chunks, that neighborhood will split, with each half of the neighborhood taking responsibility for half of the chunks. This event is referred to as a "doubling", as it results in double the number of neighborhoods. The split is done by increasing the storage depth by one, so that the number of shared leading bits is increased by one. This results in a binary splitting of the neighborhood and associated chunks into two new neighborhoods and respective groups of chunks.

:::info
Note that when chunks begin to expire and new chunks are not uploaded to Swarm, it is possible for node's reserves to empty out, once they fall below a certain threshold, a "halving" will occur in which the storage depth will be decreased by one and two neighborhoods will merge to make a new one so that they are responsible for a wider set of chunks.
:::

Using our previous example neighborhood, during a doubling, the storage depth would increase from 10 to 11, and the neighborhood would be split based on the 11th leading bit.

**neighborhood A:**
| Hex prefix | Binary Bits     |
|------------|-----------------|
| da4c       | 1101101001001100|
| da5d       | 1101101001011101|

**neighborhood B:**
| Hex prefix | Binary Bits     |
|------------|-----------------|
| da76       | 1101101001110110|
| da7a       | 1101101001111010|
| da7b       | 1101101001111011|
| da7f       | 1101101001111111|

Each of our two example chunks will also be split amongst the two new neighborhoods based on their 11th leading bit:

**neighborhood A:**
| Hex prefix | Binary Bits     |
|------------|-----------------|
| da4c       | 1101101001001100|
| da5d       | 1101101001011101|
|da49 (chunk)| 1101101001001001|

**neighborhood B:**
| Hex prefix | Binary Bits     |
|------------|-----------------|
| da76       | 1101101001110110|
| da7a       | 1101101001111010|
| da7b       | 1101101001111011|
| da7f       | 1101101001111111|
|da69 (chunk)| 1101101001101001|

#### Doubling Implications for Node Operators

One of the implications of doubling for node operators is that the reward chances for a node depends in part on how many other nodes are in its neighborhood. If it is in a neighborhood with fewer nodes, its chances of winning rewards are greater. Therefore node operators should make certain to place their nodes into less populated neighborhoods, and also should look ahead to neighborhoods at the next depth after a doubling. For more details about how to adjust node placement, see [here](/docs/bee/installation/set-target-neighborhood).

---

## Access Control

The Access Control Trie (ACT) implements the operation of encryption at the chunk level, with the presence of a decryption/encryption key being the only distinction between accessing private and public data.

:::info
This article describes the high level concepts and functionalities of ACT. If you're ready to try it out for yourself, please refer to this [hands on usage guide with specific details](/docs/develop/act/).
:::

In decentralized public data storage systems like Swarm, data is distributed across multiple nodes. Ensuring
confidentiality, integrity, and availability becomes paramount. The Access Control Trie (ACT) addresses these challenges
by managing access control information for Swarm nodes.

## Key Concepts

From the perspective of access controlled content, we can identify two main roles:

| Role                         | Rights & responsibilities                                                                                                                                            |
|------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Content Publisher**        | Publishers upload data and grant access to viewers based on their wallets‚Äô public keys.They can also revoke access from specific viewers. |
| **Grantee (Content Viewer)** | Grantees can access the content version allowed by the publisher.However, they may be blocked from accessing new versions of the content. |

The control is defined by a process to obtain the full (decrypted) reference to the protected content uploaded by the
publisher, which makes granted access possible.

For the management of access by multiple grantees (viewers), an additional layer is introduced to derive the access key
from their specific session key. This data structure, the lookup table for ACT, is implemented as key-value store in a
Swarm manifest format. The publisher is able to add and remove grantees from this ACT.

### Session

For each grantee, their public key is used as the session key. Using Diffie-Hellman key derivation, two additional keys
will be derived from the session key: a lookup key and an access key decryption key (used for symmetric encryption of
the access key). This means each grantee will have the content's access key specifically encrypted for them, and only
they will be able to decrypt this, thus gain access to the content.

### ACT lookup table

The ACT lookup table is a key-value store implemented over a Swarm manifest. It holds lookup keys and encrypted access
keys prepared for the grantees when they are added to the ACT (granting access to the content).

### History

The history of the ACT is maintained as well. This allows to retrieve a historical version of the ACT based on the
timestamp attached to it. This also ensures that grantees will be able to retrieve the content version they were
granted access to (using the relevant timestamp), even if their access to newer versions were revoked.

### Encryption

It is important to emphasise that all elements of the process will undergo encryption. Including the grantee list
itself, which is encrypted using the publisher‚Äôs own lookup key, as well as the grantee list‚Äôs content reference. This
ensures that the security of the process and the data is always maintained.

---

## Bandwidth Incentives (SWAP)

The Swarm Accounting Protocol (SWAP) is a protocol used to manage the exchange of bandwidth resources between nodes. SWAP ensures that node operators collaborate in routing messages and data while protecting the network against frivolous use of bandwidth. The protocol combines off-chain peer-to-peer based accounting with on-chain settlement through the chequebook contract.

As nodes relay requests and responses, they keep track of their bandwidth usage with each of their peers. Peers engage in a service-for-service exchange, where they provide resources to each other based on their relative usage. 

Once a node's relative debt with one of their peers crosses a certain threshold, the party in debt can either send a xBZZ payment in the form of a "cheque" (an off chain commitment to pay their debt), or can continue to provide bandwidth services in kind until their debt is paid off. Each node can set their own threshold for the level of relative debt they accept. Freeloader nodes which do not pay their debts are at risk of being blacklisted by other nodes. 

## Chequebook Contract

The [chequebook contract](https://github.com/ethersphere/swap-swear-and-swindle/blob/master/contracts/ERC20SimpleSwap.sol) is a smart contract used in the SWAP protocol to manage cheques that are sent between nodes on the network. It acts as a wallet which nodes can fund with xBZZ which can be used to issue payments when presented with a valid cheque. The contract is also responsible for ensuring that cheques are valid and are cashed out correctly.

When a node sends a cheque to one of its peers, it includes a signed message that specifies the amount of xBZZ tokens being transferred and the recipient's address. The chequebook contract receives this message and verifies that it is valid by checking the signature and ensuring that the sender has enough funds to cover the transfer.

If the cheque is valid, the contract updates the balances of both nodes accordingly. The recipient can then cash out their xBZZ tokens by sending a transaction to the blockchain that invokes a function in the chequebook contract. This function transfers the specified amount of xBZZ tokens from the sender's account to the recipient's account.

## Opportunistic Caching

When a node serves a chunk, the chunk is saved in the nodes' cache. Popular chunks which are frequently requested are kept in the cache so that they can be served again without the need to re-download the chunks from the network. This allows nodes to maximise their earnings by retaining popular chunks. This mechanism also contributes to Swarm's scalability, as popular chunks are always readily available for download as a result of opportunistic caching.  

To learn in more detail about how bandwidth incentives work, refer to sections 3.1 and 3.2 from [The Book of Swarm](https://papers.ethswarm.org/p/book-of-swarm/).

---

## Incentives Overview

import { globalVariables } from '/src/config/globalVariables'

A key challenge in decentralized data networks is incentivizing users to store and transmit data. Swarm addresses this with two incentive mechanisms: **storage incentives**, which reward nodes for storing data, and **bandwidth incentives**, which reward nodes for relaying data. Together, these mechanisms establish a self-sustaining economic system where nodes are compensated for contributing resources honestly.

Swarm's storage incentives are detailed in the [Future Proof Storage](https://www.ethswarm.org/swarm-storage-incentives.pdf) paper and [The Book of Swarm](https://papers.ethswarm.org/p/book-of-swarm/).

## Storage Incentives  

Storage incentives reward node operators for providing disk space and reliably storing data. The system is governed by three interconnected smart contracts:  

- **Postage Stamp Contract** ‚Äì Handles payments for uploading data by way of purchasing "postage stamp batches".  
- **Redistribution Contract** ‚Äì Distributes payments for postage stamps to nodes that store data.  
- **Price Oracle Contract** ‚Äì Uses network redundancy data to determine postage stamp prices.  

If you want to dig into the code, check out the [incentives contracts repo](https://github.com/ethersphere/storage-incentives)
You can find the on-chain address for each contract within the docs [here](/docs/references/smart-contracts#storage-incentives-contracts), however since the contracts there are updated manually, they may at times fall slightly behind the most recent changes. For the most up to date address for each storage incentives contract refer to the [storage incentives ABI repo](https://github.com/ethersphere/go-storage-incentives-abi/commits/master/abi/abi_mainnet.go), and you can also find past addresses of older versions of the incentives contracts by reviewing previous commits.

### Postage Stamps  

Postage stamps are required to upload data to Swarm, similar to how real-world postage stamps prepay for mail delivery. Instead of being purchased individually, they are bought in batches using xBZZ through the postage stamp smart contract.  

The xBZZ used to buy postage stamps is later redistributed as storage incentives. The **price oracle contract** adjusts postage stamp pricing based on network redundancy to ensure a sustainable level of storage. You can find more details about postage stamps [here](/docs/concepts/incentives/postage-stamps).  

### Redistribution Game  

The redistribution game determines how xBZZ from postage stamp purchases is distributed among full staking nodes that store data. The system is designed so that **honestly storing assigned data** is the most profitable strategy. Rules for this process are encoded in the [redistribution smart contract](https://github.com/ethersphere/storage-incentives).  

Additionally, the game generates a **utilization signal**, which the price oracle uses to regulate postage stamp prices. Read more [here](/docs/concepts/incentives/redistribution-game).

### Price Oracle  

The price oracle contract dynamically adjusts postage stamp prices based on network utilization data from the redistribution contract. This mechanism ensures optimal redundancy by increasing or decreasing the price of storage as needed. [Read more](/docs/concepts/incentives/price-oracle).  

## Bandwidth Incentives  

Nodes in Swarm not only store data but relay data across the network. **Bandwidth incentives** compensate nodes for these services.  

The **Swarm Accounting Protocol (SWAP)** facilitates bandwidth payments between nodes, which can be settled either **in-kind** (data exchange) or via **cheques** processed through a **chequebook contract** on Gnosis Chain. Only full nodes can participate in SWAP. 

Read more [here](/docs/concepts/incentives/bandwidth-incentives).

---

## Postage Stamps

Postage stamps are used to pay for storing data on Swarm. They are purchased in batches, granting a prepaid right to store data on Swarm, similar to how real-world postage stamps pay for mail delivery.

When a node uploads data to Swarm, it 'attaches' postage stamps to each [chunk](/docs/concepts/DISC/) of data. The value assigned to a stamp indicates how much it is worth to persist the associated data on Swarm, which nodes use to prioritize which chunks to remove from their reserve first.

The value of a postage stamp decreases over time as if storage rent was regularly deducted from the batch balance. A stamp expires when its batch runs out of balance. Chunks with expired stamps cannot be used as proof in the redistribution game, meaning storer nodes will no longer receive rewards for storing them and can safely remove them from their reserves.

Postage stamp prices are dynamically set based on a utilization signal supplied by the price oracle smart contract. Prices will automatically increase or decrease according to the level of utilization. 

## Batch Buckets

Postage stamps are issued in batches with a certain number of storage slots partitioned into $$2^{bucketDepth}$$ equally sized address space buckets (bucket depth has a fixed value of 16). Each bucket is responsible for storing chunks that fall within a certain range of the address space. When uploaded, files are split into 4kb chunks, each chunk is assigned a unique address, and each chunk is then assigned to the bucket in which its address falls. 

### Bucket Size

Bucket depth determines how the address space is partitioned, with each bucket storing chunks that share a common address prefix. Each bucket contains a fixed number of slots, each capable of storing a stamped chunk. Once all slots in any single bucket are filled, the entire postage batch becomes fully utilized, preventing further uploads.

Together with `batch depth`, `bucket depth` determines how many chunks are allowed in each bucket. The number of chunks allowed in each bucket is calculated like so:

$$
2^{(batchDepth - bucketDepth)}
$$

So with a batch depth of 24 and a bucket depth of 16:

$$
2^{(24 - 16)} = 2^{8} = 256 \text{ chunks/bucket}
$$

:::info
Note that due how buckets fill as described above, a batch can become fully utilised before its theoretical maximum volume has been reached. See [batch utilisation section below](/docs/concepts/incentives/postage-stamps#batch-utilisation) for more information.
:::

## Batch Depth and Batch Amount

Each batch of stamps has two key parameters, `batch depth` and `amount`, which are recorded on Gnosis Chain at issuance. Note that these "depths" do not refer to the depth terms used to describe topology which are outlined [here in the glossary](/docs/references/glossary#depth-types).

### Batch Depth 

:::caution
The minimum value for `depth` is 17, however higher depths are recommended for most use cases due to the [mechanics of stamp batch utilisation](#batch-utilisation). See [the depths utilisation table](#effective-utilisation-table) to help decide which depth is best for your use case. 
:::

`Batch depth` determines how much data can be stored by a batch. The number of chunks which can be stored (stamped) by a batch is equal to  $$2^{batchDepth}$$. 

For a batch with a `batch depth` of 24, a maximum of $$2^{24} = 16,777,216$$ chunks can be stamped.   

Since we know that one chunk can store 4 kb of data, we can calculate the theoretical maximum amount of data which can be stored by a batch from the `batch depth`. 

$$
\text{Theoretical maximum batch volume} = 2^{batchDepth} \times \text{4 kb}   
$$

However, due to the way postage stamp batches are utilised, batches will become fully utilised before stamping the theoretical maximum number of chunks. Therefore when deciding which batch depth to use, it is important to consider the effective amount of data that can be stored by a batch, and not the theoretical maximum. The effective rate of utilisation increases along with the  batch depth. See [section on stamp batch utilisation below](/docs/concepts/incentives/postage-stamps#batch-utilisation) for more information.

### Batch Amount (& Batch Cost)

The `amount` parameter is the quantity of xBZZ in PLUR $$(1 \times 10^{16}PLUR = 1 \text{ xBZZ})$$ that is assigned per chunk in the batch. The total number of xBZZ that will be paid for the batch is calculated from this figure and the `batch depth` like so:

$$2^{batchDepth} \times {amount}$$

The paid xBZZ forms the `balance` of the batch. This `balance` is then slowly depleted as time ticks on and blocks are mined on Gnosis Chain.

For example, with a `batch depth` of 24 and an `amount` of 1000000000 PLUR:

$$
2^{24} \times 1000000000 = 16777216000000000 \text{ PLUR} = 1.6777216 \text{ xBZZ}
$$

### Calculating *amount* needed for desired TTL 

To calculate the required `amount`, divide the current postage price by the Gnosis block time (5 sec) and multiply by the desired storage duration in seconds. For the example below we assume a stamp price of 24000 PLUR / chunk / block:

:::info
The postage stamp price is dynamically determined according to a network utilisation signal. You can view the current storage price at [Swarmscan.io](https://swarmscan.io/).
:::

$$
(\text{stamp price} \div \text{block time in seconds}) \times \text{storage time in seconds}
$$

There are 1036800 seconds in 12 days, so the `amount` value required to store for 12 days can be calculated:

$$
(\text{24000} \div \text{5}) \times \text{1036800} = 4976640000
$$

So we can use 4976640000 as our `amount` value in order for our postage batch to store data for 12 days.
 

## Batch Utilisation

There are two types of postage stamp batches: immutable and mutable. Immutable batches permanently store data, while mutable batches allow overwriting older data as new chunks are added.

### Immutable Batches

Utilisation of an immutable batch is computed using a hash map of size $$2^{bucketDepth}$$ which is $$2^{16}$$ for all batches, so 65536 total entries. For the keys of the key-value pairs of the hash map, the keys are 16 digit binary numbers from 0 to 65535, and the value is a counter. 

![](/img/batches_01.png)

As chunks are uploaded to Swarm, each chunk is assigned to a bucket based the first 16 binary digits of the [chunk's hash](/docs/concepts/DISC/#chunks). The chunk will be assigned to whichever bucket's key matches the first 16 bits of its hash, and that bucket's counter will be incremented by 1. 

The batch is deemed "full" when ANY of these counters reach a certain max value. The max value is computed from the batch depth as such: $$2^{(batchDepth-bucketDepth)}$$. For example with batch depth of 24, the max value is $$2^{(24-16)}$$ or 256. A bucket can be thought of as have a number of "slots" equal to this maximum value, and every time the bucket's counter is incremented, one of its slots gets filled. 

In the diagram below, the batch depth is 18, so there are $$2^{(18-16)}$$ or 4 slots for each bucket. The utilisation of a batch is simply the highest number of filled slots out of all 65536 entries or "buckets". In this batch, none of the slots in any of the buckets have yet been filled with 4 chunks, so the batch is not yet fully utilised. The most filled slots out of all buckets is 2, so the stamp batch's utilisation is 2 out of 4. 

![](/img/batches_02.png)

As more chunks get uploaded and stamped, the bucket slots will begin to fill. As soon as the slots for any SINGLE bucket get filled, the entire batch is considered 100% utilised and can no longer be used to upload additional chunks.  

![](/img/batches_03.png)

### Mutable Batches

Mutable batches use the same hash map structure as immutable batches, however its utilisation works very differently. In contrast with immutable batches, mutable batches are never considered fully utilised. Rather, at the point where an immutable batch would be considered fully utilised, a mutable batch can continue to stamp chunks. However, if any chunk's address lands in a bucket whose slots are already filled, rather than the batch becoming fully utilised, that bucket's counter gets reset, and the new chunk will replace the oldest chunk in that bucket.

![](/img/batches_04.png)

Therefore rather than speaking of the number of slots as determining the utilisation of a batch as with immutable batches, we can think of the slots as defining a limit to the amount of data which can be uploaded before old data starts to get overwritten. 

### Which Type of Batch to Use

Immutable batches are suitable for long term storage of data or for data which otherwise does not need to be changed and should never be overwritten, such as records archival, legal documents, family photos, etc. 

Mutable batches are great for data which needs to be frequently updated and does not require a guarantee of immutability. For example, a blog, personal or company websites, ephemeral messaging app, etc.

The default batch type when unspecified is immutable. This can be modified through the Bee api by setting the `immutable` header with the [`\stamps POST` endpoint](https://docs.ethswarm.org/api/#tag/Transaction/paths/~1transactions~1%7BtxHash%7D/post) to `false`.
 
### Re-uploading 

There are several nuances to how the re-uploading of previously uploaded data to Swarm affect stamp batch utilisation. For single chunks, the behaviour is relatively straightforward, however with files that must get split into multiple chunks, the behaviour is less straightforward.

#### Single chunks

When a chunk which has previously been uploaded to Swarm is re-uploaded from the same node while the initial postage batch it was stamped by is still valid, no additional stamp will be utilised from the batch. However if the chunk comes from a different node than the original node, then a stamp WILL be utilised, and as long as at least one of the batches the chunk was stamped by is still valid, the chunk will be retained by storer nodes in its neighborhood.

#### Files 

When an identical file is re-uploaded then the stamp utilisation behaviour will be the same as with single chunks described in the section above. However, if part of the file has been modified and then re-uploaded, stamp utilisation behaviour will be different. This is due to how the chunking process works when a file is uploaded to Swarm. When uploaded to Swarm, files are split into 4kb sized chunks (2^12 bytes), and each chunk is assigned an address which is based on the content of the chunk. If even a single bit within the chunk is modified, then the address of the chunk will also be modified. 

When a file which was previously uploaded with a single bit flipped is again split into chunks by a node before being uploaded to Swarm, then only the chunk with the flipped bit will have an updated address and require the utilisation of another stamp. The content of all the other chunks will remain the same, and therefore will not require new stamps to be utilised. 

However, if rather than flipping a single bit we add some data to our file, this could cause changes in the content of every chunk of the file, meaning that every single chunk must be re-stamped. We can use a simplified example of why this is the case to more easily understand the stamp utilisation behaviour. Let us substitute a message containing letters of the alphabet rather than binary data.

Our initial message consists of 16 letters:

> abcdefghijklmnop 

When initially uploaded, it will be split into four chunks of four letters each:

> abcdefghijklmnop => abcd | efgh | ijkl | mnop

Let us look at what happens when a single letter is changed (here we change a to z):

> abcdefghijklmnop => zbcd | efgh | ijkl | mnop

In this case, only the first chunk is affected, all the other chunks retain the same content.

> Now let is examine the case where a new letter is added rather than simply modifying an already existing one. Here we add the number 1 at the start of the message: 

> 1abcdefghijklmnop => 1abc | defg | hijk | lmno | p

As you can see, by adding a single new letter at the start of the message, all the letters are shifted to the right by a single position, which a has caused EVERY chunk in the message to be modified rather than just a single chunk.

#### Affect on Batch Utilisation

The implications of this behaviour are that even a small change to the data of a file may cause every single chunk from the file to be changed, meaning that new stamps must be utilised for every chunk from that file. In practice, this could lead to high costs in data which is frequently changed, since for even a small change, every chunk from the file must be re-stamped. 

### Implications for Swarm Users

Due to the nature of batch utilisation described above, batches are often fully utilised before reaching their theoretical maximum storage amount. However as the batch depth increases, the chance of a postage batch becoming fully utilised early decreases. At batch depth 24, there is a 0.1% chance that a batch will be fully utilised/start replacing old chunks before reaching 64.33% of its theoretical maximum.

Let's look at an example to make it clearer. Using the method of calculating the theoretical maximum storage amount [outlined above](/docs/concepts/incentives/postage-stamps#batch-depth), we can see that for a batch depth of 24, the theoretical maximum amount which can be stored is 68.72 gb:

$$
2^{24+12} = \text{68,719,476,736 bytes} = \text{68.72 gb}
$$

Therefore we should use 64.33% the effective rate of usage for the stamp batch:

$$
\text{68.72 gb} \times{0.6433} =  \text{44.21 gb }
$$

## Effective Utilisation Tables

When a user buys a batch of stamps they may make the naive assumption that they will be able to upload data equal to the sum total size of the maximum capacity of the batch. However, in practice this assumption is incorrect, so it is essential that Swarm users understand the relationship between batch depth and the theoretical and effective volumes of a batch.

Columns:

* **Theoretical Volume:** The theoretical maximum volume which can be reached if the batch is completely utilized.
* **Effective Volume:** The actual volume which a batch can be expected to store based with a failure rate of less than or equal to 0.1% (1 in 1000)
* **Batch Depth:** The batch depth value.
* **Utilization Rate:** The percentage of the theoretical volume which can be expected to be used according to the effective volume.

:::info
The title of each table below states whether it is for encrypted or unencrypted uploads along with the erasure coding level.

[Erasure coding](/docs/concepts/DISC/erasure-coding) on Swarm has five named levels:

1. NONE
2. MEDIUM
3. STRONG
4. INSANE 
5. PARANOID
:::

### Unencrypted - NONE 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 44.70 kB | 17 | 0.01% |
| 1.07 GB | 6.66 MB | 18 | 0.61% |
| 2.15 GB | 112.06 MB | 19 | 5.09% |
| 4.29 GB | 687.62 MB | 20 | 15.65% |
| 8.59 GB | 2.60 GB | 21 | 30.27% |
| 17.18 GB | 7.73 GB | 22 | 44.99% |
| 34.36 GB | 19.94 GB | 23 | 58.03% |
| 68.72 GB | 47.06 GB | 24 | 68.48% |
| 137.44 GB | 105.51 GB | 25 | 76.77% |
| 274.88 GB | 227.98 GB | 26 | 82.94% |
| 549.76 GB | 476.68 GB | 27 | 86.71% |
| 1.10 TB | 993.65 GB | 28 | 88.37% |
| 2.20 TB | 2.04 TB | 29 | 92.88% |
| 4.40 TB | 4.17 TB | 30 | 94.81% |
| 8.80 TB | 8.45 TB | 31 | 96.06% |
| 17.59 TB | 17.07 TB | 32 | 97.01% |
| 35.18 TB | 34.36 TB | 33 | 97.65% |
| 70.37 TB | 69.04 TB | 34 | 98.11% |
| 140.74 TB | 138.54 TB | 35 | 98.44% |
| 281.47 TB | 277.72 TB | 36 | 98.67% |
| 562.95 TB | 556.35 TB | 37 | 98.83% |
| 1.13 PB | 1.11 PB | 38 | 98.91% |
| 2.25 PB | 2.23 PB | 39 | 98.96% |
| 4.50 PB | 4.46 PB | 40 | 98.98% |
| 9.01 PB | 8.93 PB | 41 | 99.11% |

### Unencrypted - MEDIUM 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 41.56 kB | 17 | 0.01% |
| 1.07 GB | 6.19 MB | 18 | 0.57% |
| 2.15 GB | 104.18 MB | 19 | 4.73% |
| 4.29 GB | 639.27 MB | 20 | 14.54% |
| 8.59 GB | 2.41 GB | 21 | 28.11% |
| 17.18 GB | 7.18 GB | 22 | 41.79% |
| 34.36 GB | 18.54 GB | 23 | 53.95% |
| 68.72 GB | 43.75 GB | 24 | 63.66% |
| 137.44 GB | 98.09 GB | 25 | 71.37% |
| 274.88 GB | 211.95 GB | 26 | 77.11% |
| 549.76 GB | 443.16 GB | 27 | 80.61% |
| 1.10 TB | 923.78 GB | 28 | 82.16% |
| 2.20 TB | 1.90 TB | 29 | 86.30% |
| 4.40 TB | 3.88 TB | 30 | 88.14% |
| 8.80 TB | 7.86 TB | 31 | 89.26% |
| 17.59 TB | 15.87 TB | 32 | 90.21% |
| 35.18 TB | 31.94 TB | 33 | 90.77% |
| 70.37 TB | 64.19 TB | 34 | 91.22% |
| 140.74 TB | 128.80 TB | 35 | 91.52% |
| 281.47 TB | 258.19 TB | 36 | 91.73% |
| 562.95 TB | 517.23 TB | 37 | 91.88% |
| 1.13 PB | 1.04 PB | 38 | 91.95% |
| 2.25 PB | 2.07 PB | 39 | 92.00% |
| 4.50 PB | 4.15 PB | 40 | 92.15% |
| 9.01 PB | 8.30 PB | 41 | 92.14% |

### Unencrypted - STRONG 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 37.37 kB | 17 | 0.01% |
| 1.07 GB | 5.57 MB | 18 | 0.51% |
| 2.15 GB | 93.68 MB | 19 | 4.25% |
| 4.29 GB | 574.81 MB | 20 | 13.07% |
| 8.59 GB | 2.17 GB | 21 | 25.26% |
| 17.18 GB | 6.46 GB | 22 | 37.58% |
| 34.36 GB | 16.67 GB | 23 | 48.50% |
| 68.72 GB | 39.34 GB | 24 | 57.24% |
| 137.44 GB | 88.20 GB | 25 | 64.17% |
| 274.88 GB | 190.58 GB | 26 | 69.33% |
| 549.76 GB | 398.47 GB | 27 | 72.48% |
| 1.10 TB | 830.63 GB | 28 | 73.85% |
| 2.20 TB | 1.71 TB | 29 | 77.59% |
| 4.40 TB | 3.49 TB | 30 | 79.27% |
| 8.80 TB | 7.07 TB | 31 | 80.34% |
| 17.59 TB | 14.27 TB | 32 | 81.12% |
| 35.18 TB | 28.72 TB | 33 | 81.63% |
| 70.37 TB | 57.71 TB | 34 | 82.01% |
| 140.74 TB | 115.81 TB | 35 | 82.29% |
| 281.47 TB | 232.16 TB | 36 | 82.48% |
| 562.95 TB | 465.07 TB | 37 | 82.61% |
| 1.13 PB | 931.23 TB | 38 | 82.67% |
| 2.25 PB | 1.86 PB | 39 | 82.71% |
| 4.50 PB | 3.73 PB | 40 | 82.78% |
| 9.01 PB | 7.46 PB | 41 | 82.79% |

### Unencrypted - INSANE 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 33.88 kB | 17 | 0.01% |
| 1.07 GB | 5.05 MB | 18 | 0.46% |
| 2.15 GB | 84.92 MB | 19 | 3.86% |
| 4.29 GB | 521.09 MB | 20 | 11.85% |
| 8.59 GB | 1.97 GB | 21 | 22.90% |
| 17.18 GB | 5.86 GB | 22 | 34.09% |
| 34.36 GB | 15.11 GB | 23 | 43.97% |
| 68.72 GB | 35.66 GB | 24 | 51.90% |
| 137.44 GB | 79.96 GB | 25 | 58.18% |
| 274.88 GB | 172.77 GB | 26 | 62.85% |
| 549.76 GB | 361.23 GB | 27 | 65.70% |
| 1.10 TB | 753.00 GB | 28 | 66.95% |
| 2.20 TB | 1.55 TB | 29 | 70.38% |
| 4.40 TB | 3.16 TB | 30 | 71.92% |
| 8.80 TB | 6.41 TB | 31 | 72.85% |
| 17.59 TB | 12.93 TB | 32 | 73.53% |
| 35.18 TB | 26.04 TB | 33 | 74.01% |
| 70.37 TB | 52.32 TB | 34 | 74.35% |
| 140.74 TB | 104.99 TB | 35 | 74.60% |
| 281.47 TB | 210.46 TB | 36 | 74.77% |
| 562.95 TB | 421.61 TB | 37 | 74.89% |
| 1.13 PB | 844.20 TB | 38 | 74.94% |
| 2.25 PB | 1.69 PB | 39 | 74.98% |
| 4.50 PB | 3.38 PB | 40 | 75.03% |
| 9.01 PB | 6.77 PB | 41 | 75.10% |

### Unencrypted - PARANOID 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 13.27 kB | 17 | 0.00% |
| 1.07 GB | 1.98 MB | 18 | 0.18% |
| 2.15 GB | 33.27 MB | 19 | 1.51% |
| 4.29 GB | 204.14 MB | 20 | 4.64% |
| 8.59 GB | 771.13 MB | 21 | 8.75% |
| 17.18 GB | 2.29 GB | 22 | 13.34% |
| 34.36 GB | 5.92 GB | 23 | 17.22% |
| 68.72 GB | 13.97 GB | 24 | 20.33% |
| 137.44 GB | 31.32 GB | 25 | 22.79% |
| 274.88 GB | 67.68 GB | 26 | 24.62% |
| 549.76 GB | 141.51 GB | 27 | 25.74% |
| 1.10 TB | 294.99 GB | 28 | 26.23% |
| 2.20 TB | 606.90 GB | 29 | 27.56% |
| 4.40 TB | 1.24 TB | 30 | 28.15% |
| 8.80 TB | 2.51 TB | 31 | 28.54% |
| 17.59 TB | 5.07 TB | 32 | 28.82% |
| 35.18 TB | 10.20 TB | 33 | 28.99% |
| 70.37 TB | 20.50 TB | 34 | 29.13% |
| 140.74 TB | 41.13 TB | 35 | 29.22% |
| 281.47 TB | 82.45 TB | 36 | 29.29% |
| 562.95 TB | 165.17 TB | 37 | 29.34% |
| 1.13 PB | 330.72 TB | 38 | 29.37% |
| 2.25 PB | 661.97 TB | 39 | 29.39% |
| 4.50 PB | 1.32 PB | 40 | 29.41% |
| 9.01 PB | 2.65 PB | 41 | 29.43% |

### Encrypted - NONE 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 44.35 kB | 17 | 0.01% |
| 1.07 GB | 6.61 MB | 18 | 0.60% |
| 2.15 GB | 111.18 MB | 19 | 5.05% |
| 4.29 GB | 682.21 MB | 20 | 15.52% |
| 8.59 GB | 2.58 GB | 21 | 30.04% |
| 17.18 GB | 7.67 GB | 22 | 44.62% |
| 34.36 GB | 19.78 GB | 23 | 57.56% |
| 68.72 GB | 46.69 GB | 24 | 67.93% |
| 137.44 GB | 104.68 GB | 25 | 76.16% |
| 274.88 GB | 226.19 GB | 26 | 82.29% |
| 549.76 GB | 472.93 GB | 27 | 86.02% |
| 1.10 TB | 985.83 GB | 28 | 87.66% |
| 2.20 TB | 2.03 TB | 29 | 92.25% |
| 4.40 TB | 4.14 TB | 30 | 94.21% |
| 8.80 TB | 8.39 TB | 31 | 95.37% |
| 17.59 TB | 16.93 TB | 32 | 96.22% |
| 35.18 TB | 34.09 TB | 33 | 96.88% |
| 70.37 TB | 68.50 TB | 34 | 97.34% |
| 140.74 TB | 137.45 TB | 35 | 97.67% |
| 281.47 TB | 275.53 TB | 36 | 97.89% |
| 562.95 TB | 551.97 TB | 37 | 98.05% |
| 1.13 PB | 1.11 PB | 38 | 98.13% |
| 2.25 PB | 2.21 PB | 39 | 98.18% |
| 4.50 PB | 4.43 PB | 40 | 98.36% |
| 9.01 PB | 8.86 PB | 41 | 98.37% |

### Encrypted - MEDIUM 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 40.89 kB | 17 | 0.01% |
| 1.07 GB | 6.09 MB | 18 | 0.56% |
| 2.15 GB | 102.49 MB | 19 | 4.65% |
| 4.29 GB | 628.91 MB | 20 | 14.30% |
| 8.59 GB | 2.38 GB | 21 | 27.68% |
| 17.18 GB | 7.07 GB | 22 | 41.15% |
| 34.36 GB | 18.24 GB | 23 | 53.09% |
| 68.72 GB | 43.04 GB | 24 | 62.63% |
| 137.44 GB | 96.50 GB | 25 | 70.21% |
| 274.88 GB | 208.52 GB | 26 | 75.86% |
| 549.76 GB | 435.98 GB | 27 | 79.30% |
| 1.10 TB | 908.81 GB | 28 | 80.82% |
| 2.20 TB | 1.87 TB | 29 | 84.98% |
| 4.40 TB | 3.81 TB | 30 | 86.67% |
| 8.80 TB | 7.73 TB | 31 | 87.84% |
| 17.59 TB | 15.61 TB | 32 | 88.74% |
| 35.18 TB | 31.43 TB | 33 | 89.34% |
| 70.37 TB | 63.15 TB | 34 | 89.74% |
| 140.74 TB | 126.71 TB | 35 | 90.03% |
| 281.47 TB | 254.01 TB | 36 | 90.24% |
| 562.95 TB | 508.85 TB | 37 | 90.39% |
| 1.13 PB | 1.02 PB | 38 | 90.47% |
| 2.25 PB | 2.04 PB | 39 | 90.51% |
| 4.50 PB | 4.08 PB | 40 | 90.64% |
| 9.01 PB | 8.17 PB | 41 | 90.65% |

### Encrypted - STRONG 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 36.73 kB | 17 | 0.01% |
| 1.07 GB | 5.47 MB | 18 | 0.50% |
| 2.15 GB | 92.07 MB | 19 | 4.18% |
| 4.29 GB | 564.95 MB | 20 | 12.85% |
| 8.59 GB | 2.13 GB | 21 | 24.86% |
| 17.18 GB | 6.35 GB | 22 | 36.97% |
| 34.36 GB | 16.38 GB | 23 | 47.67% |
| 68.72 GB | 38.66 GB | 24 | 56.26% |
| 137.44 GB | 86.69 GB | 25 | 63.07% |
| 274.88 GB | 187.31 GB | 26 | 68.14% |
| 549.76 GB | 391.64 GB | 27 | 71.24% |
| 1.10 TB | 816.39 GB | 28 | 72.59% |
| 2.20 TB | 1.68 TB | 29 | 76.34% |
| 4.40 TB | 3.43 TB | 30 | 77.89% |
| 8.80 TB | 6.94 TB | 31 | 78.86% |
| 17.59 TB | 14.02 TB | 32 | 79.71% |
| 35.18 TB | 28.23 TB | 33 | 80.23% |
| 70.37 TB | 56.72 TB | 34 | 80.60% |
| 140.74 TB | 113.82 TB | 35 | 80.88% |
| 281.47 TB | 228.18 TB | 36 | 81.06% |
| 562.95 TB | 457.10 TB | 37 | 81.20% |
| 1.13 PB | 915.26 TB | 38 | 81.26% |
| 2.25 PB | 1.83 PB | 39 | 81.30% |
| 4.50 PB | 3.67 PB | 40 | 81.43% |
| 9.01 PB | 7.34 PB | 41 | 81.45% |

### Encrypted - INSANE 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 33.26 kB | 17 | 0.01% |
| 1.07 GB | 4.96 MB | 18 | 0.45% |
| 2.15 GB | 83.38 MB | 19 | 3.79% |
| 4.29 GB | 511.65 MB | 20 | 11.64% |
| 8.59 GB | 1.93 GB | 21 | 22.52% |
| 17.18 GB | 5.75 GB | 22 | 33.50% |
| 34.36 GB | 14.84 GB | 23 | 43.19% |
| 68.72 GB | 35.02 GB | 24 | 50.96% |
| 137.44 GB | 78.51 GB | 25 | 57.12% |
| 274.88 GB | 169.64 GB | 26 | 61.71% |
| 549.76 GB | 354.69 GB | 27 | 64.52% |
| 1.10 TB | 739.37 GB | 28 | 65.74% |
| 2.20 TB | 1.52 TB | 29 | 69.15% |
| 4.40 TB | 3.10 TB | 30 | 70.56% |
| 8.80 TB | 6.29 TB | 31 | 71.48% |
| 17.59 TB | 12.70 TB | 32 | 72.18% |
| 35.18 TB | 25.57 TB | 33 | 72.67% |
| 70.37 TB | 51.37 TB | 34 | 73.00% |
| 140.74 TB | 103.08 TB | 35 | 73.24% |
| 281.47 TB | 206.65 TB | 36 | 73.42% |
| 562.95 TB | 413.98 TB | 37 | 73.54% |
| 1.13 PB | 828.91 TB | 38 | 73.59% |
| 2.25 PB | 1.66 PB | 39 | 73.62% |
| 4.50 PB | 3.32 PB | 40 | 73.72% |
| 9.01 PB | 6.64 PB | 41 | 73.74% |

### Encrypted - PARANOID 

| Theoretical Volume | Effective Volume | Batch Depth | Utilization Rate |
| ------------- | ------------- | ------------- | ------------- |
| 536.87 MB | 13.17 kB | 17 | 0.00% |
| 1.07 GB | 1.96 MB | 18 | 0.18% |
| 2.15 GB | 33.01 MB | 19 | 1.50% |
| 4.29 GB | 202.53 MB | 20 | 4.61% |
| 8.59 GB | 765.05 MB | 21 | 8.68% |
| 17.18 GB | 2.28 GB | 22 | 13.27% |
| 34.36 GB | 5.87 GB | 23 | 17.08% |
| 68.72 GB | 13.86 GB | 24 | 20.17% |
| 137.44 GB | 31.08 GB | 25 | 22.61% |
| 274.88 GB | 67.15 GB | 26 | 24.43% |
| 549.76 GB | 140.40 GB | 27 | 25.54% |
| 1.10 TB | 292.67 GB | 28 | 26.03% |
| 2.20 TB | 602.12 GB | 29 | 27.35% |
| 4.40 TB | 1.23 TB | 30 | 27.94% |
| 8.80 TB | 2.49 TB | 31 | 28.32% |
| 17.59 TB | 5.03 TB | 32 | 28.60% |
| 35.18 TB | 10.12 TB | 33 | 28.77% |
| 70.37 TB | 20.34 TB | 34 | 28.91% |
| 140.74 TB | 40.80 TB | 35 | 29.00% |
| 281.47 TB | 81.80 TB | 36 | 29.06% |
| 562.95 TB | 163.87 TB | 37 | 29.11% |
| 1.13 PB | 328.11 TB | 38 | 29.14% |
| 2.25 PB | 656.76 TB | 39 | 29.16% |
| 4.50 PB | 1.31 PB | 40 | 29.18% |
| 9.01 PB | 2.63 PB | 41 | 29.19% |

---

## Price Oracle

The job of the [oracle contract](https://github.com/ethersphere/storage-incentives/blob/master/src/PriceOracle.sol) is to set the price of postage stamps. The oracle contract uses data from the [redistribution contract](https://github.com/ethersphere/storage-incentives/blob/master/src/Redistribution.sol) in order to set the appropriate price for postage stamps through the [postage stamp contract](https://github.com/ethersphere/storage-incentives/blob/master/src/PostageStamp.sol). The data from the redistribution contract is used to calculate a "utilisation signal". This signal is an indicator of how much the Swarm network‚Äôs data storage capacity is being utilized. Specifically, the signal is a measure of data redundancy on the network. Redundancy is a measure of how many copies of each piece of data can be stored by the network. The protocol targets a fourfold level of data redundancy as a safe minimum. 

For example, if there is an increase in postage stamps being purchased while the number of nodes remains constant, the data redundancy level will begin to fall as data storers‚Äô available space begins to become reserved. If too many postage stamps are purchased without an equivalent increase in storage providers, the redundancy level may fall below four. In this case, the oracle will increase the price of postage stamps so that it becomes more expensive to store data on Swarm. The higher cost of storage will then lead to less postage stamps being purchased, and will push the redundancy level back up towards four. 

Conversely, if the amount of Stamps being purchased decreases while the number of storage provider nodes remains constant, the redundancy level will increase as there are fewer chunks of data to be distributed amongst the same number of nodes. In this case, the oracle will decrease the Postage Stamp price in order to promote more data storers to store their data on Swarm. The lower cost of storage will then lead to more Postage Stamps being purchased and push the redundancy level back down towards four.

---

## Redistribution Game

The redistribution game distributes xBZZ collected from [postage stamp](/docs/concepts/incentives/postage-stamps) purchases, rewarding nodes for providing storage. Redistribution rewards incentivize nodes to continue providing storage to the network. The game is designed so that the most profitable strategy for participants is to store their assigned data honestly.

### Redistribution Game Details

Uploading data to Swarm requires purchasing postage stamp batches with xBZZ. The collected xBZZ is later redistributed as rewards to storage nodes. Every 152 Gnosis Chain blocks ***a single [neighborhood](/docs/concepts/DISC/neighborhoods)*** is selected to play the redistribution game. For each round of the game, one node from the selected neighborhood will have the chance to win a reward which is paid out from the accumulated xBZZ. 

The game has 3 phases, `commit`, `reveal`, and `claim`. In the `reveal` phase of a previous game, an "anchor" address is randomly generated and used to determine the neighborhood for the current round. 

In the `commit` phase, nodes issue an on-chain transaction including an encrypted hash of the data they are storing (the unencrypted hash is known as the "reserve commitment") along with the [depth](/docs/references/glossary#2-area-of-responsibility-related-depths) for which they are reporting. This serves as an attestation of the data they are storing without revealing any other information.

In the `reveal` phase, each node reveals the decryption key for their encrypted hashes thereby publishing the hash. The winner is chosen at random among the honest nodes, but it is weighted in proportion to each node's stake density. Stake density is calculated as so:

$$
\text{stake density} = \text{stake(xBZZ)} \times {2}^\text{storage depth}
$$

### Penalties

During the `reveal` phase if a nodes' revealed hash does not match the honest nodes' hash, that node will be temporarily frozen and will not be able to participate in a number of upcoming rounds. Currently the freeze period is defined in the [redistribution smart contract](https://github.com/ethersphere/storage-incentives/blob/master/src/Redistribution.sol#L536C1-L536C100) as:

$$
152 \times 2^\text{storage radius} \text{ blocks (at 5s per block)}
$$

So for example at a storage radius of 10:

$$
152 \times 2^{10} \text{ blocks (at 5s per block)} ‚âà \text{ 9 days}
$$

---

## Introduction(Concepts)

Swarm is a peer-to-peer network of Bee nodes that collectively provide censorship-resistant decentralised storage and communication services. Swarm's mission is to enable a self-sovereign global society and permissionless open markets by providing scalable decentralized storage infrastructure for Web3. Its incentive system is enforced through smart contracts on the Gnosis Chain blockchain and powered by the xBZZ token, making it economically self-sustaining. 

## Bee Client

Bee is a Swarm client implemented in Go and serves as the foundation of the Swarm network. Bee nodes form a private, decentralized, and self-sustaining network for permissionless publishing and data storage. You can learn more about how Bee clients work by reading about the [concepts and protocols](/docs/concepts/what-is-swarm/) which underpin the Swarm network. To get hands on experience working with Swarm, you can start by learning how to [install and operate a Bee node](/docs/bee/installation/getting-started). 

## Swarm Foundation

The [Swarm Foundation](https://www.ethswarm.org/foundation) is dedicated to advancing open-source technology for decentralized data storage and exchange. It fosters a sustainable, independent ecosystem by supporting the development of free and open-source software (FLOSS) and empowering a community built around crypto-economic incentives for processing, distributing, and storing data.

Its mission is to champion digital freedom by promoting the Swarm network as the foundational layer of the fair data economy, while nurturing the community that sustains it.

To achieve this, the foundation provides financial grants and other forms of support, evaluated on a case-by-case basis.

---

## PSS

PSS, or Postal Service over Swarm, is a messaging protocol that enables users to send and receive messages over Swarm. It is an essential component of Swarm's infrastructure, providing secure, private, and efficient communication between nodes.

PSS is designed to be secure by encrypting messages for the intended recipient and wrapping them with a topic in a content-addressed chunk. The chunk is crafted in such a way that its content address falls into the recipient's neighborhood, ensuring that delivery is naturally taken care of by the push-sync protocol. This ensures that messages are delivered only to the intended recipient's neighborhood and cannot be intercepted or read by unauthorized parties. While the chunk will be delivered to all members of the recipient's neighborhood, only the recipient will be able to decrypt the message using their private key.

PSS also provides privacy by allowing users to receive messages from previously unknown identities. This makes it an ideal communication primitive for sending anonymous messages to public identities such as registrations or initial contact to start a thread by setting up secure communication.

Efficiency is another key feature of PSS. It uses direct node-to-node messaging in Swarm, which means that messages are delivered directly from one node to another without the need for intermediaries. This reduces latency and ensures that messages are delivered quickly and reliably.

PSS also supports mailboxing, which allows users to deposit messages for download if the recipient is not online. This ensures that messages are not lost if the recipient is offline when they are sent.

---

## What is Swarm?

import bos_fig_1_1 from '/static/img/bos_fig_1_1.jpg';

# What is Swarm?

The complete vision of Swarm is described in detail in [The Book of Swarm](https://papers.ethswarm.org/p/book-of-swarm/) written by Swarm founder Viktor Tron, with further high level details described in the [whitepaper](https://papers.ethswarm.org/p/whitepaper/). More in depth low level implementation details can be found in the [Swarm Specification paper](https://papers.ethswarm.org/p/swarm-specification/). The latest research and technical papers from Swarm can be found on the ["Papers" section](https://papers.ethswarm.org/) of the Ethswarm homepage.

Swarm is peer-to-peer network of nodes which work together to provide decentralised storage and communication infrastructure.

Swarm can be divided into four main parts:

1. Underlay Network - A peer-to-peer network protocol to serve as underlay transport. Swarm's underlay network is built with [libp2p](https://libp2p.io/). 
2. Overlay Network - An overlay network with protocols powering a distributed immutable storage of chunks (fixed size data blocks). 
3. Data Access Layer - A component providing high-level data access and defining APIs for base-layer features.
4. Application Layer - An application layer defining standards and outlining best practices for more elaborate use cases.

  
  
    Source: The Book of Swarm - Figure 1.1 - "Swarm‚Äôs Layered Design"
  

Of these four main parts, parts 2 and 3 form the core of Swarm.

### 1. Underlay Network

The first part of Swarm is a peer-to-peer network protocol that serves as the underlay transport. The underlay transport layer is responsible for establishing connections between nodes in the network and routing data between them. It provides a low-level communication channel that enables nodes to communicate with each other directly, without relying on any centralised infrastructure.

Swarm is designed to be agnostic of the particular underlay transport used, as long as it satisfies certain requirements described in The Book of Swarm.  

As the [libp2p](https://libp2p.io/) library meets all these requirements it has been used to build the Swarm underlay network.

### 2. Overlay Network

The second part of Swarm is an overlay network with protocols powering the [Distributed Immutable Store of Chunks (DISC)](/docs/concepts/DISC/). This layer is responsible for storing and retrieving data in a decentralised and secure manner.

Swarm's overlay network is built on top of the underlay transport layer and uses [Kademlia](/docs/concepts/DISC/kademlia) overlay routing to enable efficient and scalable communication between nodes. Kademlia is a distributed hash table (DHT) algorithm that allows nodes to locate each other in the network based on their unique identifier or hash.

Swarm's DISC is an implementation of a Kademlia DHT optimized for storage. While the use of DHTs in distributed data storage protocols is common, for many implementations DHTs are used only for indexing file references. Swarm's DISC distinguishes itself from other implementations by instead breaking files into chunks and storing the chunks themselves directly within the DHT.

Each chunk has a fixed size of 4kb and is distributed across the network using the DISC model. Each chunk has a unique address taken from the same namespace as the network node addresses that allows it to be located and retrieved by other nodes in the network.

Swarm's distributed immutable storage provides several benefits, including data redundancy, tamper-proofing, and fault tolerance. Because data is stored across multiple nodes in the network, it can be retrieved even if some nodes fail or go offline. 

Built on top of the overlay network is also an [incentives layer](/docs/concepts/incentives/overview) which guarantees that node operators which share their resources with the network are fairly rewarded for their services.

### 3. Data Access Layer

The third part of Swarm is a component that provides high-level data access and defines APIs for base-layer features. This layer is responsible for providing an easy-to-use interface for developers to interact with Swarm's underlying storage and communication infrastructure.

Swarm's high-level data access component provides [APIs that allow developers to perform various operations](/api/) on the network, including [uploading and downloading data](/docs/develop/upload-and-download) and searching for content. These APIs are designed to be simple and intuitive, making it easy for developers to build decentralised applications on top of Swarm.

### 4. Application Layer 

The fourth part of Swarm is an application layer that defines standards and outlines best practices for more elaborate use cases. This layer is responsible for providing guidance to developers on [how to build complex applications](/docs/develop/introduction) on top of Swarm's underlying infrastructure.

---

## Access Content

Accessing content on Swarm using Swarm Desktop is easy. All you need to get started is the Swarm hash for the content you wish to access. Whenever content is [uploaded to Swarm](/docs/desktop/upload-content) a Swarm hash is generated as a reference to that content. 

To access content on Swarm go to the ***Files*** tab and click ***Download***:

![](/img/access1.png)

From there, paste the Swarm hash for the content you want to access, and click ***Find***. We'll use the hash for a Swarm blog post explaining how to upload a website to Swarm:

`6843d3be17364ea0620011430e4db2a26ff781da478493a02d6eb5aae886b8ae`

![](/img/access2.png)

On the following screen you will see the data associated with the Swarm hash and see options for downloading (or browsing if it is a hash for a website):

![](/img/access3.png)

Click ***View Website*** to see the site in your browser, or ***Download*** to download the files:

![](/img/access4.png)

---

## Backup and Restore

## Create a Backup

To create a backup of your Bee node in Swarm Desktop, start by shutting down your node.

Right click the Bee icon in the System tray and select `Stop Bee` and then `Quit` to close and exit from Swarm Desktop:

![](/img/backup2.png)

Next navigate to the `Settings` tab in the app and copy the location of the data directory as indicated in the `Data DIR` field: 

![](/img/backup1.png)

Navigate to the directory you just copied and create copies of all the files in that directory (`\data-dir`), including `localstore`, `statestore`, `stamperstore`, `kademlia-metrics` and `keys` folders and store them in a secure and private location. 

![](/img/backup7.png)

In addition to the data folders, you will also need the password found in the `config.yaml` file in order to restore a Bee node from backup. Move up one directory from `Data DIR` to the `Data` directory, and create a copy of the `config.yaml` file and save it along with the other folders you just backed up:

![](/img/backup4.png)

Alternatively you may open the `config.yaml` and save the password as a text file along with the rest of your backup files:

![](/img/backup5.png)

Your completed backup should contain all the files from your data directory as well as your password (either in your `config.yaml` file or as a separate file or written down.)

![](/img/backup8.png)

### Back-up Gnosis Chain Key Only

If you only wish to back-up your Gnosis Chain key, navigate to the `\data-dir\keys` directory, and copy the `swarm.key` to a safe location:

![](/img/backup9.png)

You also need the password found in the `config.yaml` file in order to access your Gnosis Chain account. Move up one directory from `Data DIR` to the `Data` directory, and create a copy of the `config.yaml` file and save it along with the other folders you just backed up:

![](/img/backup4.png)

Alternatively you may open the `config.yaml` and save the password as a text file along with the rest of your backup files:

![](/img/backup5.png)

## Restore from Backup

To restore from backup, begin with a [new install](/docs/desktop/install) of Swarm Desktop. Once the installation process is finished, navigate to the `Settings` tab in the app and copy the install file directory as indicated in the `Data DIR` field:

![](/img/backup1.png)

Before navigating to the directory you just copied, right click the Bee icon in the System tray and select `Stop Bee` and then `Quit` to close and exit from Swarm Desktop:

![](/img/backup2.png)

Next open your file explorer and navigate to the directory you just copied. Delete any files present in the directory, and replace them with your own backup copies (excluding the `config.yaml` / password file):

![](/img/backup7.png)

Move up one directory from `Data DIR` to `Data`, and replace delete the `config.yaml` file and replace it with the `config.yaml` file from your backup. 

Alternatively if you have saved just the password and not the entire config file, open the default `config.yaml` file in a text editor such as VS Code or a plain text editor:

![](/img/backup4.png)

![](/img/backup5.png)

Replace the `password` string with your own password which you saved from the `config.yaml` backup.

Restart Swarm Desktop and check to see if the backup was restored successfully:

![](/img/backup6.png)

### Restore Gnosis Chain Account

If you only wish to access your Gnosis Chain account, you can [follow these instructions](/docs/bee/working-with-bee/backups#metamask-import) for exporting to Metamask in order to access your account.

---

## Configuration(Desktop)

## Setting RPC Endpoint

In order to interact with the Gnosis Chain to buy stamps, participate in staking, and manage assets such as xBZZ, Bee nodes require a valid Gnosis Chain RPC endpoint. By default the RPC endpoint is set to https://xdai.fairdatasociety.org, however any valid Gnosis Chain RPC endpoint may be used. 

To modify the RPC endpoint, first navigate to the ***Settings*** tab:

![](/img/config1.png)

From the ***Settings*** tab, expand the API Settings section and click the pen button next to Blockchain RPC URL to edit the default RPC. You can choose any valid Gnosis Chain RPC, either from your own Gnosis node or a service provider. You can find a list of paid and free RPC options from the [Gnosis Chain docs](https://docs.gnosischain.com/tools/RPC%20Providers/). For this example we will use the free endpoint - *https://xdai.fairdatasociety.org*.

:::warning
Other ***free public RPC endpoints are discouraged,*** since they may enforce rate limiting or may not store the historical smart contract data required by Bee nodes. [Read more](/docs/bee/working-with-bee/configuration#setting-blockchain-rpc-endpoint).
:::

![](/img/config2.png)

Click ***Save and Restart*** to finish changing the RPC endpoint.

## Upgrading from an Ultra-light to a Light Node

Bee ultra-light nodes are limited to only downloading small amounts of data from Swarm. In order to download greater amounts of data or to upload data to Swarm you must upgrade to a light node. To do this we need to first fund our Swarm Desktop Bee node with some xDAI (DAI bridged from Ethereum to Gnosis Chain which serves as Gnosis Chain's native token for paying transaction fees) in order to pay for the Gnosis Chain transactions required for setting up a light node.

### Bridging Ethereum DAI to Gnosis Chain as xDAI

If you already have some xDAI on a Gnosis Chain address, skip to the next step ***Funding Node with xDAI***. If you have DAI on Ethereum and need to swap it for xDAI, you can use one of the [Gnosis Chain Bridge](https://bridge.gnosischain.com/)

Five to ten xDAI is plenty to get started.

### Funding Node with xDAI

Once you have a few xDAI in your Gnosis Chain address, to fund your Bee node you need to send it from your wallet to your Swarm Desktop wallet. You can find your address from the ***Account*** tab of the app.

![](/img/config3.png)

Next simply send your xDAI to that address. Before sending, make sure you have set your wallet to use the Gnosis Chain network and not the Ethereum mainnet. If Gnosis Chain is not included as default selectable network in your wallet, you may need to add the network manually. You can use this configuration to add Gnosis Chain:

| Field         | Value     |
|--------------|-----------|
|**Network name:**|Gnosis|
| **New RPC URL:** | https://xdai.fairdatasociety.org |
| **Chain ID:**| 100 |
| **Symbol:**|  xDai   |
| **Block Explorer URL (Optional):**|  https://blockscout.com/xdai/mainnet   |

![](/img/config4.png)

The transaction should be confirmed in under a minute. We can check on the ***Account*** page to see when the xDAI has been received:

![](/img/config5.png)

### Set Up Wallet 

Now with some xDAI in the Swarm Desktop wallet, we can upgrade our Bee node from ultra-light to a light node. Completing the setup process will swap xDAI for some xBZZ at the current price, and will issue the transactions needed to set up the chequebook contract.

To get started, navigate to the ***Info*** tab and click the ***Setup wallet*** button. 
![](/img/config10.png)
Click ***Use xDAI***.
![](/img/config6.png)
Confirm that you have sufficient xDAI balance and click ***Proceed***.
![](/img/config7.png)
Click ***Swap Now and Upgrade***.
![](/img/config8.png)
Wait for the upgrade to complete.
![](/img/config9.png)
After the upgrade is complete, you will see several new sections within the ***Account*** tab: ***Chequebook***, ***Stamps***, and ***Feeds***.
 
##  Fund Chequebook

After setting up your wallet you will have access to the ***Chequebook*** section from the ***Accounts*** tab. From here you can manage your chequebook for your Swarm Desktop Bee node.

---

## Install

## Download and Install Swarm Desktop

Installing the Swarm Desktop app takes only a few clicks. To get started, simply download and install the Swarm Desktop app for your operating system. Installers are available for Windows, Linux, and OSX. You can find download links for Swarm Desktop at the Swarm [homepage](https://www.ethswarm.org/build/desktop) and you can find installers for specific operating systems at the [releases page](https://github.com/ethersphere/swarm-desktop/releases) of the Swarm Desktop GitHub repo.

:::caution
Swarm Desktop is in Beta and currently includes the Sentry application monitoring and bug reporting software which automatically collects data in order to help improve the software.
:::

:::caution
This project is in beta state. There might (and most probably will) be changes in the future to its API and working. Also, no guarantees can be made about its stability, efficiency, and security at this stage.
:::

[![](/img/desktop-homepage-dl.png)](https://www.ethswarm.org/build/desktop) 
*Ethswarm.org Swarm Desktop Page*

[![](/img/desktop-releases-dl.png)](https://github.com/ethersphere/swarm-desktop/releases)
*Swarm Desktop GitHub Releases Page*

After running the installer, a window will pop up and display the installation status:

![](/img/desktop-install-downloading.png)

Once the installation is complete, Swarm Desktop will open up in your default browser in a new window to the "Info" tab of the app:

![](/img/desktop-new-install.png)

If the installation went smoothly, you should see the message "Your node is connected" above the "Access Content" button along with a status message of "Node OK".

#### What Just Happened?

Running the Swarm Desktop app for the first time set up a new Bee node on your system. The installation process generated and saved private keys for your node in the Swarm Desktop's data directory. Those keys were used to start up a new Bee node in ultra-light mode. 

:::warning
If your Swarm Desktop files are accidentally deleted or become corrupted you will lose access to any assets or data which are secured using those keys. Make sure to [backup your keys](/docs/desktop/backup-restore).
:::

### "Ultra-light" and "Light" Nodes

Swarm Desktop by default starts up a node in "ultra-light" mode. When running in ultra-light mode Swarm Desktop  limited to only downloading data from Swarm. Moreover, it's limited to downloading only within the free threshold allowed by other nodes. For instructions on switching to light mode see the [configuration section](/docs/desktop/configuration).

## Tour of Swarm Desktop

### Info Tab

The "Info" tab gives you a quick view of your Swarm Desktop's status. From here we can quickly see if the node is connected to Swarm, whether the node is funded, and whether its chequebook contract is set up. On a new install of Swarm Desktop, the node should be connected, but the wallet and chequebook will not have been set up yet.

![](/img/swarm-desktop-info-tab.png)

### Files Tab

From "Files" tab you can input a Swarm hash in order to download the file associated with the hash. See this full [guide for downloading](/docs/desktop/access-content) using Swarm Desktop.

![](/img/swarm-desktop-files-tab.png)

### Account Tab

From the "Account" tab you can view your Swarm Desktop node's Gnosis Chain address and associated xBZZ and xDAI balances.

![](/img/swarm-desktop-account-tab.png)

### Settings Tab

From the "Settings" tab you can view important settings values. Note that the Blockchain RPC URL and ENS resolver URL are already filled in, and only the Blockchain RPC URL is modifiable through this tab. If you wish to modify other settings see the [ configuration page](/docs/desktop/configuration) for detailed instructions.

![](/img/swarm-desktop-settings-tab.png)

### Status Tab

From the "Status" tab you can see a quick overview of the health of your Swarm Desktop's Bee node.

![](/img/swarm-desktop-status-tab.png)

---

## Introduction(Desktop)

![](/img/swarm-desktop.png)

While running Bee from the terminal is a powerful and flexible approach for developers and node operators, it may not be the best option for more simple use cases.

The Swarm Desktop app is an alternative which provides an easy-to-use graphical user interface for running a Bee node and interacting seamlessly with the Swarm network. 

The Swarm Desktop App was designed to simplify the Swarm onboarding process so that anyone can benefit from decentralized storage while maintaining privacy and control over their data. Available for Windows, Mac, and Linux operating systems, the Swarm Desktop App serves as a personal gateway to the Swarm network.

---

## Postage Stamps(Desktop)

:::info
Swarm Desktop must be configured as a light node in order to access stamp related features. If you have not already upgraded from the default ultra-light configuration, complete the upgrade by following the ***[instructions here](/docs/desktop/configuration#upgrading-from-an-ultra-light-to-a-light-node)***.
:::

Postage stamps are required in order to upload data to Swarm. Postage stamps are purchased by interacting with the Swarm postage stamp smart contract on Gnosis Chain. Postage stamps are not purchased one by one, rather they are purchased in batches only.

## How to Buy a Postage Stamp Batch

Stamps can be purchased by selecting ***Stamps*** from the ***Account*** tab:

![](/img/stamps1.png)

And then clicking the ***Buy New Postage Stamp*** button:

![](/img/stamps2.png)

### Depth and Amount

Batch [depth and amount](/docs/concepts/incentives/postage-stamps) are the two required parameters which must be set when purchasing a postage stamp batch. Depth determines how many chunks can be stamped with a batch while amount determines how much xBZZ is assigned per chunk.

![](/img/stamps3.png)

Inputting a value for depth allows you to preview the upper limit of data which can be uploaded for that depth. 

Inputting a value for amount and depth together will allow you to also preview the total cost of the postage stamp batch as well as the TTL (time to live - how long the batch can store data on Swarm). Click the ***Buy New Stamp*** button to purchase the stamp batch.

![](/img/stamps4.png)

After purchasing stamps you can view stamp details from the ***Postage Stamps*** drop down menu:

![](/img/stamps5.png)

## Managing Postage Batches

After purchasing a postage batch, it is important to monitor the usage and TTL (time to live) of your batch. 

TTL is shown next to the "Expired in" label in the screenshot below. 

![](/img/stamps6.png)

For this stamp batch, it has only 6 hours left. Once the TTL has run out completely, the content uploaded using that batch will no longer be kept on Swarm, and will be lost forever. To prevent this from happening, you can "top up" your batch by adding more xBZZ to the batch balance to increase the batch TTL.

## Top-up a Batch

To get started, click on the "Topup and Dilute" button. 

![](/img/stamps7.png)

From the "Action" dropdown menu, make sure that you have "Topup" selected and then fill in the `amount` by which you wish to top up the batch. Note that the number entered here is in PLUR (1e-16 xBZZ), and it is the same `amount`` parameter described in the [section above](/docs/desktop/postage-stamps#depth-and-amount) on purchasing postage stamp batches, it is NOT equal to the total amount of xBZZ spent for this top up transaction. 

After inputting the `amount`, click "Topup" to submit the transaction.

After a few moments, you will see a notice that the transaction was successful in a green alert box. A few moments after that, you will see the updated TTL in the stamp details window.

![](/img/stamps8.png)

## Dilute a Batch

If our batch begins to come close to becoming fully utilised, we can choose to increase the `depth` of the batch to increase the amount of data it can store. This is referred to as "dilution", since by increasing the `depth` without updating the `amount`, we dilute the amount of xBZZ which is assigned to each chunk. In other words, the dilute transaction will increase the amount which can be uploaded by a batch while also ***decreasing*** the TTL. Therefore it is important to both top up and also dilute your stamp batch if you wish to increase the amount stored by the batch without decreasing its TTL.

To get started, click on the "Topup and Dilute" button. Make sure to select "Dilute" from the "Action" dropdown menu.

![](/img/stamps9.png)

From here, we can select the new `depth` value for our postage stamp batch. In this instance, we will increase it from 20 to 21.

![](/img/stamps10.png)

After a few moments the transaction will be completed and you should see the updated Depth, Capacity, and TTL.

![](/img/stamps10.png)

Note that both the Depth and Capacity have increased while the TTL has decreased.

---

## Publish a Website

## Step by Step Guide 

### Install Swarm Desktop and Deposit Funds

First, download and [install the Swarm Desktop App](/docs/desktop/install). Next, add xDAI (transaction fees) to your Node Wallet address. If you possess xBZZ (storage fees), you can deposit it alongside the xDAI. Otherwise, you can exchange your xDAI for xBZZ using the Swarm Desktop app.

Follow these steps to deposit funds:

  1. Launch the Swarm Desktop App and go to the Account section in the left menu.
  2. Transfer xDAI to your node wallet address. For safety, we suggest sending no more than 5 to 10 xDAI.
  3. After funding your wallet, click the Top Up Wallet button on the right side of the screen.
  4. Select the Use xDAI option.
  5. Verify your xDAI balance and click Proceed.
  6. Specify the amount of xDAI to convert to xBZZ and click Swap Now.
  7. Your Node Wallet address will be credited with xBZZ.

![](/img/upload-a-website1.gif)

### Setup Chequebook

Your node address is now funded with xDAI and xBZZ. However, to upload data on Swarm, you will need to transfer your funds to the Chequebook contract address.

Follow these steps:

  1. Go to the Account section in the left menu.
  2. Select the Chequebook tab in the top menu.
  3. Click the Deposit button.
  4. Specify the amount of xBZZ to deposit into your Chequebook, which will be used for storage costs.

### Publish Website

To publish your website on Swarm, follow these steps:

  1. Navigate to the Files tab.
  2. Click the Add Website button.
  3. Select your website folder. NOTE: The index.html file should be in the root folder.
  4. Purchase a Postage Stamp to publish your page. NOTE: Postage stamps cover storage costs for a specified duration.
  5. Upload the website.

![](/img/upload-a-website2.gif)

[https://api.gateway.ethswarm.org/bzz/6843d3be17364ea0620011430e4db2a26ff781da478493a02d6eb5aae886b8ae/](https://api.gateway.ethswarm.org/bzz/6843d3be17364ea0620011430e4db2a26ff781da478493a02d6eb5aae886b8ae/)

### Connecting an ENS Domain to Your Website

Associating your ENS domain with a Swarm hash generates a memorable, user-friendly identifier for your website, allowing users to easily locate and access your website without having to recall a lengthy, complex Swarm hash.

Initially, you‚Äôll need to register your domain name. To register and manage your ENS domain, you can use the ENS Domains Dapp along with the MetaMask browser extension.

After registering your name and connecting MetaMask to the relevant Ethereum account, set the resolver to use the public ENS if you haven‚Äôt already.

  1. Navigate to My Names and select the name you want to link to your Swarm content.
  2. Click on ADD/EDIT RECORD.
  3. From the "add record" dropdown menu, select Content.
  4. Enter your Swarm Hash, beginning with "bzz://" and click "Save."

![](/img/upload-a-website3.gif)

Your website is now available on:

[https://api.gateway.ethswarm.org/bzz/swarm-devrel.eth/](https://api.gateway.ethswarm.org/bzz/swarm-devrel.eth/)

### Update the Website: Set up and update a feed

Swarm feeds allow you to easily create a permanent address for your content stored on Swarm that you can update at any time.

If you plan to update your website in the future, it‚Äôs recommended that you set up a ‚ÄúFeed‚Äù before uploading your website to Swarm. This way, the Swarm Hash connected to your ENS domain will stay the same, even as you change the content behind that hash. This will enable you to update your website‚Äôs content without changing the Swarm Hash and incurring Ethereum transaction costs each time you do so.

#### Set up a Feed:

  1. Navigate to to Account
  2. Click on Feeds in the top menu
  3. Click on Create New Feed
  4. Define Identity name
  5. And click Create Feed.

#### Upload Website on Swarm and connect it to the Feed:

  1. Navigate to to Account
  2. Click on Feeds in the top menu
  3. Choose the Feed you want to update
  4. Click View Feed Page
  5. Click the Add Website button.
  6. Select your website folder. NOTE: The index.html file should be in the root folder.
  7. Add Postage Stamp to publish your page. NOTE: Postage stamps cover storage costs for a specified duration.
  8. Upload the website to your Node.
  9. Connect Feed hash to ENS domain as described above.

![](/img/upload-a-website4.gif)

By following these instructions, you can now leverage the benefits of decentralised storage, maintain a censorship-resistant website, and create a user-friendly experience by connecting your site to an ENS domain.

---

## Start a Blog

## A Guide to Starting Your Blog on Swarm

There are many different approaches to starting a blog on Swarm, however the easiest is to use the Etherjot Web blogging tool. Etherjot Web is a straightforward tool for publishing and editing your blog on Swarm. It handles all uploading of files, page customization, basic UI template, and even comes with a "comments" feature so any other Swarm user can leave a comment on your blog.

## Requirements

* [Swarm Desktop](/docs/desktop/install) with a [valid postage stamp batch](/docs/desktop/postage-stamps)

## Getting Started

To get started you must first have installed Swarm Desktop and have it running on your computer with a [valid stamp batch](/docs/desktop/postage-stamps). Note that your blog will only stay online as long as the postage batch is still valid, therefore you must make sure to stay aware of the postage batch TTL (time to live), and [top up your batch](/docs/desktop/postage-stamps#top-up-a-batch) regularly in order to keep your content online.

### Open Etherjot

To open Etherjot, right click the Swarm Desktop icon in your dashboard and navigate to "Apps", and then click on "Etherjot".

![](/img/etherjot27.png)

## Initialize Your Blog

When first starting Etherjot Web, you will be greeted with this page:

![](/img/etherjot1.png)

On this page, as long as you have fulfilled the requirements outlined above, you will see two green checkmarks confirming you have Swarm Desktop running with a valid postage stamp batch. You will also see a warning reminding you of the importance of [topping up your stamp batch](/docs/desktop/postage-stamps#top-up-a-batch) to prevent the batch TTL from running out. 

:::danger
In addition to monitoring your postage stamp batch TTL, it is also important that you back up your blog, or else you may lose access to your blog in Etherjot (although it will still remain live on Swarm as long as its stamp batch has not expired). 
:::

Fill in your blog name, check the box with the TTL warning, and click the "Create" button to initialize your blog. This will issue a Swarm transaction to set up a feed for your blog. The transaction will take a few moments, after which you will be greeted with the Etherjot Web blog editor.

![](/img/etherjot17.png)

The "Swarm Hash" displayed at the top of the editor is the address for the homepage of your blog. Click "Open" to navigate to your blog. We can see now that the blog has been initialized, but no content has been uploaded. 

![](/img/etherjot18.png)

## Don't Lose Your Work!

Due to the decentralised nature of Swarm and applications built on Swarm, there are several precautions you should take which you may be unfamiliar with when coming from a Web 2.0 application. 

### Back-up Your Blog

No username and password are required for editing your blog and uploading new posts. However, you do need to make sure to back up your blog in order to prevent losing access to it. You should do this after initializing your blog, and you should also back up your blog again after publishing any changes. To back up your blog, start by clicking "Settings."  

![](/img/etherjot21.png)

From the Settings page, click "Export."

![](/img/etherjot22.png)

Copy the displayed text to a `.json` file, make certain to copy the entire displayed text. This is your backup file and is used to import your blog. Note that the backup contains the private key of your blog, so should not be revealed to anyone else. 

![](/img/etherjot23.png)

### Avoid Losing Changes (DANGER)

Etherjot currently does not allow you to save drafts locally, so if you navigate away from the blog post you are currently editing, you will lose any changes you have made which have not yet been uploaded to Swarm. Take note of the three UI elements highlighted in the screenshot - using the "+" or "Settings" buttons will cause you to lose any changes not uploaded to Swarm, and hitting the "Reset" button will cause you to lose everything which has not been backed up.

:::danger
Hitting the "Reset" button will cause you to lose any content which has not yet been published and [backed up](/docs/desktop/start-a-blog#back-up-your-blog). 
:::

![](/img/etherjot19.png)

If you click the "+" button or the "Settings" button, you will see a warning to notify you that any unsaved changes will be lost.

![](/img/etherjot20.png)

You will NOT see a warning for refreshing your browser page, however, so be careful not to refresh your browser before publishing any changes to Swarm.

## Writing Your Blog

### Add Some Text

The text editor for your blog has two main panels. The one on the left is where you can write your content using [Markdown](https://www.markdownguide.org/). On the right side is where you can see a preview of your rendered markdown as it will appear to a visitor to your blog.

![](/img/etherjot3.png)

Let's fill in some content and examine the preview.

![](/img/etherjot4.png)

Here you can see the new content we just wrote, note that there is no auto-save functionality, so any changes we make will not be saved until we click "Publish" to upload the changes to Swarm. However you will see that the "Publish" button is greyed currently, as we have not yet filled in all the required fields for publishing.

### Add Media Files

Next let's try to add an image. To get started, we need to click the "Asset Browser" button.

![](/img/etherjot2.png)

This will open up the Asset Browser where you can manage your blog assets such as images.

![](/img/etherjot5.png)

To upload your file, click the "browse" button and choose the file you wish to upload.

:::info
In addition to images, video and audio files may also be uploaded, however currently the URL to the Swarm hash must be manually inserted into html `<video>` and `<audio>` tags such as:

```
<video controls width="630" height="300" src="http://localhost:1633/bzz/f8abc6161fb6305437d2bf514d3a34ce32c234eddad1ca907d438a6f4f43183b/"></video>

<audio controls width="630" height="300" src="http://localhost:1633/bzz/7613d1b9e5d409301b6aaaf7b6a41f0457cd4130471d87de6b49bced70aaafce/"></audio>
```
This will become automatic in a near future update to Etherjot Web.
:::

After selecting your file and clicking "OK", your file will be uploaded to Swarm, this will take a few moments.

![](/img/etherjot6.png)

Once it has finished uploading, you will then be able to view your file in the Asset Browser.

![](/img/etherjot7.png)

To add the image to your blog, simple click the "Insert" button.

![](/img/etherjot8.png)

After hitting "Insert", the URL with the Swarm hash of the file you just uploaded will be automatically inserted into a markdown image tag, and will then become viewable in the preview panel on the right.

### Set Background Image

To set the banner image which will display as the preview image for your blog, click the "Select" button under the "Banner image" label. This will open up the same Asset Browser mentioned in the section above on adding media, and you may choose an image from there or upload a new one.

![](/img/etherjot11.png)

### Set Blog "Type"

Next let's set the "Type" of our blog. "Type" determines where within the layout your blog post will appear. Simply click the dropdown button next to "Type" and make your choice.

![](/img/etherjot13.png)

### Set Blog "Category" (REQUIRED)

:::info
Note that "Category" must be set in order to publish any blog post.
:::

Next we must also choose the "Category" for your blog post. This is used to group articles within your blog. You may choose whatever name you wish for the blog category.

![](/img/etherjot14.png)

### Publishing Your Blog

Now we are ready to publish our first blog post! Simply click the "Publish" button to initiate the transaction on Swarm to upload your blog.

![](/img/etherjot15.png)

Once your blog has finished uploading, you will see it in the panel on the left. From here you can click "Edit" or "Delete" to continue editing your post or delete it.

![](/img/etherjot16.png)

### Add a New Post

To add a new post, click the "+" button. Note that this will discard any changes from the current post you are editing which have not yet been published. Make certain to publish any changes you want saved first.

![](/img/etherjot24.png)

## Settings and Optional Features
Click "Settings" to open up the Settings page. Note that any unpublished changes will be lost. 

![](/img/etherjot21.png)

From here you can [back up](/docs/desktop/start-a-blog#back-up-your-blog) and restore blogs, and can set a variety of other options. 

### Setting Custom Text and Links

Highlighted in this first screenshot you can see the options for setting custom text and links.

![](/img/etherjot26.png)

### Changing Default Text

## Reset Your Blog (DANGER)

:::danger
Hitting the "Reset" button will cause you to lose any content which has not yet been published and backed up. Make sure you have [backed up your blog](/docs/desktop/start-a-blog#back-up-your-blog) before clicking "reset!"
:::

To reset your blog and start a new blog, click the "Reset" button. This will immediately reset your blog, and you will lose any changes which have not yet been published. You will ALSO lose anything which you have not backed up, so it is important to [back up your blog](/docs/desktop/start-a-blog#back-up-your-blog) before resetting.

![](/img/etherjot25.png)

---

## Upload Content

After [purchasing a batch of postage stamps](/docs/desktop/postage-stamps) you will be able to upload files. First go to the ‚ÄúFiles‚Äù tab. Here you can choose between three options, depending on what you want to upload: a single file, a folder or a website. After choosing your option you‚Äôll need to add a postage stamp.

![](/img/upload1.png)

Click ‚ÄúAdd postage stamp‚Äù and choose a postage stamp.

![](/img/upload2.png)

![](/img/upload3.png)

Click ‚ÄúProceed with the selected stamp‚Äù and upload your data.

![](/img/upload4.png)

![](/img/upload5.png)

![](/img/upload6.png)

Once your file is uploaded a Swarm hash and Swarm Gateway link will be displayed (which you can use to share the file with others) along with other pertinent information.

---

## Add Access Control

:::info
This is guide contains a detailed explanation of how to use the ACT feature, but does not cover its higher level concepts. To better understand how ACT works and why to use it, read [the ACT page in the "Concepts" section](/docs/concepts/access-control).
:::

In this section we'll provide information on how to use the **swarm-cli** to upload, download data with ACT or update the grantee list. 

### Upload

Uploading data without ACT to the network remains unchanged.

To upload with ACT use the **act** and **act-history-address** flags following the **upload** command:
```bash
swarm-cli upload test.txt --act --stamp $stamp_id --act-history-address $swarm_history_address
```

Here **act** indicates that the file provided shall be uploaded using ACT.
The **act-history-address** flag is the reference of the historical version of the ACT. It can be omitted, in which case the data is uploaded to a new history. If provided, then the data will be uploaded to that history as the latest version. In both cases the timestamp of the upload is taken as the key of the history entry.
If the provided **act-history-address** is invalid then the request will fail with a not found error.

The response returns the newly created reference encrypted with ACT and the header contains history reference.

### Download

Downloading data which was uploaded without ACT from the network remains unchanged.

To download with ACT use the **act**, **act-publisher**, **act-timestamp** and **act-history-address** flags following the **download** command:
```bash
swarm-cli download $swarm_hash test.txt --act --act-history-address $swarm_history_address --act-publisher $public_key --timestamp $timestamp
```
Here **act** indicates that the **swarm_hash** shall be decrypted using the content publisher's public key as **act-publisher** and the lookup table mentioned above. The **act-history-address** flag is the reference of the historical version of the ACT based on the timestamp provided, however the **act-timestamp** flag can be omitted in which case the current timestamp is used.

If the **act-history-address** or **act-publisher** flags are omitted then the request is treated as a "usual" download.
If the data was uploaded with ACT and we try to download it without the ACT flags then the request will fail with a not found error.

### Grantee management

Updating a grantee list literally means patching a json file containing the list of grantee swarm public keys.

#### Create

A brand new grantee list can be created using the following command:
```bash
swarm-cli grantee create grantees.json --stamp $stamp_id
```
where **grantees.json** shall contain the key **grantees** with the list of public keys:
```json
{
  "grantees": [
    "03ec55e9fb2aefb8600f69142abaad79311516c232b28919d66efb4d41bce15bfa",
    "03fdcab22b455ce08a481d929a4cb9f447752545818eded1ad1785c51581e822c6"
  ]
}
```
The response returns the newly created and encrypted grantee list and the history reference. Only the publisher can decrypt and therefore access the list.
If **act-history-address** is provided then the grantee list is uploaded as the newest version under that history.

#### Patch

```bash
swarm-cli grantee patch grantees-patch.json --reference $grantee_reference --history $grantee_history_reference --stamp $stamp_id
```
where **grantees.json** shall contain the keys **add** and **revoke** with the list of public keys for granting and revoking access, respectively:
```json
{
  "add": ["03fdcab22b455ce08a481d929a4cb9f447752545818eded1ad1785c51581e822c6"],
  "revoke": [
    "03ec55e9fb2aefb8600f69142abaad79311516c232b28919d66efb4d41bce15bfa"
  ]
}
```
The **reference** flag indicates the already existing encrypted grantee list reference that needs to be updated.
The **grantee_history_reference** indicates the reference of historical version of the list, where the encrypted list reference is added as a metadata to the history entry with the key **"encryptedglref"**

**Limitation**: If an update is called again within a second from the latest upload/update of a grantee list, then mantaray save fails with an invalid input error, because the key (timestamp) already exists, hence a new fork is not created.

#### Get

As stated above, only the publisher can decrypt and therefore access the list with the following command:
```bash
swarm-cli grantee get $grantee_reference
```
which simply returns the latest version of the list.

Non-authorized access causes the request to fail with a not found error.
For each of the above operations, if the provided **act-history-address** or **reference** is invalid then the request will fail with a not found error.

---

## Introduction(Contribute)

# Contribute to Bee Development

Welcome to the Dev area! We love PR's! üêù

We would would love you to get involved with our [Github repo](https://github.com/ethersphere/bee).

Connect with other Bee developers over at the official [Discord Server](https://discord.gg/wdghaQsGq5). Sign up and get involved with our buzzing hive of daily dev chat.

- If you would like to contribute, please read the [coding guidelines](https://github.com/ethersphere/bee/blob/master/CODING.md) before you get started.

- Installation from source is described in the [Installation](/docs/bee/installation/build-from-source).

- Contribute to Swarm‚Äôs evolution by proposing your own Swarm Improvement Proposal (SWIP) [here](https://github.com/ethersphere/SWIPs).

## Testing a connection with PingPong protocol

To check if two nodes are connected and to see the round trip time for message exchange between them, get the overlay address from one node, for example local node 2:

```bash
curl localhost:1833/addresses
```

Make sure addresses are configured as in examples above.

And use that address in the API call on another node, for example, local node 1:

```bash
curl -X POST localhost:1735/pingpong/d4440baf2d79e481c3c6fd93a2014d2e6fe0386418829439f26d13a8253d04f1
```

## Generating protobuf

To process protocol buffer files and generate the Go code from it two tools are needed:

- [protoc](https://github.com/protocolbuffers/protobuf/releases)
- [protoc-gen-gogofaster](https://github.com/gogo/protobuf)

Makefile rule `protobuf` can be used to automate `protoc-gen-gogofaster` installation and code generation:

```bash
make protobuf
```

---

## Protocols

## Protocols specifications

An attempt to describe the desired behaviour of the main DISC protocols, which corresponds to the rational choice a node should make in order to maximize its profitability.

A communications protocol is a set of formal rules describing how to transmit or exchange data, especially across a network.

We'll begin by describing the Hive, Retrieval, PushSync, PullSync communication protocols as well as the Kademlia topology component.

## Proposal

The purpose of this document is to specify these concepts:

- A pattern of exchange of messages which in semantic units corresponds to the high level function of what a node accomplishes in an exchange.
- Strategies of behaviour that a node should adopt in situations like network disconnects, timeouts, invalid chunks etc.
- An incentivisation strategy such that constructive behaviour should be rewarded and encouraged while deviating from the protocol rules should result in punishing measures.

## Hive

The Hive protocol defines how nodes exchange information about their peers in order to reach and maintain a saturated Kademlia connectivity.

The exchange of this information happens upon connection, however nodes can broadcast newly received peers to their peers during the lifetime of connection.

While the simplest approach is to share all known peers (during an exchange) it might be more optimal to narrow down to a useful subset of peers - for instance all the peers up to a certain depth or belonging to a certain bin.

The exchanged information includes both overlay and underlay addresses of the known remote peers.

The overlay address serves to select peers to achieve the connectivity pattern needed for the desired network topology, while the underlay address is needed to establish the peer connections by dialing selected peers.

Upon receiving a peers message, nodes should store the peer information in their address book, i.e., a data structure containing info about peers known to the node that is meant to be persisted across sessions.

### Appendix

The protobuf definitions

```protobuf
// Copyright 2020 The Swarm Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

syntax = "proto3";

package hive;

option go_package = "pb";

message Peers {
    repeated BzzAddress peers = 1;
}

message BzzAddress {
    bytes Underlay = 1;
    bytes Signature = 2;
    bytes Overlay = 3;
    bytes Nonce = 4;
}
```

## Kademlia

Kademlia topology is a specific connectivity pattern used by all DISC protocols and its purpose is to route messages between nodes in a network using overlay addressing.

The message routing happens in such a fashion that with every network hop we will get closer to the target node, specifically at half of the distance covered by the previous hop.

Swarm uses the recursive/forwarding style of Kademlia.

This approach implies that every forwarding node - once it received a request - will keep an in-memory record that captures the request related information (requester, time of the request etc.) until the request is satisfied, rejected or times out.

Because a forwarder can not reliably tell how much time the downstream peer will need to satisfy the request - the choice of a reasonable value for waiting period is a point of contention.

It is constrained by these factors:

- keeping the in-memory record for too long means that there's going to be a limit on how many concurrent requests a peer can keep "in-flight", because memory is limited.
- if the peer decides to time out prematurely (while downstream peers are still processing the request) then the effort of all the downstream peers will be wasted.
- we should distinguish between unsolicited chunks and chunks that we received from the downstream after we stopped waiting for the response (timed out). After a certain period of time all responses will be treated the same because of the need to free the allocated resources (the in-memory record).

Conversely the downstream should be informed when the upstream is no longer interested in the previously sent request, so it could free the used resources. This way the downstream won't return the chunk after the request has timed out, risking being punished.

## Push and pull: chunk retrieval and syncing

Swarm involves a direct storage scheme of fixed size where chunks are stored on nodes with address corresponding to the chunk address.

The syncing protocols act in such a way that they reach those neighborhoods whenever a request is initiated.

Such a route is sure to exist as a result of the Kademlia topology of keep-alive connections between peers.

The process of relaying the request from the initiator to the storer is called forwarding and also the process of passing the chunk data along the same path is called backwarding.

Conversely - Backwarding and Forwarding are both notions defined on a keep alive network of peers as strategies of reaching certain addresses.

If we zoom into a particular node in the forwarding (or backwarding) path we see the following strategy:

- Receive a request
- Decide who to forward the request to (decision strategy)
- Have a way to match the the response to the original request.

The crucial step is the second one - strategy of choosing the peer to forward the request to and how they react to failure like stream closure or nodes dropping offline or closing the protocol connection and whether we proactively initiate several requests to peers.

The last step does not apply for the storer nodes, since they do not forward the request but they satisfy it.

The key element of these notions is that the decision about the next action is being done on the node level, which will select the next peers(s) and delegate them with handling the request.

The simplest representation of this would be a recursive algorithm that with every iteration gets closer to the target address and stops when it runs out of peers or successfully reaches the target node.

### Requirements

- We need a way to determine the "best" candidate peer to forward the request to, and if this option fails, continue with the "next-best" candidate until we exhaust available peers. The decision of picking the "best" peer is delegate to an overlay driver that has the best knowledge of this peer's past history and performance and topology structure.
- We need a strategy of parallelisation of requests that we pass downstream, where appropriate. Parallel requests to different peers allow us increase the chances of successfully syncing the chunk but it comes with the cost of using our bandwidth allowance, so it's imperative to zoom in on an optimal balance between the two.
- We need a way to ensure that when we issue a syncing request we don‚Äôt end up in a situation when this request comes back around to us, wasting network resources.
- In the case when we are a "forwarder" node, we might consider a decision strategy on whether we want to cache the chunk in the event of a repeated request.
- To every forwarding/backwarding exchange we attach an incentivisation action that would take into account variables like the success of the action and the cost of performing the action.
- We need to design the optimal incentivisation scheme, to determine the optimal payment/settlement frequency and correctness of computation of the payment/charged amount. This also applies to both chunk storage scheme and the relayed request-response scheme.
- We need to have a sensible strategy when it comes to waiting for a peer to respond to our request; as a forwarder, we want to make our best effort to sync the chunk but without waiting for an excessive amount of time, which would lead to waste of resources.
- When receiving a response to an expired request - or we are unable to conclude if such a request has ever been issued - punishing measures should be imposed on the upstream peer.

### Incentivisation strategy

An incentivisation strategy should be put in place in such way that it encourages honest collaboration between nodes.

This implies that a given peer will make the best effort to satisfy any request while not allowing any abuse and waste of its resources.

Having an accounting component that would keep track of the exchange activity between peers ensures that we do not allow excessive freeloading from the misbehaving peers.

Having a granular punishment strategy ensures that the peers who misbehave (perhaps due to network latencies) will not be sanctioned to the same extent as peers who engage in grave protocol breaches, but are given a chance to "clean up their act".

## Retrieval

The retrieval of a chunk is a process which fetches a given chunk from the network by its address.

It follows the general semantics of the chunk syncing described above and follows the same network path as the push sync protocol, but in reverse.

### Protocol breach

- Receiving a repeated request for a non-existent chunk should lead to rate limiting in order to discourage resource wasteful actions.
- Receiving a response in the form of an invalid chunk constitutes a protocol breach and punishing measures are being taken against the peer at fault.

```markdown
step I
1) check if this exact request has been received within last N minutes and it is for a non-existent chunk
2) if such request is found - take punishing measures against the requester (blocklisting)
3) request a peer from Kademlia
4) request chunk from the peer
5) if the peer does not return a valid chunk - go back to step 3
6) if the chunk is found and valid, log the event details in the local state and return the chunk to the requester
7) consider caching the chunk in case there might be a repeated request for it

Error states
- if we exhaust the list of peers (candidates) for this action, return a 'failure to get chunk' response to the requester. We might consider increasing our peer connections pool to avoid such situation in the future
- if we are able to conclude that the chunk is non-existent (TBD) we return 'chunk not found' and consider rate limiting measures against the requester.
- if we ran out of allowed time while looking for the chunk we return a 'timeout' response to the requester
- if the chunk is retrieved successfully but does not pass validation, take punishing measures against the peer (blocklisting).
- if the attempt fails, log the relevant attempt details in the local state and repeat the attempt against a new peer
- if the peer times out responding to our request we log the attempt details and repeat step II against a new peer
```

### Request chunk - sequence diagram

```mermaid
sequenceDiagram
    Originator->>+Backwarder: Request for chunk
    Backwarder->>+Backwarder: Have I seen this request before
    Backwarder->>-Originator: Reject duplicate request
    Backwarder->>+Backwarder: Request next peer from the Kademlia iterator
    Backwarder->>+Storer: Request for chunk
    Storer->>-Backwarder: Success
    Backwarder->>-Originator: Return chunk
```

### Request chunk - flow diagram

```mermaid
flowchart TD
    A[Get next peer from Kademlia] --> H{Any time left?}
    H --> |No| I(Reject request) --> S[STOP]
    H --> |Yes| M{Any peers left}
    M --> |Yes| N(Request the chunk from peer) --> K{Check response}
    M --> |No peers left to try| I
    K --> |Timeout| L(Update peer stats) --> A
    K --> |Invalid chunk| N1(Punish peer) --> A
    K --> |Success| R(Return the chunk to the upstream peer) --> S
```

### Appendix

The protobuf definitions

```protobuf
// Copyright 2020 The Swarm Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

syntax = "proto3";

package retrieval;

option go_package = "pb";

message Request {
  bytes Addr = 1;
}

message Delivery {
  bytes Data = 1;
  bytes Stamp = 2;
}
```

## Pushsync

Pushsync protocol is responsible for ensuring delivery of the chunk to its prescribed storer after it has been uploaded to any arbitrary node.

It works in a similar way to the Retrieval protocol in the sense that the chunk is being passed to a peer whose address is closest to the chunk address and a custody receipt is received in response.

Then the same process is repeated until the chunk eventually reaches the storer node located in a certain "neighborhood".

Since the Pushsync protocol is a "mirror" version of the Retrieval protocol - it ensures that a successfully uploaded chunk is retrievable from the same "neighborhood" by the virtue of the fact that nodes in a neighborhood are connected to each other.

### Multiplexing

Multiplexing is a recommended node strategy for the push sync protocol that involves early replication and opportunistic receipting. Its intention is to reduce the dependence on single closest nodes and to improve on network performance, i.e., push sync success rate, bandwidth overhead and latency.

#### Context

The current implementation of the push sync protocol aims to push a chunk to the closest node in the neighborhood which is then supposed to give out a receipt.

This is motivated by the  retrieval protocol that aims to find the chunk at this closest node.

When the closest node hands out a receipt, this node also replicates the chunk to 3 peers in the neighborhood which are further away from the chunk than him.

This replication takes place to ensure that the chunk is not lost when the closest node shuts down before the chunk is not pull-sync'ed and to speed up the spreading of the chunk in the neighborhood, in advance of pull sync.

#### Problem

Treating the closest node as a single target of push sync is fragile. If this peer has a badly-performing blockchain backend, slow or incomplete connectivity or is malicious, it may not spread the chunk and/or does not respond with a receipt.

In this case, currently, the originator must retry the entire push-sync operation many times before the other peers within neighborhood recognise the improper behaviour.

In-neighborhood retries are ideally avoided because such retries might cause the downstream timeouts to expire.

In case of incomplete connectivity, the push sync protocol can end at a different branch of the neighborhood than the retrieval protocol--causing the chunk not to be retrievable.

It should be noted that the pull sync protocol (may?) remedies this problem with a small time-delay.

#### Multiplexing: early replication within neighborhood

The first node in the push sync forward chain that falls within the neighborhood acts as *multiplexer*, i.e., it forwards the request to a number of closest nodes and responds with a self-signed receipt.

Thus in achieving retrievability and security via early replication, we do not critically rely on the closest node to be available any more.

#### Push sync flow

We define the different roles peers have as part of the push sync forwarding chain:

- originator -- creator of the request
- forwarder -- closer to the chunk than the originator, further away away than the 1-before node.
- multiplexer -- first node in the forward chain who is in the neighborhood
- closest nodes -- according to the downstream node (usually the multiplexer), within the `n` closest nodes to the chunks (not including self)

we describe the envisioned flow of push sync by describing the intended behaviour strategy of the various roles.

1. originator sends chunk to a peer closer to the chunk.
2. forwarder(s) forwards chunk that ends up with a node already within the neighborhood that acts as multiplexer
3. The multiplexer concurrently sends the chunk to 3 closest nodes attaching a multiplexing-list as part of the protocol message. At the same time they respond to their upstream peer with a self-signed receipt (unless the multiplexer is itself the originator).
4. Non-multiplexing closest nodes, i.e., nodes in the neighborhood that receive the pushsync message from a not-closest neighbour with a multiplexing list included, validate whether, based on their view, the multiplexing list covers all 3 closest nodes (potentially including the peer and/or the upstream peer themselves). If not, the node forwards the chunk to the peers left out. These peers are also added to the multiplexing list received from upstream and the extended list is attached with the chunk pushed.

If the multiplexer node does not know a closest peer *p* but several of its chosen closest nodes do, then that node *p* will receive the same pushsynced chunk multiple times

### Appendix

The protobuf definitions

```protobuf
// Copyright 2020 The Swarm Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

syntax = "proto3";

package pushsync;

option go_package = "pb";

message Delivery {
  bytes Address = 1;
  bytes Data = 2;
  bytes Stamp = 3;
}

message Receipt {
  bytes Address = 1;
  bytes Signature = 2;
  bytes Nonce = 3;
}
```

## Pullsync

While the other described protocols are request scoped, Pullsync is a subscription type protocol.

It's worth mentioning that the chunks that are being synchronized between nodes always travel alongside their corresponding postage stamps.

Pullsync's role is to help synchronization of the chunks between neighborhood nodes. It bootstraps new nodes by filling up their storage with the chunks in range of their storage radius and also ensures eventual consistency - by making sure that the chunks will gradually migrate to their storer nodes.

There are two kinds of syncing:

- historical syncing: catching up with content that arrived to relevant neighborhood before this session started (after an outage or for completely new nodes).
- live syncing: fetching the chunks that are received after the session has started.

The chunks are served in batches (ordered by timestamp) and they cover contiguous ranges.

The downstream peers coordinate their syncing by requesting ranges from the upstream with the help of the "interval store" - to keep track of which ranges are left to be synchronized.

Because live syncing happens in sessions - it is inevitable that after a session is completed - the downstream peer disconnects and will be missing chunks that arrive later.

For this purpose the downstream peer will make a note about the timestamp of the last synced chunk on disconnect.

The point of the interval based approach is to cover those gaps that inevitably arise in between syncing sessions.

To save bandwidth, before the contents of the chunk is being sent over the wire, the upstream will sent a range of chunk addresses for approval. If the downstream decides that some (or all) addresses are desired - a confirmation message is sent to the upstream, to which it responds with the chunks mentioned in the request.

```mermaid
sequenceDiagram
    Downstream->>+Upstream: Get
    Upstream-->>-Downstream: Offer address range
    Downstream->>+Upstream: Want address range
    Upstream-->>-Downstream: Delivery
```

### Appendix

The protobuf definitions

```protobuf
// Copyright 2020 The Swarm Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

syntax = "proto3";

package pullsync;

option go_package = "pb";

message Syn {}

message Ack {
  repeated uint64 Cursors = 1;
}

message Get {
  int32 Bin = 1;
  uint64 Start = 2;
}

message Chunk {
  bytes Address = 1;
  bytes BatchID = 2;
}

message Offer {
  uint64 Topmost = 1;
  repeated Chunk Chunks = 2;
}

message Want {
  bytes BitVector = 1;
}

message Delivery {
  bytes Address = 1;
  bytes Data = 2;
  bytes Stamp = 3;
}
```

### Peer rating

When choosing a peer in relation to a given address - in addition to the distance between them - the Kademlia component will take into account two other factors:

- the historical performance of the given peer, both in terms of latencies and past occurrences of protocol misalignments.
- the accounting aspect, peers with whom we have higher credit will be preferred.
- we should also prioritise those downstream peers that managed to produce responses in a previously computed amount of time (that would take into consideration the average time needed for a hop multiplied by the expected number of hops needed to reach a target neighborhood).

Kademila should be indexing peers by their proximity order and peers rating in order to prioritize peers based on their expected performance.

### Decision strategy

An optimal decision strategy will take into account both proximity order and peer rating to select (out of all connected peers) the best one to pass down the request.

At the implementation level the Kademlia component will offer (in exchange for a given address) a stateful iterator that the client (protocol) will use to get the "next-best" peer.

### Transport

A reliable network transport is required for the proper functionality of DISC protocols.

It can be a distinct component whose responsibilities would be ensuring delivery, re-tries on network issues and timeouts and optimal use of network related resources.

One example of usage for such a component could be embedding into the Kademlia driver so that the topology component is only concerned with overlay related operations, abstracting away any low level transport concerns.

---

## Dynamic Content

:::warning
Under construction, set for major revisions.
:::

## Feeds for Dynamic Content

:::info
Although all data on Swarm is immutable, feeds provide an updatable reference that lets you simulate dynamic content. A feed is an append‚Äëonly sequence of updates that always resolves to its latest entry through a stable feed manifest.
:::

:::tip
If you are not familiar with feeds, read:
- Bee docs: /docs/develop/tools-and-features/feeds/
- Bee-JS docs: https://bee-js.ethswarm.org/docs/soc-and-feeds/
:::

Single‚Äëpage applications (SPAs) deployed on Swarm work best when their static assets can be updated independently. Instead of reuploading the entire site when one file changes, you can create a separate feed manifest for each asset. Each asset feed provides a stable URL that always resolves to the latest version of that file.

## Why Use Per‚ÄëAsset Feeds

- Each React/Vite build artifact (HTML, JS, CSS, images) becomes individually updatable.
- Every asset has a dedicated feed manifest with its own stable Swarm URL.
- Updating a single file only updates its feed; the rest of the site stays untouched.
- This keeps deployments small, fast, and cost‚Äëefficient.

## Architecture Overview

| Asset                | Feed Topic      | Purpose                     |
|----------------------|-----------------|-----------------------------|
| Main site bundle     | `website`       | HTML/JS/CSS entry point     |
| CSS theme            | `main-css`      | Global styling              |
| JS bundle            | `main-js`       | Application logic           |
| Images               | `img-*`         | Media resources             |

Each asset has:
- a private key  
- a feed topic  
- a feed manifest  
- a stable Swarm URL (`bzz://<MANIFEST_HASH>/`)

## Generate a Publisher Key

```js
import crypto from "crypto";
import { PrivateKey } from "@ethersphere/bee-js";

const hex = "0x" + crypto.randomBytes(32).toString("hex");
const pk = new PrivateKey(hex);

console.log("Private key:", pk.toHex());
console.log("Address:", pk.publicKey().address().toHex());
```

## Create a Feed for an Asset

```js
import { Bee, PrivateKey, Topic } from "@ethersphere/bee-js";

const bee = new Bee("http://localhost:1633");
const batchId = "<BATCH_ID>";

const pk = new PrivateKey("<ASSET_PRIVATE_KEY>");
const topic = Topic.fromString("main-js");
const owner = pk.publicKey().address();

const writer = bee.makeFeedWriter(topic, pk);
```

## Upload an Asset and Publish a Feed Update

```js
const upload = await bee.uploadFile(batchId, "./dist/assets/index-398a.js");
await writer.upload(batchId, upload.reference);

const manifest = await bee.createFeedManifest(batchId, topic, owner);
console.log("JS Manifest:", manifest.toHex());
```

Stable URL:

```
bzz://<JS_MANIFEST_HASH>/
```

This URL never changes, even when you replace the underlying file.

## Updating an Existing Asset

```js
const nextUpload = await bee.uploadFile(batchId, "./dist/assets/index-new.js");
await writer.upload(batchId, nextUpload.reference);
```

No new manifest is created. The old URL now resolves to the updated file.

## Referencing Asset Feeds in Your SPA

Rather than referencing a static build hash, point your SPA to feed manifests.

Example `index.html`:

```html
<script type="module" src="bzz://<JS_MANIFEST_HASH>/"></script>
<link rel="stylesheet" href="bzz://<CSS_MANIFEST_HASH>/" />
```

Example React image reference:

```jsx

```

This makes your deployment resilient to Vite‚Äôs changing file names, because Swarm fetches the latest version through the feed instead of the literal file path.

## Example Deployment Workflow

1. Build Vite:
   ```
   npm run build
   ```

2. For each file in `dist/`:
   - assign (or reuse) a feed topic
   - upload the file
   - update its feed
   - store the feed manifest hash in a hard‚Äëcoded list inside your SPA

3. Rebuild your SPA to reference:
   - `bzz://<JS_FEED_MANIFEST>/`
   - `bzz://<CSS_FEED_MANIFEST>/`
   - `bzz://<IMAGE_FEED_MANIFEST>/`

4. Upload only the *main* SPA entrypoint (often a small static HTML + JS shell) using `swarm-cli feed upload`.

This gives you a fully working dynamic SPA with lightweight incremental updates.

## Summary

- Each build artifact gets its own updatable feed.
- Your SPA uses stable feed manifest URLs instead of build‚Äëhashed filenames.
- Only changed files need to be uploaded.
- This keeps deployments fast while ensuring long‚Äëlived URLs remain valid.

The next section (not included here) expands this into a registry‚Äëbased system for large dynamic sites.

---

## Host a Webpage

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

There are two primary methods most developers should use for uploading a website to Swarm - `swarm-cli` and `bee-js`. Depending on the specific use case, it may make more sense to pick one or the other.

For a simple website such as a personal blog or company page, using `swarm-cli` is simplest and fastest way to get your site uploaded and accessible on Swarm.

However for developers who need finer grained control over the process or who wish to build a more complex application which require programmatically spinning up new pages, `bee-js` is required.

:::tip
The guides below assume you already have a registered ENS domain name. By using an ENS domain name, you can make your Swarm hosted website accessible through an easy to remember human-readable name rather than a Swarm hash. If you don't have an ENS domain name registered, you can get one using the official ENS application at [app.ens.domains](https://app.ens.domains/). Refer to their support section for a step-by-step guide to [register an ENS domain](https://support.ens.domains/en/articles/7882582-how-to-register-a-eth-name).
:::

:::tip FIX FOR ENS NOT WORKING ON LOCALHOST
If the site doesn‚Äôt load from localhost, it‚Äôs probably an with the resolver RPC (the RPC endpoint for the Ethereum node used to resolve your ENS domain name). 

Some endpoints, such as:

```
https://cloudflare-eth.com
```

may not resolve properly on localhost.

As of the writing of this guide, both of these free and public endpoints work reliably for localhost resolution:

```
https://mainnet.infura.io/v3/<infura-api-key>
https://eth-mainnet.public.blastapi.io
```

Alternatively, you can run your own Ethereum node and use that as the RPC.
:::

## Host a Site With **swarm-cli** 

This guide shows you how to get your website hosted on Swarm with just a few simple commands by using `swarm-cli` from your terminal. 

### Prerequisites

* A running Bee node (either a [standard installation](/docs/bee/installation/quick-start) or [Swarm Desktop](/docs/desktop/install))
* A valid postage batch
* [`swarm-cli` installed](https://docs.ethswarm.org/docs/bee/working-with-bee/swarm-cli)
* A valid postage stamp batch
* Your static website files (you can also use the example website files provided below)  
* (Optional for part one - "Upload & Access by Hash") An ENS domain which you [previously registered](https://support.ens.domains/en/articles/7882582-how-to-register-a-eth-name)

### Upload & Access by Hash

You can download the example website files from the [ethersphere/examples](https://github.com/ethersphere/examples/tree/main/basic-static-website) repository.

#### Uploading the Website

1. Go to the folder containing your website files.

The example website files look like this:

```
my-website/
‚îú‚îÄ‚îÄ index.html       # main landing page
‚îú‚îÄ‚îÄ 404.html         # custom error page
‚îú‚îÄ‚îÄ styles.css       # basic styling
‚îú‚îÄ‚îÄ script.js        # optional script
‚îú‚îÄ‚îÄ favicon.svg      # site icon
‚îî‚îÄ‚îÄ robots.txt       # default robots config
```

* `index.html` will be served by default when users visit the root URL.
* `404.html` will be served for non-existent paths.
* The other files are optional and can be customized.

2. Run:

<Tabs
defaultValue="powershell"
values={[
{label: 'PowerShell', value: 'powershell'},
{label: 'Linux / macOS', value: 'bash'},
]}>

<TabItem value="powershell">

```powershell
swarm-cli upload . `
  --stamp <BATCH_ID> `
  --index-document index.html `
  --error-document 404.html
````

</TabItem>

<TabItem value="bash">

```bash
swarm-cli upload . \
  --stamp <BATCH_ID> \
  --index-document index.html \
  --error-document 404.html
```

</TabItem>
</Tabs>

* Replace `<BATCH_ID>` with your postage batch ID.
* `--index-document` tells Bee which file to serve at the root.
* `--error-document` defines the fallback file for missing paths.

3. The upload will return a Swarm reference hash, for example:

```
cf50756e6115445fd283691673fa4ad2204849558a6f3b3f4e632440f1c3ab7c
```

Copy this and save it. You‚Äôll need it for both direct access and [ENS integration](#connect-site-to-ens-domain).

#### Accessing the Website

Anyone with a Bee node can now access the site using the Swarm hash you just saved:

```
http://localhost:1633/bzz/<SWARM_HASH>/
```

### (Recommended) Use Feeds for Seamless Updates - swarm-cli

*If you have not already connected your site to your ENS domain, [do that now](#connect-site-to-ens-domain) before returning here.*

If you have an ENS domain and Swarm hosted website, you can make the site available through the domain by registering website's Swarm hash as a content hash through the [ENS domain management app](https://app.ens.domains/). However, if you ever edit and reupload your site to Swarm, you will need to re-register your new website hash to make it available at your ENS domain.  

Therefore, instead of directly using your website hash as the content hash for your ENS domain, upload your site as a feed update and use the feed manifest hash as the content hash. Then every time you update your site as a new feed update, the ENS domain will always resolve to the newest version of your site without the need to register a new hash each time.

:::tip
The examples below refer to core feed concepts such as "publisher identity", and "topic". To learn more about these concepts refer to the [bee-js documentation](https://bee-js.ethswarm.org/docs/soc-and-feeds/#feeds).
:::

In this section, you will:

1. Create a publisher identity  
2. Upload your site to a feed (this automatically creates the feed manifest)  
3. Copy the feed manifest reference  
4. Use that manifest reference as your ENS contenthash  

#### Step 1: Create a dedicated publisher identity

This key will sign feed updates.  

```bash
swarm-cli identity create website-publisher
```

Terminal output:

```bash
Name: website-publisher
Type: V3 Wallet
Private key: 0x22e918ef68c9bc975112ceaaee0ee0f147baa79da257873659bddbfd84a646fe
Public key: 0x218c79f8dfb26d077b6379eb56aa9c6e71edf74dde8ecd27dac5016528aea80ee121b9e5050adf3948c8b0d8cffda763d7fb1f5608250b5009c5d50e158ab4a5
Address: 0x2fb11d37a9913bd3258b9918c399f35fd842a232
```

Record the output in a secure location as a backup ‚Äî you will need this identity for future updates.

If you need to view/export it later:

```bash
swarm-cli identity export website-publisher
```

#### Step 2: Upload your website to a feed (creates the manifest automatically)

<Tabs>
<TabItem value="linux" label="Linux / macOS">

```bash
swarm-cli feed upload ./website \
  --identity website-publisher \
  --topic-string website \
  --stamp <BATCH_ID> \
  --index-document index.html \
  --error-document 404.html
```

</TabItem>
<TabItem value="powershell" label="Windows PowerShell">

```powershell
swarm-cli feed upload .\website `
  --identity website-publisher `
  --topic-string website `
  --stamp <BATCH_ID> `
  --index-document index.html `
  --error-document 404.html
```
</TabItem>
</Tabs>

You will see output that includes your **feed manifest reference**, for example:

```bash
Swarm hash: 387dc3cf98419dcb20c68b284373bf7d9e8dcb27daadb67e1e6b6e0f17017f1f
URL: http://localhost:1633/bzz/387dc3cf98419dcb20c68b284373bf7d9e8dcb27daadb67e1e6b6e0f17017f1f/
Feed Manifest URL: http://localhost:1633/bzz/6c30ef2254ac15658959cb18dd123bcce7c16d06fa7d0d4550a1ee87b0a846a2/
Stamp ID: 3d98a22f
Usage: 50%
Capacity (mutable): 20.445 KB remaining out of 40.890 KB
```

You can find the manifest hash at `Feed Manifest URL` in the URL right after `/bzz/`: `6c30ef2254ac15658959cb18dd123bcce7c16d06fa7d0d4550a1ee87b0a846a2`

Save this hash, you will use it for the next step.

This is your **permanent website reference**. It is a reference to a feed manifest which points to the latest feed entry so that you can use it as a static, unchanging reference for your website even as you make multiple updates to the site. Every time you update the website through the feed, this manifest will point to the hash for the newest version of the website.

#### Step 3: Use the feed reference as the ENS contenthash

Follow the [official ENS guide](https://support.ens.domains/en/articles/12275979-how-to-add-a-decentralized-website-to-an-ens-name) for registering a content hash adding your content hash in the ENS UI (see [guide](#connect-site-to-ens-domain)). However, rather than registering your website's hash directly, register the feed manifest hash we saved from the previous step from our example above.

Example:

```
bzz://6c30ef2254ac15658959cb18dd123bcce7c16d06fa7d0d4550a1ee87b0a846a2
```

Now your ENS name will always point to a static reference which will always resolve to the latest version of your website.

#### Updating your site in the future

When you have a new version of your site, just run `feed upload` again using the same topic and identity:

<Tabs>
<TabItem value="linux" label="Linux / macOS">

```bash
swarm-cli feed upload ./website \
  --identity website-publisher \
  --topic-string website \
  --stamp <BATCH_ID> \
  --index-document index.html \
  --error-document 404.html
```

</TabItem>
<TabItem value="powershell" label="Windows PowerShell">

```powershell
swarm-cli feed upload .\website `
  --identity website-publisher `
  --topic-string website `
  --stamp <BATCH_ID> `
  --index-document index.html `
  --error-document 404.html
```

</TabItem>
</Tabs>

* The **feed manifest reference stays the same**.
* The feed now points to the newly uploaded site version.
* No ENS changes needed.

## Host a Website with **bee-js**

This guide explains how to host a website on Swarm using the `bee-js` JavaScript SDK instead of the CLI.

For developers building apps, tools, or automated deployments, `bee-js` offers programmatic control over uploading and updating content on Swarm.  

### Prerequisites

* A running Bee node (either a [standard installation](/docs/bee/installation/quick-start) or [Swarm Desktop](/docs/desktop/install))
* A valid postage stamp batch
* Node.js (18+) and `@ethersphere/bee-js` installed in your project
* Static website files (HTML, CSS, etc.) - feel free to use the [provided example site](https://github.com/ethersphere/examples/tree/main/basic-static-website)
* (Optional for part one - "Upload & Access by Hash") An ENS domain which you [previously registered](https://support.ens.domains/en/articles/7882582-how-to-register-a-eth-name)

### Upload and Access by Hash

Install bee-js:

```bash
npm install @ethersphere/bee-js
```

Website upload script:

```js
import { Bee } from "@ethersphere/bee-js";

const bee = new Bee("http://localhost:1633");

const batchId = "<BATCH_ID>"; // Replace with your actual postage batch ID

const result = await bee.uploadFilesFromDirectory(batchId, "./website", {
  indexDocument: "index.html",
  errorDocument: "404.html"
});

console.log("Swarm hash:", result.reference.toHex());
```

```bash
Swarm hash: 6c45eae389b3bffce21443316d0bd47c4101545092b7c72c313a33ee7d003475
```

After running the script, copy the Swarm hash output to the console and then use it to open your Swarm hosted website in the browser:

```bash
http://localhost:1633/bzz/<SWARM_HASH>/
```

### (Recommended) Use Feeds for Seamless Updates - bee-js

*If you have not already connected your site to your ENS domain, [do that now](#connect-site-to-ens-domain) before returning here.*

If you have an ENS domain and Swarm hosted website, you can make the site available through the domain by registering website's Swarm hash as a content hash through the [ENS domain management app](https://app.ens.domains/). However, if you ever edit and reupload your site to Swarm, you will need to re-register your new website hash to make it available at your ENS domain.  

Therefore, instead of directly using your website hash as the content hash for your ENS domain, upload your site as a feed update and use the feed manifest hash as the content hash. Then every time you update your site as a new feed update, the ENS domain will always resolve to the newest version of your site without the need to register a new hash each time.

:::tip
You will need a publisher key to use for setting up your website feed.

You can use the `PrivateKey` class to generate a dedicated publisher key:

```js
const crypto = require('crypto');
const { PrivateKey } = require('@ethersphere/bee-js');

// Generate 32 random bytes and construct a private key
const hexKey = '0x' + crypto.randomBytes(32).toString('hex');
const privateKey = new PrivateKey(hexKey);

console.log('Private key:', privateKey.toHex());
console.log('Public address:', privateKey.publicKey().address().toHex());
````

Example output:

```bash
Private key: 634fb5a872396d9693e5c9f9d7233cfa93f395c093371017ff44aa9ae6564cdd
Public address: 8d3766440f0d7b949a5e32995d09619a7f86e632
```

Store this key securely.

Anyone with access to it can publish to your feed.

*It is recommended to use a separate publishing key for each feed.*
:::

#### Example Script

:::tip
The script below refers to some core feed concepts such as the feed "topic" and "writer". To learn more about these concepts and feeds in general, refer to the [bee-js documentation](https://bee-js.ethswarm.org/docs/soc-and-feeds/#feeds).
:::

The script performs these steps:

1. **Connects to your Bee node** and loads your postage batch + publisher private key.
2. **Creates a feed topic and writer** for publishing website updates.
3. **Uploads the `./website` directory** to Swarm and logs the resulting content hash.
4. **Publishes that hash to the feed** so it becomes the latest feed entry.
5. **Creates a feed manifest** and logs its reference ‚Äî this is the permanent hash you use for ENS or stable URLs.

```js
import { Bee, Topic, PrivateKey } from "@ethersphere/bee-js";
const bee = new Bee("http://localhost:1633");
const batchId =  "<BATCH_ID>" // Replace with your batch id
const privateKey = new PrivateKey("<PUBLISHER_KEY>"); // Replace with your publisher private key
const owner = privateKey.publicKey().address();

// Upload and Create Feed Manifest

const topic = Topic.fromString("website");
const writer = bee.makeFeedWriter(topic, privateKey);

const upload = await bee.uploadFilesFromDirectory(batchId, "./website", {
  indexDocument: "index.html",
  errorDocument: "404.html"
});

console.log("Website Swarm Hash:", upload.reference.toHex())

await writer.uploadReference(batchId, upload.reference);

const manifestRef = await bee.createFeedManifest(batchId, topic, owner);
console.log("Feed Manifest:", manifestRef.toHex());
```

Upon the successful execution of the script, the hash of the uploaded website will be logged along feed manifest hash. Copy the "Feed Manifest" hash to be used in the next step: 

```bash
Website Swarm Hash: 6c45eae389b3bffce21443316d0bd47c4101545092b7c72c313a33ee7d003475
Feed Manifest: caa414d70028d14b0bdd9cbab18d1c1a0a3bab1b20a56cf06937a6b20c7e7377
```

Follow the [official ENS guide](https://support.ens.domains/en/articles/12275979-how-to-add-a-decentralized-website-to-an-ens-name) for registering a content hash adding your content hash in the ENS UI (see [guide](#2-connecting-your-website-to-ens)). However, rather than registering your website's hash directly, register the feed manifest hash we saved from the previous step from our example above.

```
bzz://<manifestRef>
```

Future updates just re-run:

```js
await writer.upload(batchId, newUpload.reference);
```

Your ENS domain will always point to the latest upload via the feed manifest.

You‚Äôve now got a programmatic way to deploy and update your Swarm-hosted site with ENS support using `bee-js`!

## Connect Site to ENS Domain 

Once your site is uploaded to Swarm, you can make it accessible via an easy to remember ENS domain name rather than its Swarm hash:

```
https://yourname.eth.limo/
https://yourname.bzz.link/
```

or through your own node:

```
http://localhost:1633/bzz/yourname.eth/
```

#### Using the Official ENS Guide

ENS provides a clear walkthrough with screenshots showing how to add a content hash to your domain with their [easy to use app](https://app.ens.domains/):

[How to add a Decentralized website to an ENS name](https://support.ens.domains/en/articles/12275979-how-to-add-a-decentralized-website-to-an-ens-name)

The guide covers:

* Opening your ENS domain in the ENS Manager
* Navigating to the Records tab
* Adding a Content Hash
* Confirming the transaction

#### Swarm-Specific Step

When you reach Step 2 in the ENS guide (‚ÄúAdd content hash record‚Äù), enter your Swarm reference in the following format:

:::tip
For the content hash, you can use a Swarm hosted website's hash directly, or as is recommended in the [`swarm-cli`](#recommended-use-feeds-for-seamless-updates---swarm-cli) and [`bee-js`](#recommended-use-feeds-for-seamless-updates---bee-js) guides above, publish your site to a feed and use the feed manifest hash instead. By using a feed manifest as the content hash, you can avoid repeated ENS registry updates. 
:::

```
bzz://<SWARM_HASH>
```

Example:

```
bzz://cf50756e6115445fd283691673fa4ad2204849558a6f3b3f4e632440f1c3ab7c
```

This works across:

* eth.limo and bzz.link
* localhost (with a compatible RPC)
* any ENS-compatible Swarm resolver

You do not need to encode the hash or use any additional tools. `bzz://<hash>` is sufficient.

---

## Getting Started(Develop)

This is the go-to starting point for web3 developers who want to build with Swarm. The guides on this page will help you get started with setting up a Bee node, using that node to integrate your dApp with Swarm, and to begin exploring some example applications to better understand the possibilities of building on Swarm.

## Setup

 
    
      
        
          Install Bee
          
            Install and run a Bee node ‚Äî your entry point for development on the Swarm network.
          
          Get started
        
      
      
        
          Connect Your App
          
            Use the official <code>bee-js</code> SDK to connect your app to your Bee node and integrate Swarm based storage, feeds, and more.
          
          Connect dApp
        
      
    
  

## Building on Swarm

  
    
      
        
          Upload and Download
          
            Learn how to upload and download a variety of data types from Swarm (with support for both browser and Node.js environments).
          
          Open guide
        
      
      
        
          Host a Webpage
          
             Host a website on Swarm and link it to your ENS domain for easy access at through gateways and Bee nodes. 
          
          Open guide
        
      
      
        
          Manifests
          
            Learn about how manifests enable a virtual "file system" on Swarm, and how to manipulate the manifest to re-write virtual paths to add, remove, or move content. 
          
          Open Guide
        
      
      
        
          Routing
          
            Learn about routing on Swarm and the various options at your disposal for approaching website routing.
          
          Open Guide
        
      
      <!--
      
        
          Dynamic Content
          
            Build dynamic sites on Swarm by updating individual assets through feeds ‚Äî
            enabling granular updates, decentralized CDN patterns, and zero-downtime content changes.
          
          Open guide
        
      
      
        
          Add Access Control
          
            Control who is able to view your uploaded content, such as a list of paid newsletter or blog subscribers.
          
          Open guide
        
      
      
        
          Messaging on Swarm
          
            Learn how to integrate Swarm-based messaging into your app ‚Äî with support for both real-time messaging use cases and private encrypted messaging.
          
          Open guide
        
      
      -->

---

## Manifests ("Virtual Filesystem")

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::warning
Under construction, set for major revisions.
:::

Bee nodes ‚Äî and tools like `bee-js` and `swarm-cli` ‚Äî let you upload entire folders of files to Swarm.

Swarm doesn‚Äôt have a traditional file system like your computer does. Instead, when uploading a collection of files, it uses something called a **manifest**, which acts like a map between relative file paths (like `/images/cat.jpg`) and the actual content stored on Swarm.

A manifest is stored in a compact binary [encoded **prefix trie**](https://en.wikipedia.org/wiki/Trie). 

A prefix trie is like a tree that stores file paths by breaking them into shared chunks. For example, `images/cat.jpg` and `images/dog.jpg` both start with `images/`, so they share a common branch.

This *saves space* and *makes lookups fast*.

Each entry in the manifest includes:

* a part of the file path (like `images/`)
* a reference to the file's data (its Swarm hash)
* optional metadata (such as file name or content type)

With manifests, Swarm can serve your content at readable URLs while still storing it securely and immutably.

Manifests give Swarm two powerful features:

- A **filesystem-like structure** that preserves the directory layout of uploaded folders.
- **Clean, customizable website routing**, such as mapping `/about`, `/about/`, and `/about.html` to the same file or redirecting old paths to new ones.

:::info
Manifests are stored on Swarm as raw binary data.  
To work with them, these bytes must be [**unmarshalled** (decoded)](https://en.wikipedia.org/wiki/Marshalling_(computer_science)) into a structured form.

`bee-js` provides this functionality through the `MantarayNode.umarshal` method.

After unmarshalling, the data is still quite low-level (for example, many fields are `Uint8Array` values) and usually needs additional processing to make it human-readable. You can find a [script for this in the `ethersphere/examples` repo](https://github.com/ethersphere/examples/blob/main/manifests/manifestToJson.js). 
:::

## Introduction to Manifests

Whenever you upload a folder using Bee‚Äôs [`/bzz` endpoint](https://docs.ethswarm.org/api/#tag/BZZ) (and tools built on top of it such as `bee-js` and `swarm-cli`), Bee automatically creates a manifest that records:

- every file inside the folder
- the file‚Äôs relative path
- metadata (content type, filename, etc.)
- optional website metadata ([index document, error document](https://docs.ethswarm.org/api/#tag/BZZ/paths/~1bzz/post))

Uploads made through the Bee API using `/bytes` or `/chunks` **do not** create manifests.  

However, these endpoints are typically used only for custom use cases requiring lower level control, and are not required for standard use cases such as storing and retrieving files and hosting websites or dapps. 

Because `bee-js` and `swarm-cli` call `/bzz` when appropriate, **you get a manifest automatically** whenever you upload a directory.

:::info
Although working with a manifest may _feel_ like moving or deleting files in a regular filesystem, **no data on Swarm is ever changed**, because all content is immutable.

When you "modify" a manifest, you‚Äôre actually creating a _new_ manifest based on the previous one.  

Removing an entry only removes it from the manifest ‚Äî the underlying file remains available as long as its postage batch is valid.
:::

:::tip
You can find complete examples of all manifest scripts in the ethersphere/examples repo under  
[`/manifests`](https://github.com/ethersphere/examples/tree/main/manifests).

The `bee-js` [`cheatsheet.ts`](https://github.com/ethersphere/bee-js/blob/master/cheatsheet.ts) and  
[manifest source code](https://github.com/ethersphere/bee-js/blob/master/src/manifest/manifest.ts)  
are also excellent references.
:::

## Manifest Structure Explained

The printed output below shows a **decoded Mantaray manifest** (printed using the `printManifestJson` method from the [`manifestToJson.js` script in the examples repo](https://github.com/ethersphere/examples/blob/main/manifests/manifestToJson.js)), represented as a tree of nodes and forks. Each part plays a specific role in describing how file paths map to Swarm content: 

```json
{
    "path": "/",
    "target": "0x0000000000000000000000000000000000000000000000000000000000000000",
    "metadata": null,
    "forks": {
        "folder/": {
            "path": "folder/",
            "target": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "metadata": null,
            "forks": {
                "nested.txt": {
                    "path": "nested.txt",
                    "target": "0x9442e445c0d58adea58e0a8afcdcc28ed7642d7a4ff9a253e8f1595faafbb808",
                    "metadata": {
                        "Content-Type": "text/plain; charset=utf-8",
                        "Filename": "nested.txt"
                    },
                    "forks": {}
                },
                "subfolder/deep.txt": {
                    "path": "subfolder/deep.txt",
                    "target": "0x6aa935879ad2a547e57ea6350338bd04ad758977b542e86b31c159f31834b8fc",
                    "metadata": {
                        "Content-Type": "text/plain; charset=utf-8",
                        "Filename": "deep.txt"
                    },
                    "forks": {}
                }
            }
        },
        "root.txt": {
            "path": "root.txt",
            "target": "0x98e63f7e826a01634881874246fc873cdf06bb5409ff5f9ec61d1e2de1dd3bf6",
            "metadata": {
                "Content-Type": "text/plain; charset=utf-8",
                "Filename": "root.txt"
            },
            "forks": {}
        }
    }
}

```

Here‚Äôs what each piece means:

#### **Node:**

A **Node** represents a position within the manifest trie.
Each node corresponds to a **path prefix**‚Äîa segment of the full file path.

For example:

- A node with `path: folder/` represents the prefix `"folder/"`.
- A node with `path: nested.txt` represents a leaf for the file `"nested.txt"`.

Every node may contain:

- a path segment (`path`)
- zero or more child connections (`forks`)
- optional metadata (e.g., content-type, filename)
- a `target` (if the node corresponds to an actual file)

#### **Forks:**

A **fork** is an [edge](https://en.wikipedia.org/wiki/Glossary_of_graph_theory#edge) from one node to another.
It is how the trie branches when file paths diverge.

For example, under `folder/`, you see:

```bash
forks:
  nested.txt/
  subfolder/deep.txt/
```

This means:

- the `folder/` node has two children
- those children represent different path continuations (i.e., different files)

Forks are how shared prefixes are stored only once. Everything that starts with `folder/` branches from the same node.

#### **Path:**

`path` is the **path segment stored at that node**.

Examples:

- `path: root.txt`
- `path: nested.txt`
- `path: subfolder/deep.txt`

These are _not_ full paths.
They represent only the part needed at that position in the trie.
The full path is reconstructed by walking from the root down through forks.

#### **Target:**

The `target` field holds the **Swarm hash of the file the node points to**.

Example:

```
target: 0x9442e445c0d58a...
```

This hash is the immutable reference of the actual content uploaded to Swarm.

**Why is the target sometimes `0x000000...000`?**

Because **not every node corresponds to a file**.

Nodes represent **prefixes**, not necessarily files.

For example:

- The root node (`Node:` at the top) has no file associated so its `target` is zero.
- The node for `folder/` also has no file associated ‚Üí target is zero.
  It is just an internal directory-like prefix.

Only [**leaf nodes**](https://en.wikipedia.org/wiki/Glossary_of_graph_theory#leaf) where a file actually exists, have a non-zero target (the file‚Äôs Swarm reference).

So:

| Node type               | Example       | Has file? | Target        |
| ----------------------- | ------------- | --------- | ------------- |
| Internal directory node | `folder/`     | No        | `0x000...000` |
| Leaf node               | `nested.txt`  | Yes       | real hash     |
| Root node               | (top of tree) | No        | `0x000...000` |

:::warning
The `target` field in a manifest points to the raw file root chunk, not a manifest. `bee-js` and `swarm-cli` file download functions expect a file manifest, even for single-file uploads, so downloading using the raw target hash will not work properly.
Instead, download files by using the top-level directory root manifest plus the file path relative to the root hash as it is represented in the manifest trie.

Example:

```bash
curl http://127.0.0.1:1633/bzz/4d5e6e3eb532131e128b1cd0400ca249f1a6ce5d4005c0b57bf848131300df9d/folder/subfolder/deep.txt
```

Terminal output:

```bash
DEEP
```
:::

**Metadata:**

Metadata stores information about a file, such as:

- Content-Type
- Filename
- Website index/error docs (if configured)

Example:

```yaml
metadata:
  Content-Type: text/plain; charset=utf-8
  Filename: nested.txt
```

Only **file nodes** (leaf nodes) normally have metadata.
Internal nodes generally do not.

Metadata helps:

- gateways set HTTP headers (e.g., correct MIME type)
- browsers display files correctly
- filesystem-like behavior

#### **Putting it together:**

Let‚Äôs interpret a branch:

```
folder/
  nested.txt
```

This means:

1. There is a prefix node representing `"folder/"`.
2. Inside it, there is a file `"nested.txt"`.
3. The file node has:

   - a target (its Swarm content hash)
   - metadata (filename + content-type)

Meanwhile, `"folder/"` has **no file itself**, so its target is zero.

## Usage & Example Scripts

In this section we explain how to inspect and modify manifests for non-website directories. You can find the completed [example scripts on GitHub](https://github.com/ethersphere/examples/tree/main/manifests/directory).

In the following guides we will explain how to:

1. Upload a directory and print its manifest
2. Add a new file 
3. Move a file (delete + add new entry)

#### Pre-requisites:

- NodeJS and npm
- Linux or WSL preferred but most commands should work from windows Powershell with slight modifications
- git
- The RPC endpoint for a currently running Bee node (either on your machine or remote, try [Swarm Desktop](https://www.ethswarm.org/build/desktop) for a no-hassle way to get started)

If you'd like to follow along with the guides shown below, clone the [`ethersphere/examples` repo](https://github.com/ethersphere/examples/) and navigate to the `/manifests` folder:

```bash
git clone git@github.com:ethersphere/examples.git
cd examples/manifests/
```

Print the file tree to confirm you're in the right place:

```bash
user@machine:~/examples/manifests$ tree
.
‚îú‚îÄ‚îÄ directory
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ folder
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ nested.txt
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ subfolder
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ deep.txt
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ root.txt
‚îú‚îÄ‚îÄ env
‚îú‚îÄ‚îÄ manifestToJson.js
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ script-01.js
‚îú‚îÄ‚îÄ script-02.js
‚îî‚îÄ‚îÄ script-03.js
```

If you're using Powershell you can use the `tree /f` command instead and the output file tree should look similar.

After confirming, run `npm install` to install dependencies:

```bash
npm install
```

Locate the `env` file and add a period/full stop to change the file name to a standard `dotenv` file (`.env`). Then modify the file to replace `<BEE_API_ENDPOINT>` with your RPC endpoint and `<BATCH_ID>` with your own postage batch ID:

```bash
BEE_RPC_URL=<BEE_API_ENDPOINT> // Default: http://localhost:1633
POSTAGE_BATCH_ID=<BATCH_ID>
```

Great! Now you're all set up and ready to go.

### Uploading and Printing

In our first script, we will simply upload our sample directory and print its contents:

**script-01.js (initial upload script)**

```js
import { Bee, MantarayNode } from "@ethersphere/bee-js";
import path from "path";
import { fileURLToPath } from "url";
import "dotenv/config";
import { printManifest } from "./printManifest.js";

// Recreate __dirname for ES modules
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const bee = new Bee(process.env.BEE_RPC_URL);
const postageBatchId = process.env.POSTAGE_BATCH_ID;

// Build the folder path safely
const directoryPath = path.join(__dirname, "directory");

async function uploadDirectory() {
  try {
    console.log("Uploading directory:", directoryPath);

    // Upload using the resolved directory and get manifest reference
    const { reference } = await bee.uploadFilesFromDirectory(
      postageBatchId,
      directoryPath
    );

    console.log("Directory uploaded successfully!");
    console.log("Manifest reference:", reference.toHex());

    // Download each file using its relative path as recorded by the manifest
    const root = await bee.downloadFile(reference, "root.txt");
    const nested = await bee.downloadFile(reference, "folder/nested.txt");
    const deep = await bee.downloadFile(reference, "folder/subfolder/deep.txt");

    // Print out file contents
    console.log("root.txt:", root.data.toUtf8());
    console.log("folder/nested.txt:", nested.data.toUtf8());
    console.log("folder/subfolder/deep.txt:", deep.data.toUtf8());

    // Load the generated manifest
    const node = await MantarayNode.unmarshal(bee, reference);
    await node.loadRecursively(bee);

    // Print manifest in human readable format
    console.log("\n--- Manifest Tree ---");
    printManifest(node);
  } catch (error) {
    console.error("Error during upload or download:", error.message);
  }
}

uploadDirectory();
```

Note that in the script when downloading our files individually we must use the same relative paths that match the directory we uploaded:

```js
// Download each file using its relative path as recorded by the manifest
const root = await bee.downloadFile(reference, "root.txt");
const nested = await bee.downloadFile(reference, "folder/nested.txt");
const deep = await bee.downloadFile(reference, "folder/subfolder/deep.txt");
```

Run the script:

```bash
node script-01.js
```

If you've set up everything properly, you should see the file contents printed to the terminal followed by the manifest tree:

```json
Uploading directory: /home/user/examples/manifests/directory
Directory uploaded successfully!
Manifest reference: 4d5e6e3eb532131e128b1cd0400ca249f1a6ce5d4005c0b57bf848131300df9d
root.txt: ROOT
folder/nested.txt: NESTED
folder/subfolder/deep.txt: DEEP

--- Manifest Tree ---
{
  "path": "/",
  "target": "0x0000000000000000000000000000000000000000000000000000000000000000",
  "metadata": null,
  "forks": {
    "folder/": {
      "path": "folder/",
      "target": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "metadata": null,
      "forks": {
        "nested.txt": {
          "path": "nested.txt",
          "target": "0x9442e445c0d58adea58e0a8afcdcc28ed7642d7a4ff9a253e8f1595faafbb808",
          "metadata": {
            "Content-Type": "text/plain; charset=utf-8",
            "Filename": "nested.txt"
          },
          "forks": {}
        },
        "subfolder/deep.txt": {
          "path": "subfolder/deep.txt",
          "target": "0x6aa935879ad2a547e57ea6350338bd04ad758977b542e86b31c159f31834b8fc",
          "metadata": {
            "Content-Type": "text/plain; charset=utf-8",
            "Filename": "deep.txt"
          },
          "forks": {}
        }
      }
    },
    "root.txt": {
      "path": "root.txt",
      "target": "0x98e63f7e826a01634881874246fc873cdf06bb5409ff5f9ec61d1e2de1dd3bf6",
      "metadata": {
        "Content-Type": "text/plain; charset=utf-8",
        "Filename": "root.txt"
      },
      "forks": {}
    }
  }
}
```

Note and record the manifest reference returned before the manifest tree is printed:

```bash
Manifest reference: 4d5e6e3eb532131e128b1cd0400ca249f1a6ce5d4005c0b57bf848131300df9d
```

We will use this in the next section when adding a file manually to the manifest.

### Adding a New File

This script uploads a **new file** (e.g. `newfile.txt`) and then updates the existing manifest so the new file becomes part of the directory structure.

**script-02.js**

*The following script is almost identical to script-01.js, only the changed sections will be highlighted. Remember you can always refer to the complete version of the script in the examples repo.*

```js

import "dotenv/config"
import { Bee, MantarayNode } from "@ethersphere/bee-js"
import { printManifestJson } from './manifestToJson.js'

const bee = new Bee(process.env.BEE_RPC_URL)
const batchId = process.env.POSTAGE_BATCH_ID

// We specify the manifest returned from script-01.js here
const ROOT_MANIFEST = '4d5e6e3eb532131e128b1cd0400ca249f1a6ce5d4005c0b57bf848131300df9d'

async function addFileToManifest() {
    try {
        // Load the generated manifest from script-01.js
        const node = await MantarayNode.unmarshal(bee, ROOT_MANIFEST)
        await node.loadRecursively(bee)

        // File details for new file
        const filename = 'new.txt'
        const content = "Hi, I'm new here."
        const bytes = new TextEncoder().encode(content)

        // Upload raw file data 
        // Note we use "bee.uploadData()", not "bee.uploadFile()", since we need the root reference hash of the content, not a manifest reference. 
        const { reference } = await bee.uploadData(batchId, bytes)
        console.log('Uploaded raw reference:', reference.toHex())

        // Metadata must be a plain JS object ‚Äî NOT a Map or Uint8Array
        const metadata = {
            'Content-Type': 'text/plain; charset=utf-8',
            'Filename': filename,
        }

        // Insert the new file data into our new manifest
        node.addFork(filename, reference, metadata)

        // Save and print updated manifest
        const newManifest = await node.saveRecursively(bee, batchId)
        const newReference = newManifest.reference
        console.log('Updated manifest hash:', newReference.toHex())
        printManifestJson(node)

        // Download new file and print its contents
        const newFile = await bee.downloadFile(newReference, "new.txt")
        console.log("new.txt:", newFile.data.toUtf8())

    }
    catch (error) {
        console.error("Error during upload or download:", error.message)
    }
}

addFileToManifest()
```

Terminal output:

```bash
noah@NoahM16:~/examples/manifests$ node script-02.js
Uploaded raw reference: 3515db2f5e3c075b7546d7dd7dea1680c3e0785c6584e66b7e4f56fc344a0a78
Updated manifest hash: 4f67218844a814655c8d81aae4c4286a142318d672113973360c33c7930ce2f5
{
    "path": "/",
    "target": "0x0000000000000000000000000000000000000000000000000000000000000000",
    "metadata": null,
    "forks": {
        "folder/": {
            "path": "folder/",
            "target": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "metadata": null,
            "forks": {
                "nested.txt": {
                    "path": "nested.txt",
                    "target": "0x9442e445c0d58adea58e0a8afcdcc28ed7642d7a4ff9a253e8f1595faafbb808",
                    "metadata": {
                        "Content-Type": "text/plain; charset=utf-8",
                        "Filename": "nested.txt"
                    },
                    "forks": {}
                },
                "subfolder/deep.txt": {
                    "path": "subfolder/deep.txt",
                    "target": "0x6aa935879ad2a547e57ea6350338bd04ad758977b542e86b31c159f31834b8fc",
                    "metadata": {
                        "Content-Type": "text/plain; charset=utf-8",
                        "Filename": "deep.txt"
                    },
                    "forks": {}
                }
            }
        },
        "root.txt": {
            "path": "root.txt",
            "target": "0x98e63f7e826a01634881874246fc873cdf06bb5409ff5f9ec61d1e2de1dd3bf6",
            "metadata": {
                "Content-Type": "text/plain; charset=utf-8",
                "Filename": "root.txt"
            },
            "forks": {}
        },
        "new.txt": {
            "path": "new.txt",
            "target": "0x3515db2f5e3c075b7546d7dd7dea1680c3e0785c6584e66b7e4f56fc344a0a78",
            "metadata": {
                "Content-Type": "text/plain; charset=utf-8",
                "Filename": "new.txt"
            },
            "forks": {}
        }
    }
}
new.txt: Hi, I'm new here.
```

This produces a new manifest where `/new.txt` is now accessible as a root level entry.

### Moving a File

This script:

1. Removes `/new.txt` from the manifest
2. Adds it back under `/nested/deeper/new.txt`
3. Prints the updated manifest

**script-03.js**

```js
import "dotenv/config"
import { Bee, MantarayNode } from "@ethersphere/bee-js"
import { printManifestJson } from './manifestToJson.js'

const bee = new Bee(process.env.BEERPC_URL || process.env.BEE_RPC_URL)
const batchId = process.env.POSTAGE_BATCH_ID

// Manifest returned from script-02.js
const ROOT_MANIFEST = 'SCRIPT_2_MANIFEST'

async function moveFileInManifest() {
    try {
        // Load manifest generated in script-02
        const node = await MantarayNode.unmarshal(bee, ROOT_MANIFEST)
        await node.loadRecursively(bee)

        // Reload manifest to capture original file reference *before* deletion
        const original = await MantarayNode.unmarshal(bee, ROOT_MANIFEST)
        await original.loadRecursively(bee)

        const existing = original.find("new.txt")
        if (!existing) {
            throw new Error("Could not retrieve file reference for new.txt ‚Äî run script-02.js first.")
        }

        const fileRef = existing.targetAddress

        // STEP 1 ‚Äî Remove /new.txt
        node.removeFork("new.txt")
        console.log("Removed /new.txt from manifest.")

        // STEP 2 ‚Äî Re-add under /nested/deeper/new.txt
        const newPath = "nested/deeper/new.txt"

        node.addFork(
            newPath,
            fileRef,
            {
                "Content-Type": "text/plain; charset=utf-8",
                "Filename": "new.txt"
            }
        )

        console.log(`Added file under /${newPath}`)

        // STEP 3 ‚Äî Save updated manifest
        const updated = await node.saveRecursively(bee, batchId)
        const newManifestRef = updated.reference.toHex()

        console.log("Updated manifest hash:", newManifestRef)

        // STEP 4 ‚Äî Print JSON
        printManifestJson(node)

        // STEP 5 ‚Äî Download the file from its new location and print contents
        const downloaded = await bee.downloadFile(updated.reference, newPath)
        console.log(`\nContents of /${newPath}:`)
        console.log(downloaded.data.toUtf8())

    } catch (error) {
        console.error("Error while modifying manifest:", error.message)
    }
}

moveFileInManifest()
```

Terminal output:

```bash
user@machine:~/examples/manifests$ node script-03.js
Removed /new.txt from manifest.
Added file under /nested/deeper/new.txt
Updated manifest hash: 656ea924fb4d98b7fa327eb9e4d98ece6c2f4370515d23b40dfca71bc99a08a6
{
    "path": "/",
    "target": "0x0000000000000000000000000000000000000000000000000000000000000000",
    "metadata": null,
    "forks": {
        "folder/": {
            "path": "folder/",
            "target": "0x0000000000000000000000000000000000000000000000000000000000000000",
            "metadata": null,
            "forks": {
                "nested.txt": {
                    "path": "nested.txt",
                    "target": "0x9442e445c0d58adea58e0a8afcdcc28ed7642d7a4ff9a253e8f1595faafbb808",
                    "metadata": {
                        "Content-Type": "text/plain; charset=utf-8",
                        "Filename": "nested.txt"
                    },
                    "forks": {}
                },
                "subfolder/deep.txt": {
                    "path": "subfolder/deep.txt",
                    "target": "0x6aa935879ad2a547e57ea6350338bd04ad758977b542e86b31c159f31834b8fc",
                    "metadata": {
                        "Content-Type": "text/plain; charset=utf-8",
                        "Filename": "deep.txt"
                    },
                    "forks": {}
                }
            }
        },
        "root.txt": {
            "path": "root.txt",
            "target": "0x98e63f7e826a01634881874246fc873cdf06bb5409ff5f9ec61d1e2de1dd3bf6",
            "metadata": {
                "Content-Type": "text/plain; charset=utf-8",
                "Filename": "root.txt"
            },
            "forks": {}
        },
        "nested/deeper/new.txt": {
            "path": "nested/deeper/new.txt",
            "target": "0x3515db2f5e3c075b7546d7dd7dea1680c3e0785c6584e66b7e4f56fc344a0a78",
            "metadata": {
                "Content-Type": "text/plain; charset=utf-8",
                "Filename": "new.txt"
            },
            "forks": {}
        }
    }
}

Contents of /nested/deeper/new.txt:
Hi, I'm new here.
```

Now the file appears under:

```
/nested/deeper/root.txt
```

Note that the only new method we used was `node.removeFork()` to remove the entry from the manifest.

---

## Website Routing

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

## Routing on Swarm 

:::warning
Under construction, set for major revisions.
:::

Swarm does not behave like a traditional web server ‚Äî there is **no server-side routing**, and every route must correspond to a real file inside the site [manifest](/docs/develop/manifests).

If you try to use typical "clean URLs" like:

```bash
/about
/contact
/dashboard/settings
```

Swarm will look for literal files such as:

```bash
about
contact
dashboard/settings
```

...which obviously don‚Äôt exist.

There are two main strategies for addressing routing:

* **Hash-Based Client-Side Routing**
* **Manifest-Based Routing with Aliases or Index Files**

Now let‚Äôs look at each method:

## Client-Side Hash Routing 

This section explains how to add hash based client side routing to your Swarm hosted site so that you can have clean URLs for each page of your website. See the [routing project in the examples repo](https://github.com/ethersphere/examples/tree/main/routing) for a full working example implementation.

Swarm has no server backend running code and so can‚Äôt rewrite paths, so we can use **React Router‚Äôs `HashRouter`**, which keeps all routing inside the browser.

Below is the simplest way to set this up using **create-swarm-app** and then adding your own pages.

#### 1. Create a New Vite + React Project (with `create-swarm-app`)

Run:

```bash
npm init swarm-app@latest my-dapp-new vite-tsx
```

This generates a clean project containing:

```bash
src/
  App.tsx
  index.tsx
  config.ts
public/
index.html
package.json
```

You now have a fully working Vite/React app ready for Swarm uploads.

#### 2. Install React Router

Inside the project:

```bash
npm install react-router-dom
```

This gives you client-side navigation capability.

#### 3. Switch the App to Use Hash-Based Routing

Swarm only serves literal files, so `/#/about` is the only reliable way to have ‚Äúpages.‚Äù

Replace your `App.tsx` with:

```tsx
import { HashRouter, Routes, Route, Link } from 'react-router-dom'
import { Home } from './Home'
import { About } from './About'
import { NotFound } from './NotFound'

export function App() {
    return (
        <HashRouter>
            <nav style={{ display: 'flex', gap: '12px', padding: '12px' }}>
                <Link to="/">Home</Link>
                <Link to="/about">About</Link>
            </nav>

            <Routes>
                <Route path="/" element={<Home />} />
                <Route path="/about" element={<About />} />
                <Route path="*" element={<NotFound />} />
            </Routes>
        </HashRouter>
    )
}
```

This gives you usable routes:

```bash
/#/         ‚Üí Home
/#/about    ‚Üí About
/#/anything ‚Üí React 404 page
```

#### 4. Add Your Page Components

Example `Home.tsx`:

```tsx
export function Home() {
  return (
    
      Home
      Welcome to your Swarm-powered app.
    
  )
}
```

Example `About.tsx`:

```tsx
export function About() {
  return (
    
      About
      This demo shows how to upload files or directories to Swarm using Bee-JS.
    
  )
}
```

Example `NotFound.tsx`:

```tsx
export function NotFound() {
  return (
    
      Page Not Found
      Return to Home
    
  )
}
```

#### 5. Add a Static `404.html` for Non-Hash URLs

Swarm still needs a fallback for URLs like:

```
/non-existent-file
```

Create `public/404.html`:

```html
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <title>404 ‚Äì Not Found</title>
  <style>
    body { font-family: sans-serif; padding: 40px; }
    a { color: #007bff; }
  </style>
</head>

<nav style="display: flex; gap: 12px; padding: 12px">
  Home
  About
</nav>

<body>
  404
  This page doesn't exist.
  Return to Home
</body>
</html>
```

Vite will automatically include this in `dist/`.

This file handles **non-hash** missing paths.
React handles **hash** missing paths.

#### 6. Build the Project

Before uploading, compile the Vite app into a static bundle:

```bash
npm run build
```

This produces a `dist/` folder containing:

```bash
dist/
  index.html
  404.html
  assets/
```

Everything inside `dist/` will be uploaded to your Swarm feed.

#### 7. Create a Publisher Identity and Deploy Using a Feed Manifest 

For stable URLs, use a **feed manifest** reference. This gives you a permanent Swarm URL that always resolves to the latest version of your content.

Create an identity (if you don‚Äôt have one yet):

```bash
swarm-cli identity create web-publisher
```

Upload your built site to the feed:

<Tabs>
    <TabItem value="linux" label="Linux / macOS">

    ```bash
    swarm-cli feed upload ./dist \
    --identity web-publisher \
    --topic-string website \
    --stamp <BATCH_ID> \
    --index-document index.html \
    --error-document 404.html
    ```
    </TabItem>
    
    <TabItem value="powershell" label="Windows PowerShell">

    ```bash
    swarm-cli feed upload .\dist `
    --identity web-publisher `
    --topic-string website `
    --stamp 3d98a22f522377ae9cc2aa3bca7f352fb0ed6b16bad73f0246b0a5c155f367bc `
    --index-document index.html `
    --error-document 404.html

    ```
    </TabItem>
</Tabs>

The output includes:

* the **content hash**
* the **feed manifest URL** ‚Üí this is your **permanent website URL**
* stamp usage details

Example:

```bash
Feed Manifest URL:
http://localhost:1633/bzz/<feed-manifest-hash>/
```

This URL never changes, even when you update your site.

#### 8. Visit Your Site

* Home:
  `/#/`

* About:
  `/#/about`

* Invalid hash route: handled by `NotFound.tsx`

* Invalid non-hash route: handled by `404.html`

#### Summary

You now have:

* A Vite + React app
* Hash-based routing fully compatible with Swarm
* A static 404 for non-hash paths
* A React 404 for invalid hash paths
* Stable, versioned deployments using feed manifests

## Manifest Based Routing

The second routing method involves directly manipulating the manifest so that routes resolve properly to the intended content. There are two primary approaches to manifest based routing:

* Alias based routing with arbitrary file names
* Directory based with index files

### 1. Upload the Site 

Start by uploading the site:

```ts
const { reference } = await bee.uploadFilesFromDirectory(batchId, './site')
```

Without manifest edits, routes only work via exact file paths like:

```
/index.html
/about.html
/contact.html
```

Trying to access `/about` or `/about/` will fail.

### 2. Fix Routing With Manifest Manipulation

You can fix clean URLs using **two strategies**:

#### Strategy A: Add Aliases to the Manifest

```ts
node.addFork('about', referenceForAbout, metadata)
node.addFork('about/', referenceForAbout, metadata)
```

Now `/about` and `/about/` work like `/about.html`.

#### Strategy B: Use Directory Index Files

Restructure files like:

```
/about/index.html
/contact/index.html
```

And add them to the manifest like:

```ts
node.addFork('about/', referenceForAboutIndex, metadata)
```

This gives you full directory-style clean URLs.

## Remove and Redirect Routes

To "delete" a page you would need to remove all entries for it from the manifest to remove it entirely:

```js
node.removeFork('old-page.html')
```
For example, if you had manually added:

```js
node.addFork('about', referenceForAbout, metadata)
node.addFork('about/', referenceForAbout, metadata)
```

You will need to make sure to fully remove all entries for that file from the manifest to really "delete" it from your site.

```js
node.removeFork('about')
node.removeFork('about/')
node.removeFork('about.html')
```

:::info 
The file is not removed from the Swarm network since data on Swarm is immutable. Anyone who has its reference can still retrieve it. Removing it from your manifest just means it's no longer available through a route on your site.
:::

We are adding a record with the same path but now pointing at new content to achieve "redirect" like behavior.

To redirect:

```js
node.addFork('old-page.html', newPageReference, metadata)
```

This lets you ‚Äúdeprecate‚Äù a page and point old paths to new ones.

#### 4. Manifest Routing Enables Dynamic Content

Once you understand manifest-based routing, you can dynamically:

* Add new paths (e.g. blog posts, product pages)
* Create custom routes
* Redirect old paths
* Remove unwanted paths

Next, learn how to combine all the previously covered concepts to enable [dynamic content](/docs/develop/dynamic-content) on Swarm. It will allow you to turn Swarm into a decentralized CMS and decouple your front end from your back end.

---

## Bee JS

bee-js is Bee's complementary JavaScript library. It is the technology underpinning [swarm-cli](/docs/bee/working-with-bee/swarm-cli) and [Swarm Desktop](/docs/desktop/introduction/) and is a powerful tool for building completely decentralized apps.

See the [bee-js](https://bee-js.ethswarm.org/) documentation for detailed information on using and installing the library.

---

## Postage Stamp Batches

import VolumeAndDurationCalc from '@site/src/components/VolumeAndDurationCalc.js';
import AmountAndDepthCalc from '@site/src/components/AmountAndDepthCalc.js';

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

import { globalVariables } from '/src/config/globalVariables'

A postage batch is required to upload data to Swarm. Postage stamp batches represent _right to write_ data on Swarm's [DISC (Distributed Immutable Store of Chunks)](/docs/concepts/DISC/). The parameters which control the duration and quantity of data that can be stored by a postage batch are `depth` and `amount`, with `depth` determining data volume that can be uploaded by the batch and `amount` determining storage duration of data uploaded with the batch. 

:::info
      The storage volume and duration are both non-deterministic. Volume is non-deterministic due to the details of how [postage stamp batch utilization](/docs/concepts/incentives/postage-stamps#batch-utilisation) works. While duration is non-deterministic due to price changes made by the [price oracle contract](/docs/concepts/incentives/price-oracle).

      **Storage volume and `depth`:**

      When purchasing stamp batches for larger volumes of data (by increasing the `depth` value), the amount of data which can be stored becomes increasingly more predictable. For example, at `depth` 22 a batch can store between 4.93 GB and 17.18 GB, while at `depth` 28, a batch can store between 1.0 and 1.1 TB of data, and at higher depths the difference between the minimum and maximum storage volumes approach the same value.

      **Storage duration and `amount`:** 

      The duration of time for which a batch can store data is also non-deterministic since the price of storage is automatically adjusted over time by the [price oracle contract](/docs/concepts/incentives/price-oracle). However, limits have been placed on how swiftly the price of storage can change, so there is no danger of a rapid change in price causing postage batches to unexpectedly expire due to a rapid increase in price. You can view a history of price changes by inspecting the events emitted by the oracle contract, or also through the [Swarmscan API](https://api.swarmscan.io/v1/events/storage-price-oracle/price-update). As you can see, if and when postage batch prices are updated, the updates are quite small. Still, since it is not entirely deterministic, it is important to monitor your stamp batch TTL (time to live) as it will change along with price oracle changes. You can inspect your batch's TTL using the `/stamps` endpoint of the API:

      ```bash
      root@noah-bee:~# curl -s  localhost:1633/stamps | jq
      {
        "stamps": [
          {
            "batchID": "f56af59cc2c785a3b45bbf3e46c3c4b20f80379339ef337b5bbf45ebe5629a66",
            "utilization": 0,
            "usable": true,
            "label": "",
            "depth": 17,
            "amount": "432072000",
            "bucketDepth": 16,
            "blockNumber": 38498819,
            "immutableFlag": true,
            "exists": true,
            "batchTTL": 82943
          }
        ]
      }
      ```
      Here we can see from the `batchTTL` that `82943` seconds remain, or approximately 23 hours. 
      :::

For a deeper understanding of how `depth` and `amount` parameters determine the data volume and storage duration of a postage batch, see the [postage stamp page](/docs/concepts/incentives/postage-stamps/).

## Fund your node's wallet.

In order to purchase a postage stamp batch, your node's Gnosis Chain address needs to be funded with sufficient xDAI to pay gas for transaction fees on Gnosis Chain as well as sufficient xBZZ to pay for the cost of the postage stamp batch itself.

xBZZ can be obtained from a variety of different centralized and decentralized exchanges. You can find more information on [where to obtain xBZZ](https://www.ethswarm.org/get-bzz#how-to-get-bzz) on the Ethswarm homepage.

xDAI can be obtained from a wide range of centralized and decentralized exchanges. See [this list of exchanges](https://docs.gnosischain.com/about/tokens/xdai) from the Gnosis Chain documentation to get started.

You can learn more details from the [Fund Your Node](/docs/bee/installation/fund-your-node/) section.

## Buying a stamp batch

When interacting with the Bee API directly, `amount` and `depth` are passed as path parameters:

```bash
curl -s -X POST http://localhost:1633/stamps/<amount>/<depth>
```

And with Swarm CLI, they are set using option flags:

```bash
swarm-cli stamp buy --depth <depth value> --amount <amount value>
```

<Tabs
defaultValue="api"
values={[
{label: 'API', value: 'api'},
{label: 'Swarm CLI', value: 'swarm-cli'},
]}>
<TabItem value="api">

#### API

```bash
curl -s -X POST http://localhost:1633/stamps/100000000/20
```

```bash
{
  "batchID": "8fcec40c65841e0c3c56679315a29a6495d32b9ed506f2757e03cdd778552c6b",
  "txHash": "0x51c77ac171efd930eca8f3a77e3fcd5aca0a7353b84d5562f8e9c13f5907b675"
}
```

</TabItem>

<TabItem value="swarm-cli">

#### Swarm CLI

```bash
swarm-cli stamp buy --depth 20 --amount 100000000
```

```bash
Estimated cost: 0.010 BZZ
Estimated capacity: 4.00 GB
Estimated TTL: 5 hours 47 minutes 13 seconds
Type: Mutable
When a mutable stamp reaches full capacity, it still permits new content uploads. However, this comes with the caveat of overwriting previously uploaded content associated with the same stamp.
? Confirm the purchase Yes
Stamp ID: f4b9830676f4eeed4982c051934e64113dc348d7f5d2ab4398d371be0fbcdbf5
```
</TabItem>
</Tabs>

:::info
Once your batch has been purchased, it will take a few minutes for other Bee nodes in the Swarm to catch up and register your batch. Allow some time for your batch to propagate in the network before proceeding to the next step.
:::

## Setting stamp batch parameters and options

When purchasing a batch of stamps there are several parameters and options which must be considered. The `depth` parameter will control how many chunks can be uploaded with a batch of stamps. The `amount` parameter determines how much xBZZ will be allocated per chunk, and therefore also controls how long the chunks will be stored. While the `immutable` header option sets the batch as either mutable or immutable, which can significantly alter the behavior of the batch utilisation (more details below).

### Choosing *depth*

:::caution
The minimum value for `depth` is 17, however a higher depth value is recommended for most use cases due to the [mechanics of stamp batch utilisation](/docs/concepts/incentives/postage-stamps/#batch-utilisation). See [the depths utilisation table](/docs/concepts/incentives/postage-stamps/#effective-utilisation-table) to help decide which depth is best for your use case.
:::

One notable aspect of batch utilisation is that the entire batch is considered fully utilised as soon as any one of its buckets are filled. This means that the actual amount of chunks storable by a batch is less than the nominal maximum amount. 

See the [postage stamp page](/docs/concepts/incentives/postage-stamps) for a more complete explanation of how batch utilisation works and a [table](/docs/concepts/incentives/postage-stamps#effective-utilisation-table) with the specific amounts of data which can be safely uploaded for each `depth` value. 

### Choosing *amount*

:::caution
The minimum `amount` value for purchasing stamps is required to be at least enough to pay for 24 hours of storage. To find this value multiply the lastPrice value from the postage stamp contract times 17280 (the number of blocks in 24 hours). You can also use the [calculator](#calculators) below. This requirement is in place in order to prevent spamming the network.
:::

The `amount` parameter determines how much xBZZ is assigned per chunk for a postage stamp batch. You can use the calculators below to find the appropriate `amount` value for your target duration of storage and can also preview the price. For more information see the [postage stamp](/docs/concepts/incentives/postage-stamps#batch-depth-and-batch-amount) page where a more complete description is included.

### Mutable or Immutable?

Depending on the use case, uploaders may desire to use mutable or immutable batches. The fundamental difference between immutable and mutable batches is that immutable batches become unusable once their capacity is filled, while for mutable batches, once their capacity is filled, they may continue to be used, however older chunks of data will be overwritten with the newer once over capacity. The default batch type is immutable. In order to set the batch type to mutable, the `immutable` header should be set to `false`. See [this section on postage stamp batch utilisation](/docs/concepts/incentives/postage-stamps#which-type-of-batch-to-use) to learn more about mutable vs immutable batches, and about which type may be right for your use case.

## Calculators

The following postage batch calculators allow you to conveniently find the depth and amount values for a given storage duration and storage volume, or to find the storage duration and storage volume for a given depth and amount. The results will display the cost in xBZZ for the postage batch. The current pricing information is sourced from the Swarmscan API and will vary over time. 

:::info
The 'effective volume' is the volume of data that can safely stored for each storage depth. The 'theoretical max volume' is significantly lower than the effective volume at lower depths and the two values trend towards the same value at higher depths. The lowest depth with an effective volume above zero is 22, with an effective depth of 4.93 GB. Lower depth values can be used for smaller uploads but do not come with the same storage guarantees. [Learn more here](/docs/concepts/incentives/postage-stamps#effective-utilisation-table). 
:::

### Depth & Amount to Time & Volume Calculator

<VolumeAndDurationCalc />

### Time & Volume to Depth & Amount Calculator

The recommended depth in this calculator's results is the lowest depth value whose [effective volume](/docs/concepts/incentives/postage-stamps#effective-utilisation-table) is greater than the entered volume. 

<AmountAndDepthCalc />

## Viewing Stamps

To check on your stamps, send a GET request to the stamp endpoint.

<Tabs
defaultValue="api"
values={[
{label: 'API', value: 'api'},
{label: 'Swarm CLI', value: 'swarm-cli'},
]}>
<TabItem value="api">

#### API

```bash
curl http://localhost:1633/stamps
```

```bash
{
  "stamps": [
    {
      "batchID": "f4b9830676f4eeed4982c051934e64113dc348d7f5d2ab4398d371be0fbcdbf5",
      "utilization": 0,
      "usable": true,
      "label": "",
      "depth": 20,
      "amount": "100000000",
      "bucketDepth": 16,
      "blockNumber": 30643611,
      "immutableFlag": true,
      "exists": true,
      "batchTTL": 20588,
      "expired": false
    }
  ]
}
```

</TabItem>

<TabItem value="swarm-cli">

#### Swarm CLI

```bash
swarm-cli stamp list
```

```bash
Stamp ID: f4b9830676f4eeed4982c051934e64113dc348d7f5d2ab4398d371be0fbcdbf5
Usage: 0%
Remaining Capacity: 4.00 GB
TTL: 5 hours 42 minutes 18 seconds
Expires: 2023-10-26

```
</TabItem>
</Tabs>

:::info
It is not possible to reupload unencrypted content which was stamped using an expired postage stamp. 
:::

## Checking the remaining TTL (time to live) of your batch

:::info
At present, TTL is a primitive calculation based on the current storage price and the assumption that storage price will remain static in the future. As more data is uploaded into Swarm, the price of storage will begin to increase. For data that it is important to keep alive, make sure your batches have plenty of time to live!
:::

In order to make sure your *batch* has sufficient *remaining balance* to be stored and served by nodes in its [*area of responsibility*](/docs/references/glossary#2-area-of-responsibility-related-depths), you must regularly check on its _time to live_ and act accordingly. The *time to live* is the number of seconds before the chunks will be considered for garbage collection by nodes in the network.

The remaining *time to live* in seconds is shown in the API in the returned json object as the value for `batchTTL`, and with Swarm CLI you will see the formatted TTL as the `TTL` value.

<Tabs
defaultValue="api"
values={[
{label: 'API', value: 'api'},
{label: 'Swarm CLI', value: 'swarm-cli'},
]}>
<TabItem value="api">

#### API

```bash
curl http://localhost:1633/stamps
```

```bash
{
  "stamps": [
    {
      "batchID": "f4b9830676f4eeed4982c051934e64113dc348d7f5d2ab4398d371be0fbcdbf5",
      "utilization": 0,
      "usable": true,
      "label": "",
      "depth": 20,
      "amount": "100000000",
      "bucketDepth": 16,
      "blockNumber": 30643611,
      "immutableFlag": true,
      "exists": true,
      "batchTTL": 20588,
      "expired": false
    }
  ]
}
```

</TabItem>

<TabItem value="swarm-cli">

#### Swarm CLI

```bash
swarm-cli stamp list
```

```bash
Stamp ID: f4b9830676f4eeed4982c051934e64113dc348d7f5d2ab4398d371be0fbcdbf5
Usage: 0%
Remaining Capacity: 4.00 GB
TTL: 5 hours 42 minutes 18 seconds
Expires: 2023-10-26

```
</TabItem>
</Tabs>

## Top up your batch

:::danger
Don't let your batch run out! If it does, you will need to restamp and resync your content.
:::

If your batch is starting to run out, or you would like to extend the life of your batch to protect against storage price rises, you can increase the batch TTL by topping up your batch using the stamps endpoint, passing in the relevant batchID into the HTTP PATCH request.

<Tabs
defaultValue="api"
values={[
{label: 'API', value: 'api'},
{label: 'Swarm CLI', value: 'swarm-cli'},
]}>
<TabItem value="api">

#### API

```bash
curl -X PATCH "http://localhost:1633/stamps/topup/6d32e6f1b724f8658830e51f8f57aa6029f82ee7a30e4fc0c1bfe23ab5632b27/10000000"
```

</TabItem>

<TabItem value="swarm-cli">

#### Swarm CLI

List available stamps.

```bash
swarm-cli stamp list
```

Copy stamp ID.
```bash
Stamp ID: daa8c5b36e1cf481b10118a8b02430a6f22618deaa6ba5aa4ea660de66aa62db
Usage: 13%
Remaining Capacity: 3.50 GB
TTL: 183 days 1 hour 37 minutes 8 seconds
Expires: 2024-05-02
```

Use `swarm-cli stamp topup` with the `--amount` and `--stamp` parameters set with the amount to topup in PLUR and the stamp ID.

```bash
swarm-cli stamp topup --amount 10000000  --stamp daa8c5b36e1cf481b10118a8b02430a6f22618deaa6ba5aa
4ea660de66aa62db
```

Wait for topup transaction to complete.
```bash
‚¨° ‚¨° ‚¨¢ Topup in progress. This may take a while.
Stamp ID: daa8c5b36e1cf481b10118a8b02430a6f22618deaa6ba5aa4ea660de66aa62db
Depth: 20
Amount: 100000001000
```

</TabItem>
</Tabs>

## Dilute your batch

In order to store more data with a batch of stamps, you must "dilute" the batch. Dilution simply refers to increasing the depth of the batch, thereby allowing it to store a greater number of chunks. As dilution only increases the the depth of a batch and does not automatically top up the batch with more xBZZ, dilution will decrease the TTL of the batch. Therefore if you wish to store more with your batch but don't want to decrease its TTL, you will need to both dilute and top up your batch with more xBZZ.

<Tabs
defaultValue="api"
values={[
{label: 'API', value: 'api'},
{label: 'Swarm CLI', value: 'swarm-cli'},
]}>
<TabItem value="api">

#### API

Here we call the `/stamps` endpoint and find a batch with `depth` 24 and a `batchTTL` of 2083223 which we wish to dilute:

```bash
curl  http://localhost:1633/stamps
```

```json
{
    "stamps": [
        {
            "batchID": "0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5",
            "utilization": 0,
            "usable": true,
            "label": "",
            "depth": 24,
            "amount": "10000000000",
            "bucketDepth": 16,
            "blockNumber": 29717348,
            "immutableFlag": false,
            "exists": true,
            "batchTTL": 2083223,
            "expired": false
        }
    ]
}
```

Next we call the [`dilute`](/api/#tag/Postage-Stamps/paths/~1stamps~1dilute~1{batch_id}~1{depth}/patch) endpoint to increase the `depth` of the batch using the `batchID` and our new `depth` of 26:

```bash
curl -s -XPATCH http://localhost:1633/stamps/dilute/0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5/26
```
And a `txHash` of our successful transaction:

```bash
{
    "batchID": "0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5",
    "txHash": "0x298e80358b3257292752edb2535a1cd84440c074451b61f78fab349aea4962b7"
}
```

And finally we use the `/stamps` endpoint again to confirm the new `depth` and decreased `batchTTL`:

```bash
curl  http://localhost:1633/stamps
```

We can see the new `depth` of 26 and a decreased `batchTTL` of 519265.

```json
{
    "stamps": [
        {
            "batchID": "0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5",
            "utilization": 0,
            "usable": true,
            "label": "",
            "depth": 26,
            "amount": "10000000000",
            "bucketDepth": 16,
            "blockNumber": 29717348,
            "immutableFlag": false,
            "exists": true,
            "batchTTL": 519265,
            "expired": false
        }
    ]
}
```

</TabItem>

<TabItem value="swarm-cli">

#### Swarm CLI

List available stamps, make sure to use the `--verbose` flag so that we can see the batch depth.

```bash
swarm-cli stamp list --verbose
```

We have a stamp batch with depth 20 we want to dilute. Copy stamp ID of that batch.

```bash
Listing postage stamps...
Stamp ID: daa8c5b36e1cf481b10118a8b02430a6f22618deaa6ba5aa4ea660de66aa62db
Usage: 13%
Remaining Capacity: 3.50 GB
Total Capacity (mutable): 4.00 GB
TTL: 182 days 4 hours 39 minutes 47 seconds
Expires: 2024-05-02
Depth: 20
Bucket Depth: 16
Amount: 100010002000
Usable: true
Utilization: 2
Block Number: 29734329
```

Use `swarm-cli stamp dilute` with the `--depth` and `--stamp` parameters set with the desired new depth and the stamp ID.

```bash
swarm-cli stamp dilute --depth 21 --stamp daa8c5b36e1cf481b10118a8b02430a6f22618deaa6ba5aa4ea660de66aa62db
```

```bash
‚¨° ‚¨° ‚¨¢ Dilute in progress. This may take a while.
Stamp ID: daa8c5b36e1cf481b10118a8b02430a6f22618deaa6ba5aa4ea660de66aa62db
Depth: 20
Amount: 100010002000
```

</TabItem>
</Tabs>

## Stewardship

The stewardship endpoint in combination with [pinning](/docs/develop/tools-and-features/pinning) can be used to guarantee that important content is always available. It is used for checking whether the content for a Swarm reference is retrievable and for re-uploading the content if it is not.

An HTTP GET request to the `stewardship` endpoint checks to see whether the content for the specified Swarm reference is retrievable:

:::info
`stewardship` is not currently supported by Swarm CLI
:::

```bash
curl "http://localhost:1633/stewardship/c0c2b70b01db8cdfaf114cde176a1e30972b556c7e72d5403bea32e
c0207136f"
```
```json
{
  "isRetrievable": true
}
```

If the content is not retrievable, an HTTP PUT request can be used to re-upload the content:

```bash
curl -X PUT "http://localhost:1633/stewardship/c0c2b70b01db8cdfaf114cde176a1e30972b556c7e72d5403bea32ec0207136f"
```

Note that for the re-upload to succeed, the associated content must be available locally, either pinned or cached. Since it isn't easy to predict if the content will be cached, for important content pinning is recommended.

---

## Chunk Types

Swarm is home to many types of chunks, but these can be categoried
into 4 broad categories. Read [The Book of Swarm](https://www.ethswarm.org/the-book-of-swarm-2.pdf) for
more information on how swarm comes together.

## Content Addressed Chunks

Content addressed chunks are chunks whose addresses are determined by the BMT hashing algorithm. This means you can be sure that all content addressed chunks content is already verified - no more need to check md5 hashes of your downloaded data!

:::warning
To be able trust your data, you must run your own Bee node that automatically verifies data, using gateways puts your trust in the gateway operators.
:::

## Trojan Chunks

Trojan chunks are a special version of content addressed chunks that have been 'mined' so that their natural home is in a particular area of the Swarm. If the destination node is in the right neighborhood, it will be able to receive and decrypt the message. See [PSS](/docs/develop/tools-and-features/pss) for more information, or check out the [bee-js](https://bee-js.ethswarm.org/docs/api/classes/Bee/#psssend) bindings.

## Single Owner Chunks

Single Owner Chunks are distinct from Trojan and Content Addressed
Chunks and are the only other type of chunk which is allowed in
Swarm. These chunks represent part of Swarm's address space which is
reserved just for your personal Ethereum key pair! Here you can write
whatever you'd please. Single Owner Chunks are the technology that
powers Swarm's [feeds](/docs/develop/tools-and-features/feeds), but they are
capable of much more! Look out for more chats about this soon, and for
more info read [The Book of Swarm](https://www.ethswarm.org/the-book-of-swarm-2.pdf).

## Custom Chunk Types

Although all chunks must satisfy the constraints of either being addressed by the BMT hash of their payload, or assigned by the owner of an Ethereum private key pair, so much more is possible. How else can you use the DISC to distribute and store your data? We're excited to see what you come up with! üí°

Share your creations in the [#develop-on-swarm](https://discord.gg/C6dgqpxZkU) channel of our [Discord Server](https://discord.gg/wdghaQsGq5).

---

## Developer mode

You can start the bee in `dev` mode by running the command:

```bash
bee dev
```

It will start an instance with volatile persistence all back-ends mocked.

As a developer you interact with all the usual HTTP endpoints, for instance you can buy postage stamps and use them to upload files, which will be saved to memory.

## Configuration options

It accepts the same configuration options as a normal bee but it will ignore the ones that are not relevant (accounting, networking, blockchain etc).

---

## Erasure Coding(Tools-and-features)

import RedundancyCalc from '@site/src/components/RedundancyCalc.js';

[Erasure coding](/docs/concepts/DISC/erasure-coding) is a powerful method for safeguarding data, offering robust protection against partial data loss. This technique involves dividing the original data into multiple fragments and generating extra parity fragments to introduce redundancy. A key advantage of erasure coding is its ability to recover the complete original data even if some fragments are lost. Additionally, it offers the flexibility to customize the level of data loss protection, making it a versatile and reliable choice for preserving data integrity on Swarm. For a more in depth dive into erasure coding on Swarm, see the [erasure coding paper](https://papers.ethswarm.org/p/erasure/) from the Swarm research team. 

## Uploading With Erasure Coding

Erasure coding is available for the [`/bytes`](/api/#tag/Bytes) and [`/bzz`](/api/#tag/BZZ) endpoints, however it is not available for the [`/chunks`](/api/#tag/Chunk) endpoint which deals with single chunks. Since erasure coding relies on splitting data into chunks and the chunk is the smallest unit of data within Swarm which cannot be further subdivided, erasure coding is not applicable for the `/chunks` endpoint which deals with single chunks.

To upload data to Swarm using erasure coding, the `swarm-redundancy-level: <integer>` header is used:

```bash
    curl \
    -X POST http://localhost:1633/bzz?name=test.txt \
    -H "swarm-redundancy-level: 1" \
    -H "swarm-postage-batch-id: 54ba8e39a4f74ccfc7f903121e4d5d0fc40732b19efef5c8894d1f03bdd0f4c5" \
    -H "Content-Type: text/plain" \
    --data-binary @test.txt

    {"reference":"c02e7d943fbc0e753540f377853b7181227a83e773870847765143681511c97d"}
```

The accepted values for the `swarm-redundancy-level` header range from the default of 0 up to 4. Each level corresponds to a different level of data protection, with erasure coding turned off at 0, and at its maximum at 4. Each increasing level provides increasing amount of data redundancy offering greater protection against data loss. 

| Redundancy Level Value | Level Name |       
| ---------------- | --------- | 
| 1                | Medium    |  
| 2                | Strong    | 
| 3                | Insane    | 
| 4                | Paranoid  | 

For more details about each level of protection refer to the [erasure coding page](/docs/concepts/DISC/erasure-coding) in the learn section and refer to the [erasure coding paper](https://papers.ethswarm.org/p/erasure/) for an even deeper dive.

## Cost Calculator Widget

This calculator takes as input an amount of data and an erasure coding redundancy level, and outputs the number of additional parity chunks required to erasure code that amount of data as well as the increase in cost to upload vs. a non-erasure encoded upload:

<RedundancyCalc />

For more details of erasure coding costs, see [here](/docs/concepts/DISC/erasure-coding).

## Downloading Erasure Encoded Data

For a downloader, the process for downloading a file which has been erasure encoded does not require any changes from the [normal download process](/docs/develop/upload-and-download). There are several options for adjusting the default behaviour for erasure encoded downloads, however there is no need to adjust them.

### Default Download Behaviour

Erasure coding retrieval for downloads is enabled by default, so there is no need for a downloader to explicitly enable the feature. The default download behaviour is to use the DATA strategy with fallback enabled. With these settings, first an attempt will be made to download the data chunks only. If any of the data chunks are missing, then the retrieval method will fall back to the RACE strategy (PROX is not currently implemented and so will be skipped). With the RACE strategy, an attempt will be made to download all data and parity chunks, and chunks will continue to be downloaded until enough have been retrieved to reconstruct the original data. 

### Options

:::warning
Do not adjust these options unless you know exactly what you are doing. The default settings are the best option for almost all cases. 
:::

When downloading erasure encoded data, there are three related headers which may be used: `swarm-redundancy-strategy`, `swarm-redundancy-fallback-mode: <integer>`, and `swarm-chunk-retrieval-timeout`. 

* `swarm-redundancy-strategy`:  This header allows you to set the retrieval strategy for fetching chunks. The accepted values range from 0 to 3. Each number corresponds to a different chunk retrieval strategy. The numbers stand for the NONE, DATA, PROX and RACE strategies respectively which are described in greater detail in [the API reference](/api/#tag/BZZ) (also see [the erasure code paper](https://papers.ethswarm.org/p/erasure/) for even more in-depth descriptions).  With each increasing level, there will be a potentially greater bandwidth cost. 

    :::info Retrieval Strategies
    0. NONE: This strategy is based on direct retrieval of data chunks without pre-fetching, with parity chunks ignored. No pre-fetching is used (data chunks are fetched sequentially). 
    1. DATA: The same as NONE, except that data chunks are pre-fetched (data chunks are fetched in parallel in order to reduce latency).
    2. PROX: For this strategy, the chunks closest (in Kademlia distance) to the node are retrieved first. *(Not yet implemented.)*
    3. RACE: Initiates requests for all data and parity chunks and continues to retrieve chunks until enough chunks are retrieved that the original data can be reconstructed. 
    :::

* `swarm-redundancy-fallback-mode: <boolean>`: Enables the fallback feature for the redundancy strategies so that if one of the retrieval strategies fails, it will fallback to the more intensive strategy until retrieval is successful or retrieval fails. Default is `true`.

* `swarm-chunk-retrieval-timeout: <boolean>`: Allows you to specify the timeout time for chunk retrieval with a default value of 30 seconds. *(This is primarily used by the Bee development team for testing and it's recommended that Bee users do not need to use this option.)*

An example download request may look something like this:

```bash
    curl -OJL \
    -H "swarm-redundancy-strategy: 3" \
    -H "swarm-redundancy-fallback-mode: true" \
     http://localhost:1633/bzz/c02e7d943fbc0e753540f377853b7181227a83e773870847765143681511c97d/

       % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
```

For this request, the redundancy strategy is set to 3 (RACE), which means that it will initiate a request for all data and parity chunks and continue to retrieve chunks until enough have been retrieved to reconstruct the source data. This is in contrast with the default strategy of DATA where only the data chunks will be retrieved.

However, as noted above, it is recommended to not adjust the default settings for these options, so a typical request would actually look like this (which is the exact same as a [normal download](/docs/develop/upload-and-download) without any additional options set):

```bash
    curl -OJL http://localhost:1633/bzz/c02e7d943fbc0e753540f377853b7181227a83e773870847765143681511c97d/

       % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
```

This means that there is no need to inform downloaders that a file uses erasure coding, as even with the default download behaviour reconstruction of the source file will be attempted if any chunks are missing.

---

## Feeds

Swarm feeds cleverly combine
[single owner chunks](/docs/develop/tools-and-features/chunk-types)
into a data structure which enables you to have static addresses for
your mutable content. This means that you can signpost your data for
other Bees, and then update it at will.

:::info
Although it's possible to interact with feeds directly, it can involve
a little data juggling and crypto magic. For the easiest route, see
[the bee-js feeds functionality](/docs/develop/tools-and-features/bee-js) and
[swarm-cli](/docs/bee/working-with-bee/swarm-cli), or for the super 1337,
share your implementations in other languages in the
[#develop-on-swarm](https://discord.gg/C6dgqpxZkU) channel of our
[Discord Server](https://discord.gg/wdghaQsGq5).
:::

### What are Feeds?

A feed is a collection of Single Owner Chunks with predicatable addresses. This enables creators to upload pointers to data so that consumers of the feed are able to find the data in Swarm using only an _Ethereum address_ and _Topic ID_.

### Creating and Updating a Feed

In order to edit a feed, you will need to sign your chunks using an
Ethereum keypair. For the intrepid, check out the [The Book of Swarm](https://www.ethswarm.org/the-book-of-swarm-2.pdf) on precise details on how to do
this. For the rest of us, both [bee-js](/docs/develop/tools-and-features/bee-js)
and [swarm-cli](/docs/bee/working-with-bee/swarm-cli) provide facilities
to achieve this using JavaScript and a node-js powered command line
tool respectively.

### No More ENS Transaction Charges

Swarm's feeds provide the ability to update your immutable content in a mutable world. Simply reference your feed's `manifest address` as the `content hash` in your ENS domain's resolver, and Bee will automatically provide the latest version of your website.

### Use Cases for Feeds

Feeds are a hugely versatile data structure. They allow you to host frequently updated content such as websites, RSS feeds (for podcasts, news, etc.), or even a DNS style architecture on top of Swarm's decentralized DISC.

---

## Gateway Proxy

The [Gateway Proxy tool](https://github.com/ethersphere/gateway-proxy) is more than just a proxy, it offers a range of useful functionalities for node operators and Swarm developers. For more in depth documentation of its features, refer to its [README doc](https://github.com/ethersphere/gateway-proxy/blob/master/README.md) on GitHub.

### Public Gateway

The tool can be used to set up a public gateway which can be used to host public facing applications or websites to users on the web who aren't running Bee nodes.

### Stamp Management

In addition to acting as a proxy, it also includes convenient features for managing stamps such as automatically buying new stamps or automatically extending the life of existing stamps.

### Security

Authentication can also be enabled so that only authorized requests are fulfilled. This is a useful feature for protecting your node if its API endpoint is publicly exposed.

---

## GSOC

## Introduction

The Graffiti Several Owner Chunk (GSOC) feature enables a single Bee *service* node to receive messages from multiple Bee *writer* nodes. It is based on a [Single Owner Chunk (SOC)](/docs/develop/tools-and-features/chunk-types/#single-owner-chunks) with an address which is derived so that it falls within the neighborhood of the service node, ensuring updates are automatically synced as part of the normal full node syncing process. 

The service node determines the data used to derive the GSOC private key. Any node with access to this data can derive the same private key and update the GSOC in order to send messages to the service node. Since only full nodes sync neighborhood chunks, the service node *must be a full node to receive GSOC updates*.  

To receive messages in real time, the service node establishes a WebSocket connection to listen for GSOC update events. When a matching SOC update reaches a node with an active GSOC connection, an event is emitted, enabling the service node to dynamically receive messages as part of a many-to-one notification system.  

:::info  
GSOC was initially introduced in a [SWIP](https://github.com/ethersphere/SWIPs/blob/99e6cf90a4768b24d27e5339b205c18825b53322/SWIPs/swip-draft_graffiti-soc.md#gsoc-identifier), which outlines its core concepts and implementation details, and it is an evolution of the earlier [Graffiti feed](https://github.com/fairDataSociety/FIPs/blob/master/text/0062-graffiti-feed.md) feature.  
:::  

## *bee-js* GSOC Methods

While you can interact with GSOC directly via the `/gsoc/subscribe/{address}` endpoint, the [bee-js](/docs/develop/tools-and-features/bee-js/) library is the recommended way for most users. The library includes three methods which make it easy to get started with GSOC:

### `Bee.gsocMine()`

The `Bee.gsocMine` method mines a GSOC private key corresponding to a specific overlay address: 

- The service node uses this method to generate the private key for a GSOC in its own neighborhood, and then uses it with the `gsocSubscribe` method to listen for updates from writer nodes. 
- A writer node uses this method to generate the private key it uses to send messages to the service node with the `gsocSend()` method. 

#### Parameters

- **`targetOverlay`** (`PeerAddress | Uint8Array | string`) ‚Äì The overlay address of the service node.
- **`identifier`** (`Identifier | Uint8Array | string`) ‚Äì A unique, arbitrary value that can be modified to mine a GSOC private key derived from a specific value.
- **`proximity`** (`number`, default: `16`) ‚Äì Determines the neighborhood depth, i.e., how many prefix bits match between `targetOverlay` and the mined GSOC overlay address.

The function returns a mined private key, which corresponds to a GSOC overlay address that falls within the `targetOverlay` neighborhood.

#### Functionality:

1. Mines and returns a private key that generates a GSOC overlay address within the specified `proximity` of `targetOverlay`.
2. The service node uses this method to mine a GSOC chunk whose overlay falls within its own neighborhood, and shares the values used as input with the writer node (`targetOverlay`, `identifier`, and `proximity`). 
3. The writer node uses this method with the input values shared from the service node to generate the private key that allows it to send messages as updates to the mined GSOC. 

This function allows users to derive a GSOC overlay address that aligns with a target node‚Äôs network neighborhood.

### `Bee.gsocSend()`

The `Bee.gsocSend` method is used by a writer node for sending GSOC messages. It creates an update for the GSOC using the provided `data` as the message, signs the update with the private key mined by the `gsocMine()` method, and uploads it to Swarm. 

#### Parameters:

- **`postageBatchId`** (`BatchId | Uint8Array | string`) ‚Äì The ID of the postage batch used to pay for the upload.
- **`signer`** (`PrivateKey | Uint8Array | string`) ‚Äì The private key used to sign the chunk.
- **`identifier`** (`Identifier | Uint8Array | string`) ‚Äì A unique identifier for the GSOC.
- **`data`** (`string | Uint8Array`) ‚Äì The payload to be sent.
- **`options`** (`UploadOptions`, optional) ‚Äì Additional upload configuration.
- **`requestOptions`** (`BeeRequestOptions`, optional) ‚Äì Custom request options.

#### Functionality:

1. Used by the writer node to send a GSOC message using the private key returned from `gsocMine()`.
2. Requires the `postageBatchId` for a valid postage stamp batch (ideally [mutable](/docs/develop/tools-and-features/gsoc#script-requirements)) to send messages.

### `Bee.gsocSubscribe()`

The `Bee.gsocSubscribe` method is used by the service node to establish a WebSocket connection to listen for GSOC messages. It subscribes to messages associated with a specific `address` and `identifier`. 

#### Parameters:

- **`address`** (`EthAddress | Uint8Array | string`) ‚Äì The Gnosis Chain address associated with the private key returned by the `gsocMine()` function.
- **`identifier`** (`Identifier | Uint8Array | string`) ‚Äì A unique identifier used to track the messages.
- **`handler`** (`GsocMessageHandler`) ‚Äì A callback function to handle incoming messages.

#### Functionality:

1. The function is used by the service node to construct a GSOC address using the provided `identifier` and `address`.
2. A WebSocket connection is opened to subscribe to update events for this GSOC address.
3. Incoming messages are processed by the `handler` function.
4. The function returns a `GsocSubscription` object with a `cancel` method to terminate the subscription.

## Example Scripts

The service node and writer node scripts below are a minimalistic example of how to use `bee-js` to set up a service node to listen for GSOC messages, and a writer node to send GSOC messages.

### Script Requirements

To run both nodes and send messages from the writer node to the service node you will need:

1. A fully synced Bee full node for the service node and a second Bee light node for the writer node (they do not both need to be running on the same machine) 
2. A small amount of xDAI (~0.01) and xBZZ (~0.01)
3. [NodeJS](https://nodejs.org/en) & [NPM](https://www.npmjs.com/)  
4. A mutable stamp batch (*set the* [`immutable` header parameter](/api/#tag/Postage-Stamps/paths/~1stamps~1%7Bamount%7D~1%7Bdepth%7D/post) *to `false` when* [buying a batch](/docs/develop/tools-and-features/buy-a-stamp-batch#buying-a-stamp-batch))  

:::warning  
Only ***mutable*** postage stamp batches should be used for GSOC.

Since each GSOC update utilizes one slot within the ***same*** [postage batch bucket](/docs/concepts/incentives/postage-stamps#batch-utilisation), immutable batches will fill up very quickly (e.g., at depth 18, four GSOC messages exhaust the batch).  

Mutable batches allow updates to overwrite older ones, preventing full utilization and enabling indefinite GSOC messaging as long as the batch still has remaining TTL.  
:::  

### Service Node Script

‚úÖ For your service node project, you must use a ***full node***.

‚ùå A service node does not need a postage stamp batch.

#### Initialize Project

First, initialize the service node project on a machine running a full Bee node in the background:

```bash
mkdir service-node
cd service-node
npm init -y && npm pkg set type="module" && cat package.json
npm install bee-js --save
```
The command first creates the `service-node` directory, moves into that directory, initializes a `package.json` file, sets `"type": "module"` in the file, and finally installs the `bee-js` library.

Next create a file named `index.js` which will hold the code for our service node.

```bash
touch index.js
```

Then open in your editor of choice:

```bash
vi index.js
```

Copy the completed code below for a service node into our newly created `index.js` file:

:::tip
Read through the code and code comments for a more in-depth understanding of how the service node works.
:::

```javascript
import { Bee, NULL_IDENTIFIER } from 'bee-js';

// Configuration
const BEE_HOST = 'http://localhost:1633'; // Change this if necessary
const BEE_PROXIMITY = 12 // Mining depth of the GSOC overlay - modified from the default of 16 for a shorter mining time

const BEE = new Bee(BEE_HOST, {});
async function mineGsocKey() {
    console.log('Fetching node addresses...');
    const addresses = await BEE.getNodeAddresses();
    const privateKey = BEE.gsocMine(addresses.overlay, NULL_IDENTIFIER, BEE_PROXIMITY); // `NULL_IDENTIFIER` is a constant `Uint8Array(32)` imported from `bee-js` for use as a default identifier
    console.log('Mining completed. Public Key:', privateKey.publicKey().toCompressedHex());
    return privateKey;
}

async function createGsocListener() {
    try {
        const privateKey = await mineGsocKey();
        // Subscribe to GSOC messages
        const subscription = BEE.gsocSubscribe(privateKey.publicKey().address(), NULL_IDENTIFIER, {
            onMessage: message => console.log('Received GSOC update:', message.toJSON()),
            onError: err => console.error('Error in subscription:', err),
        });

        console.log('Listening for GSOC updates...');

        return { privateKey, subscription };
    } catch (err) {
        console.error('Error:', err.message);
    }
}

(async () => {
    await createGsocListener();
})();
```

#### Update Configuration

Update the constants in the configuration section with your own information:

* Update `BEE_HOST` if your node is not using the default `http://localhost:1633`. 

#### Run Service Node Script

Start the service node:

```bash
node index.js
```

If everything is working correctly, after a few seconds you should see output like this:

```bash
Fetching node addresses...
Node overlay address:  75703155f54cbb899a359a7e3daec75da7722baef9286522e58e86ccbfcd7f13
Mining completed. Public Key: e82d2c98a92a3b0c690f6ba28070c59e3e0cd0a2a384d3b03cba9d1fded41a9831e73a3232d85b3614833d344c7d502dd09d7ecd0614b06095c86be0c8501460
Listening for GSOC updates...
```

This means the service node has successfully mined a GSOC chunk that it falls into its own neighborhood, and is now listening for updates on that chunk. Copy the `Node overlay address:` value (`75703155f54cbb899a359a7e3daec75da7722baef9286522e58e86ccbfcd7f13` from the example output) and save it - we will need it for our writer node's configuration.

### Writer Node Script
 
‚úÖ For your writer node, either a light or a full node can be used

‚úÖ A writer node needs a valid ***mutable*** (not technically required, but [strongly recommended](/docs/develop/tools-and-features/gsoc#script-requirements)) postage stamp batch in order to send GSOC messages 

#### Initialize Project

We initialize our writer node using almost the same command as our service node, only the directory name has been changed.

```bash
mkdir writer-node
cd writer-node
npm init -y && npm pkg set type="module" && cat package.json
npm install bee-js --save
```
The command first creates the `writer-node` directory, moves into that directory, initializes a `package.json` file, sets `"type": "module"` in the file, and finally installs the `bee-js` library.

Next create a file named `index.js` which will hold the code for our writer node.

```bash
touch index.js
```

Then open in your editor of choice:

```bash
vi index.js
```

Copy the completed code below for a writer node into our newly created `index.js` file:

:::tip
Read through the code and code comments for a more in-depth understanding of how the writer node works.
:::

```javascript
import { Bee, NULL_IDENTIFIER } from 'bee-js';

// Configuration
const BEE_HOST = 'http://localhost:1643'; // Change this if necessary
const BEE_BATCH = '42a10176596ecc73dcd24b91a16fb77d874ebd108fe8bc7fb896c8e89e8cb06e'; // Ensure this is a valid hex string
const TARGET_OVERLAY = '75703155f54cbb899a359a7e3daec75da7722baef9286522e58e86ccbfcd7f13'; // Overlay of service node writer node wants to message
const BEE_PROXIMITY = 12  // Mining depth of the GSOC overlay - modified from the default of 16 for a shorter mining time

const BEE = new Bee(BEE_HOST, {});

async function mineGsocKey() {
    const privateKey = BEE.gsocMine(TARGET_OVERLAY, NULL_IDENTIFIER, BEE_PROXIMITY); // `NULL_IDENTIFIER` is a constant `Uint8Array(32)` imported from `bee-js` for use as a default identifier
    console.log('Mining completed. Public Key:', privateKey.publicKey().toCompressedHex());
    return privateKey;
}

async function sendGsocMessage(privateKey, name, body) {
    if (!privateKey) {
        console.error('Error: Private key is not available');
        return;
    }

    if (!/^[0-9a-fA-F]{64}$/.test(BEE_BATCH)) {
        console.error('Error: Invalid BEE_BATCH. It must be a 64-character hex string.');
        return;
    }

    const message = JSON.stringify({ name, body });
    await BEE.gsocSend(BEE_BATCH, privateKey, NULL_IDENTIFIER, message); 
    console.log('Message sent:', message);
}

(async () => {
    const privateKey = await mineGsocKey();

    // Example: Sending a message after a delay (simulate user input)
    setTimeout(() => {
        sendGsocMessage(privateKey, 'Alice', 'Hello from Node.js!');
    }, 5000);
})();
```

#### Update Configuration

Update the configuration section constants with your own information:

* Set `BEE_HOST` to your writer node's API endpoint
* Set `BEE_BATCH` to the batch id of a valid, *mutable* postage stamp batch - [buy a batch](/docs/develop/tools-and-features/buy-a-stamp-batch) if needed
* Set `TARGET_OVERLAY` to the service node overlay value we copied from the output of the service node script 

After updating the configuration, run the writer node script (before running the writer node script, make sure the service node script has already been started and is currently listening for GSOC updates):

#### Run Writer Node Script

```bash
node index.js
```

If everything is working correctly, after a few moments on your writer node you should see output like this:

```bash
Mining completed. Public Key: e82d2c98a92a3b0c690f6ba28070c59e3e0cd0a2a384d3b03cba9d1fded41a9831e73a3232d85b3614833d344c7d502dd09d7ecd0614b06095c86be0c8501460
Message sent: {"name":"Alice","body":"Hello from Node.js!"}
```

While in the output from our service node, we should receive the update: 

```bash
Received GSOC update: { name: 'Alice', body: 'Hello from Node.js!' }
```

Congratulations! You've just sent your first GSOC message.

---

## Introduction(Tools-and-features)

# Hosting Your Dapps & Storing Their Data

Swarm is hugely versatile, but at a very basic level you can think of
it as storage for your dapps data that is too big for blockchain, but
still needs to live in our totally decentralised universe. Swarm is
perfect for storing your NFT meta-data and images in a web3 way that
won't break the bank and can live forever!

## Tools and Features

Swarm is designed with decentralised applications in mind, and much time has been devoted to designing tools and features to support their prototyping and development. 

### Bee JS

Our maverick JavaScript team, the Bee-Gees (üï∫), have been working hard in the last few months to build some impressive tools for all you budding dapp developer Bees to get stuck into! Find out how to use the [bee-js](/docs/develop/tools-and-features/bee-js) JavaScript library to start creating your own that live and work on Swarm!

### Chunk Types

Swarm contains 3 types of chunks which enable us to build novel
structures of how data can be stored in the swarm - in a completely
decentralised way. Learn more about
[chunk types](/docs/develop/tools-and-features/chunk-types)
to change the way you deal with data in your dapps forever!

### Feeds

Swarm's single owner chunks have been cleverly combined to create user
generated [feeds](/docs/develop/tools-and-features/feeds) in the swarm, see this
example of how chunks are combined into a useful data structure you
can use to build amazing applications.

### PSS

Hey there! Pss! ü§´ Swarm's trojan chunks are implemented in Bee to
deliver [Postal Service on Swarm](/docs/develop/tools-and-features/pss) - a
pub-sub system that provides a totally leak-proof messaging system
over the swarm.

### Gateway Proxy

If you want your users to be able to access Swarm without running
their own Bee node, for the time being you will need to make use of the [Gateway Proxy tool](https://github.com/ethersphere/gateway-proxy). Join us in the
[#develop-on-swarm](https://discord.gg/C6dgqpxZkU) room in our
[Discord Server](https://discord.gg/wdghaQsGq5) for more information on how to make your Swarm based applications accessible to everyone. 

### Bee Dev Mode

If you want to test out Swarm based applications without needing to spend real xBZZ, Bee dev mode is an invaluable tool. Learn how to set up Bee in [dev mode](/docs/develop/tools-and-features/bee-dev-mode) to begin prototyping your applications.

### Starting a Test Network

While bee dev mode allows you to simulate running a single Bee node, setting up a [test network](/docs/develop/tools-and-features/starting-a-test-network) will allow you to better simulate interactions between multiple nodes.

---

## Pinning

Each Bee node is configured to reserve a certain amount of memory on your computer's hard drive to store and serve chunks within their _neighborhood of responsibility_ for other nodes in the Swarm network. Once this alloted space has been filled, each Bee node deletes older chunks to make way for newer ones as they are uploaded by the network.

Each time a chunk is accessed, it is moved back to the end of the deletion queue, so that regularly accessed content stays alive in the network and is not deleted by a node's garbage collection routine.

Bee nodes provide a facility to **pin** important content so that it is not deleted by the node's garbage collection routine. Chunks can be _pinned_ either during upload, or retrospectively using the Swarm reference.

## Pin During Upload

To store content so that it will persist even when Bee's garbage collection routine is deleting old chunks, we simply pass the `Swarm-Pin` header set to `true` when uploading.

```bash
curl -H "Swarm-Pin: true" -H "Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264"  --data-binary @bee.mp4 localhost:1633/bzz\?bee.mp4
```

```json
{
  "reference": "1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30"
}
```

## Administer Pinned Content

To check what content is currently pinned on your node, query the `pins` endpoint of your Bee API:

```bash
curl localhost:1633/pins
```

```json
{
  "references": [
    "1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30"
  ]
}
```

or, to check for specific references:

```bash
curl localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30
```

A `404` response indicates the content is not available.

### Unpinning Content

We can unpin content by sending a `DELETE` request to the pinning endpoint using the same reference:

````bash
curl -XDELETE http://localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30
``

```json
{"message":"OK","code":200}
````

Now, when check again, we will get a `404` error as the content is no longer pinned.

```bash
curl localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30
```

```json
{ "message": "Not Found", "code": 404 }
```

:::info
Pinning and unpinning is possible for files (as in the example) and also the chunks, directories, and bytes endpoints. See the [API](/api/) documentation for more details.
:::

### Pinning Already Uploaded Content

The previous example showed how we can pin content upon upload. It is also possible to pin content that is already uploaded and present in the Swarm.

To do so, we can send a `POST` request including the swarm reference to the files pinning endpoint.

```bash
curl -X POST http://localhost:1633/pins/7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f
```

```json
{ "message": "OK", "code": 200 }
```

The `pins` operation will attempt to fetch the content from the network if it is not available on the local node.

Now, if we query our files pinning endpoint again, the swarm reference will be returned.

```bash
curl http://localhost:1633/pins/7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f
```

```json
{
  "reference": "7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f"
}
```

:::warning
While the pin operation will attempt to fetch content from the network if it is not available locally, we advise you to ensure that the content is available locally before calling the pin operation. If the content, for whatever reason, is only fetched partially from the network, the pin operation only partly succeeds and leaves the internal administration of pinning in an inconsistent state.
:::

---

## PSS Messaging

Out of the ashes of Ethereum's vision for a leak-proof decentralised anonymous messaging system - Whisper - comes PSS (or BZZ, whispered! ü§´). Swarm provides the ability to send messages that appear to be normal Swarm traffic, but are in fact messages that may be received and decrypted to reveal their content only by the specific nodes they were intended to be received by.

PSS provides a pub-sub facility that can be used for a variety of tasks. Nodes are able to listen to messages received for a specific topic in their nearest neighborhood and create messages destined for another neighborhood which are sent over the network using Swarm's usual data dissemination protocols.

### Subscribe and Receive Messages

Once your Bee node is up and running, you will be able to subscribe to feeds using WebSockets. For testing, it is useful to use the [websocat](https://docs.rs/crate/websocat/1.0.1) command line utility.

Here we subscribe to the topic `test-topic`

```bash
websocat ws://localhost:1633/pss/subscribe/test-topic
```

Our node is now watching for new messages received in its nearest neighborhood.

:::info
Because a message is disguised as a normal chunk in Swarm, you will receive the message upon syncing the chunk, even if your node is not online at the moment when the message was send to you.
:::

### Send Messages

Messages can be sent simply by sending a `POST` request to the PSS API endpoint.

When sending messages, we must specify a 'target' prefix of the
recipient's Swarm address, a partial address representing their
neighborhood. Currently the length of this prefix is recommended to
be two bytes, which will work well until the network has grown to a
size of ca. 20-50K nodes. We must also provide the public key, so that
Bee can encrypt the message in such a way that it may only be read by
the intended recipient.

For example, if we want to send a PSS message with **topic** `test-topic` to a node with address...

`7bc50a5d79cb69fa5a0df519c6cc7b420034faaa61c175b88fc4c683f7c79d96`

...and public key...

`0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd`

...we must include the **target** `7bc5` and the public key itself as a query argument.

```bash
curl  -H "Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264" -X POST \
localhost:1833/pss/send/test-topic/7bc5?recipient=0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd \
--data "Hello Swarm"
```

More information on how to buy a postage stamp batch and get its batch id can be found [here](/docs/develop/tools-and-features/buy-a-stamp-batch).

### Send Messages in a Test Network

Now, let's see this in action by setting up two Bee nodes on a test network, connecting them, and sending PSS messages from one to the other.

First start two Bee nodes. We will start them with distinct ports for
the API and p2p ports, since they will be running on the
same computer.

Run the following command to start the first node. Note that we are passing `""` to the `--bootnode` argument so that our nodes will not connect to a network.

```bash
bee start \
    --api-addr=:1833 \
    --data-dir=/tmp/bee2 \
    --bootnode="" \
    --p2p-addr=:1834 \
    --blockchain-rpc-endpoint=http://localhost:8545
```

We must make a note of the Swarm overlay address, underlay address and public key which are created once each node has started. We find this information from the `/addresses` endpoint of the API.

```bash
curl -s localhost:1833/addresses | jq
```

```json
{
  "overlay": "46275b02b644a81c8776e2459531be2b2f34a94d47947feb03bc1e209678176c",
  "underlay": [
    "/ip4/127.0.0.1/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq",
    "/ip4/192.168.0.10/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq",
    "/ip6/::1/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq"
  ],
  "ethereum": "0x0b546f2817d0d889bd70e244c1227f331f2edf74",
  "public_key": "03660e8dbcf3fda791e8e2e50bce658a96d766e68eb6caa00ce2bb87c1937f02a5"
}
```

Now the same for the second node.

```bash
bee start \
    --api-addr=:1933 \
    --data-dir=/tmp/bee3 \
    --bootnode="" \
    --p2p-addr=:1934 \
    --blockchain-rpc-endpoint=http://localhost:8545
```

```bash
curl -s localhost:1935/addresses | jq
```

```json
{
  "overlay": "085b5cf15a08f59b9d64e8ce3722a95b2c150bb6a2cef4ac8b612ee8b7872253",
  "underlay": [
    "/ip4/127.0.0.1/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr",
    "/ip4/192.168.0.10/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr",
    "/ip6/::1/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr"
  ],
  "ethereum": "0x9ec47bd86a82276fba57f3009c2f6b3ace4286bf",
  "public_key": "0289634662d3ed7c9fb1d7d2a3690b69b4075cf138b683380023d2edc2e6847826"
}
```

Because we configured the nodes to start with no bootnodes, neither node should have peers yet.

```bash
curl -s localhost:1833/peers | jq
```

```bash
curl -s localhost:1935/peers | jq
```

```json
{
  "peers": []
}
```

Let's connect node 2 to node 1 using the localhost (127.0.0.1) underlay address for node 1 that we have noted earlier.

```bash
curl -X POST \
  localhost:1935/connect/ip4/127.0.0.1/tcp/1834/p2p/16Uiu2HAmP9i7VoEcaGtHiyB6v7HieoiB9v7GFVZcL2VkSRnFwCHr
```

Now, if we check our peers endpoint for node 1, we can see our nodes are now peered together.

```bash
curl -s localhost:1833/peers | jq
```

```json
{
  "peers": [
    {
      "address": "a231764383d7c46c60a6571905e72021a90d506ef8db06750f8a708d93fe706e"
    }
  ]
}
```

Of course, since we are p2p, node 2 will show node 1 as a peer too.

```bash
curl -s localhost:1935/peers | jq
```

```json
{
  "peers": [
    {
      "address": "7bc50a5d79cb69fa5a0df519c6cc7b420034faaa61c175b88fc4c683f7c79d96"
    }
  ]
}
```

We will use `websocat` to listen for the PSS messages' Topic ID
`test-topic` on our first node.

```bash
websocat ws://localhost:1833/pss/subscribe/test-topic
```

Now we can use PSS to send a message from our second node to our first node.

Since our first node has a 2 byte address prefix of `a231`, we will specify this as the `targets` section in our POST request's URL. We must also include the public key of the recipient as a query parameter so that the message can be encrypted in a way only our recipient can decrypt.

```bash
curl \
  -H "Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264"
  -X POST "localhost:1933/pss/send/test-topic/7bc5?recipient=0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd" \
  --data "Hello Swarm"
```

The PSS API endpoint will now create a PSS message for its recipient
in the form of a 'Trojan Chunk' and send this into the network so that
it may be pushed to the correct neighborhood. Once it is received by
its recipient it will be decrypted and determined to be a message with
the topic we are listening for. Our second node will decrypt the data
and we'll see a message pop up in our `websocat` console!

```bash
websocat ws://localhost:1833/pss/subscribe/test-topic
```

```
Hello Swarm
```

Congratulations! üéâ You have sent your first encrypted, zero leak message over Swarm!

---

## Starting a Private Network

A private network can be used to test your applications in an isolated environment before you deploy to Swarm mainnet. It can be started by overriding the default configuration values of your Swarm node. Throughout this tutorial, we will make use of configuration files to configure the nodes but of course you can also do the same using flags or environment variables (see [Start your node](/docs/bee/working-with-bee/configuration)).

## Start a network on your own computer

### Configuration

Starting a network is easiest achieved by making use of configuration files. We need at least two nodes to start a network. Hence, below two configuration files are provided. Save them respectively as `config_1.yaml` and `config_2.yaml`.

**config_1.yaml**

```yaml
network-id: 7357
api-addr: 127.0.0.1:1633
p2p-addr: :1634
bootnode: ""
data-dir: /tmp/bee/node1
password: set-a-strong-password
swap-enable: false
mainnet: false
blockchain-rpc-endpoint: https://sepolia.dev.fairdatasociety.org
verbosity: 5
full-node: true 
```

**config_2.yaml**

```yaml
network-id: 7357
api-addr: 127.0.0.1::1733
p2p-addr: :1734
data-dir: /tmp/bee/node2
bootnode: ""
password: set-a-strong-password
welcome-message: "Bzz Bzz Bzz"
swap-enable: false
mainnet: false
blockchain-rpc-endpoint: https://sepolia.dev.fairdatasociety.org
verbosity: 5
full-node: true 
```

Note that for each node, we provide a different `api-addr`. If we had not specified different addresses here, we
would get an `address already in use` error, as no two applications
can listen to the same port. We also specify a different
`p2p-addr`. If we had not, our nodes would not be able to communicate
with each other. We also specify a separate `data-dir` for each node,
as each node must have its own separate key and chunk data store.

We also provide a network-id, so that our network remains separate
from the Swarm mainnet, which has network-id 1. Nodes will not connect
to peers which have a different network id. We also set our bootnode
to be the empty string `""`. A bootnode is responsible for
bootstrapping the network so that a new node can find its first few
peers before it begins its own journey to find friends in the
Swarm. In Swarm any node can be used as a bootnode. Later, we will
use our first node as the bootnode for our other node(s), but for now we leave this option blank.

We have set `mainnet` to false so that our node runs on the Sepolia testnet, and we provide an RPC endpoint for Sepolia in the `blockchain-rpc-endpoint` option. We have also set `full-node` and `swap-enable` to `true` so that we can run full nodes.

Log verbosity has been set to level 5 with the `verbosity` option. By setting it at the highest level of 5, we make sure all important information is shown in our logs. Setting this is optional.

Finally, note the `welcome-message` in the first nodes configuration file. This is a friendly feature allowing you to send a message to peers that connect to you!

### Starting Your Nodes

Now we have created our configuration files, let's start our nodes by running `bee start --config config_1.yaml`, then in another Terminal session, run `bee start --config config_2.yaml`.

We can now inspect the state of our network by sending HTTP requests to the [API](/api/).

```bash
curl -s http://localhost:1633/topology | jq .connected
```

```
0
```

```bash
curl -s http://localhost:1733/topology | jq .connected
```

```
0
```

No connections yet? Right! Let's remedy that!

:::info
Here we are using the `jq` command line utility to count the amount of objects in the `peers` array in the JSON response we have received from our API, learn more about how to install and use `jq` [here](https://stedolan.github.io/jq/).
:::

### Making a network

In order to create a network from our two isolated nodes, we must first instruct our nodes to connect to each other. This step is not explicitly needed if you connect to the main Swarm network, as the default bootnodes in the Swarm network will automatically suggest peers.

First, we will need to find out the network address of the first node. To do this, we send a HTTP request to the `addresses` endpoint of the API.

```bash
curl localhost:1633/addresses | jq
```

```json
{
  "overlay": "b1978be389998e8c8596ef3c3a54214e2d4db764898ec17ec1ad5f19cdf7cc59",
  "underlay": [
    "/ip4/127.0.0.1/tcp/1634/p2p/QmQHgcpizgoybDtrQXCWRSGdTP526ufeMFn1PyeGd1zMEZ",
    "/ip4/172.25.128.69/tcp/1634/p2p/QmQHgcpizgoybDtrQXCWRSGdTP526ufeMFn1PyeGd1zMEZ",
    "/ip6/::1/tcp/1634/p2p/QmQHgcpizgoybDtrQXCWRSGdTP526ufeMFn1PyeGd1zMEZ"
  ],
  "ethereum": "0xd22cc790e2aef341827e1e49cc631d2a16898cd9",
  "publicKey": "023b26ce8b78ed8cdb07f3af3d284c95bee5e038e7c5d0c397b8a5e33424f5d790",
  "pssPublicKey": "039ceb9c1f0afedf79991d86d89ccf4e96511cf656b43971dc3e878173f7462487"
}
```

Here, we get firstly the **overlay address** - this is the permanent address Swarm uses as your anonymous identity in the network and secondly, a list of all the [multiaddresses](https://docs.libp2p.io/reference/glossary/#multiaddr), which are physical network addresses at which you node can be found by peers.

Note the addresses starting with an `/ip4`, followed by `127.0.0.1`, which is the `localhost` internal network in your computer. Now we can use this full address to be the bootnode of our second node so that when it starts up, it goes to this address and both nodes become peers of each other. Let's add this into our config_2.yaml file.

**config_2.yaml**

```yaml
network-id: 7357
api-addr: 127.0.0.1::1733
p2p-addr: :1734
data-dir: /tmp/bee/node2
bootnode: "/ip4/127.0.0.1/tcp/1634/p2p/QmQHgcpizgoybDtrQXCWRSGdTP526ufeMFn1PyeGd1zMEZ"
password: set-a-strong-password
welcome-message: "Bzz Bzz Bzz"
swap-enable: false
blockchain-rpc-endpoint: https://sepolia.dev.fairdatasociety.org
verbosity: 5
full-node: true 
```

Now, we can shut our second node and reboot with the new configuration.

Look at the the output for your first node, you should see our connection message!

Let's also verify that we can see both nodes in using each other's API's.

```bash
curl -s http://localhost:1633/peers | jq
```

```bash
curl -s http://localhost:1733/peers | jq
```

Congratulations! You have made your own tiny two bee Swarm! üêù üêù

## Funding Nodes

While you have successfully set up two nodes, they are currently unfunded with either sETH or sBZZ. Sepolia ETH (sETH) is required for issuing transactions on the Sepolia testnet, and Sepolia BZZ (sBZZ) is required for your node to operate as a full staking node. 

To fund our nodes, we need to first collect the blockchain addresses for each node. We can use the `/addresses` endpoint for this:

```bash
curl localhost:1633/addresses | jq
```

```bash
{
  "overlay": "b1978be389998e8c8596ef3c3a54214e2d4db764898ec17ec1ad5f19cdf7cc59",
  "underlay": [
    "/ip4/127.0.0.1/tcp/1634/p2p/QmQHgcpizgoybDtrQXCWRSGdTP526ufeMFn1PyeGd1zMEZ",
    "/ip4/172.25.128.69/tcp/1634/p2p/QmQHgcpizgoybDtrQXCWRSGdTP526ufeMFn1PyeGd1zMEZ",
    "/ip6/::1/tcp/1634/p2p/QmQHgcpizgoybDtrQXCWRSGdTP526ufeMFn1PyeGd1zMEZ"
  ],
  "ethereum": "0xd22cc790e2aef341827e1e49cc631d2a16898cd9",
  "publicKey": "023b26ce8b78ed8cdb07f3af3d284c95bee5e038e7c5d0c397b8a5e33424f5d790",
  "pssPublicKey": "039ceb9c1f0afedf79991d86d89ccf4e96511cf656b43971dc3e878173f7462487"
}
```

Then copy the address in the "ethereum" field. This is the address you need to send sETH and sBZZ to. 

You will need to send only a very small amount of sETH such as 0.01 sETH, to get started. You will need 10 sBZZ to run a full node with staking.

After sending sETH and sBZZ to your node's address which you copied above, restart your node and it should begin operating properly as a full node. 

Repeat these same steps with the other node in order to complete a private test network of two full nodes.

### Getting Testnet Tokens

In order to acquire sETH and sBZZ, refer to the [Fund Your Node](/docs/bee/installation/fund-your-node) page.

---

## Store with Encryption

In Swarm, all data is _public_ by default. To protect sensitive content, it must be encrypted so that only authorised users are able to decrypt and then view the plaintext content.

The Bee client provides a facility to encrypt files and directories while uploading which are only able to be read by users with access to the corresponding decryption key.

# Encrypt and Upload a File

To encrypt a file simply include the `Swarm-Encrypt: true` header with your HTTP request.

```bash
curl -F file=@bee.jpg -H "Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264" -H "Swarm-Encrypt: true" http://localhost:1633/bzz
```

When successful, the Bee client will return a 64 byte reference, instead of the usual 32 bytes.

More information on how to buy a postage stamp batch and get its batch id can be found [here](/docs/develop/tools-and-features/buy-a-stamp-batch).

```json
{
  "reference": "f7b1a45b70ee91d3dbfd98a2a692387f24db7279a9c96c447409e9205cf265baef29bf6aa294264762e33f6a18318562c86383dd8bfea2cec14fae08a8039bf3"
}
```

Here we see that, when using the Bee node's encryption function, the reference hash that is returned is 128 hex characters long. The first 64 characters of this is the familiar Swarm address - the reference that allows us to retrieve the data from the swarm. This is the same reference we would get uploading unencrypted files using Bee, so it is safe to share.

The second part of the reference is a 64 character decryption key which is required to decrypt the referenced content and view the original data in the clear. This is sensitive key material and must be kept private.

It is important that this data not be sent in requests to a public gateway as this would mean that gateway would be able to decrypt your data. However, if you are running a node on your local machine, you may safely use the API bound to `localhost`. The key material is never exposed to the network so your data remains safe.

:::info
Encryption is disabled by default on all Swarm Gateways to keep your data safe. [Install Bee on your computer](/docs/bee/installation/getting-started) to use the encryption feature.
:::

# Download and Decrypt a File

To retrieve your file, simply supply the full 64 byte string to the files endpoint, and the Bee client will download and decrypt all the relevant chunks and restore them to their original format.

```bash
curl -OJ http://localhost:1633/bzz/f7b1a45b70ee91d3dbfd98a2a692387f24db7279a9c96c447409e9205cf265baef29bf6aa294264762e33f6a18318562c86383dd8bfea2cec14fae08a8039bf3
```

:::danger
Never use public gateways when requesting full encrypted references. The hash contains sensitive key information which should be kept private. Run [your own node](/docs/bee/installation/getting-started) to use Bee's encryption features.
:::

---

## Ultra Light Nodes

:::danger
When running without a blockchain connections, bandwidth incentive payments (SWAP) cannot be made so there is a risk of getting blocklisted by other peers for unpaid services.
:::

#### Configuration

To run Bee as an ultra-light node `full-node` and `swap-enable` must both be set to `false`, and the `blockchain-rpc-endpoint` value should be set to an empty string `""` or commented out in the [configuration](/docs/bee/working-with-bee/configuration).

#### Mode of Operation

The target audience for this mode of operations are users who want to try out running a node but don't
want to go through the hassle of blockchain onboarding. Ultra-light nodes will be able to download data as long as the data consumed does not exceed the payment threshold (`payment-threshold` in [configuration](/docs/bee/working-with-bee/configuration)) set by peers they connect to.

Running Bee without a connected blockchain backend, however, imposes some limitations:

- Can't do overlay verification
- Can't do SWAP settlements

Since we can't buy postage stamps:

- Can't send PSS messages
- Can't upload data to the network

---

## Upload & Download

Uploading to Swarm has two steps: (1) **buy storage** as a **postage stamp batch** with a unique **batch ID**‚Äîand (2) **upload using the batch ID**. The upload returns a **Swarm reference hash**, anyone with that reference can download the content.

**Before you begin:**

- You need a running Bee node connected to Gnosis Chain and funded with **xBZZ** and **xDAI**.
- Uploads always require a **postage stamp batch**.
- Ultra-light nodes can download but **cannot upload**.

## Upload & Download with bee-js

The `bee-js` library is the **official SDK for building Swarm-based applications**. It works in both **browser** and **Node.js** environments and **greatly simplifies development** compared with using the Bee HTTP API directly. It is the recommended method for developing applications on Swarm.

Refer to the [`bee-js` documentation](https://bee-js.ethswarm.org/) for more usage guides.

:::tip
**Environment-specific methods:**

- **Browser-only:** [`uploadFiles`](https://bee-js.ethswarm.org/docs/api/classes/Bee/#uploadfiles) (multi-file via `File[]`/`FileList`)
- **Node.js-only:** [`uploadFilesFromDirectory`](https://bee-js.ethswarm.org/docs/api/classes/Bee/#uploadfiles) (recursively reads local filesystem to upload multiple files in a directory using `fs`),
- **Both:** [`uploadFile`](https://bee-js.ethswarm.org/docs/api/classes/Bee/#uploadfile) (with some environment specific usage), [`downloadFile`](https://bee-js.ethswarm.org/docs/api/classes/Bee/#downloadfile)
  :::

### Single file ‚Äî Node.js

**Step-by-step Walkthrough:**

1. Create a Bee client:

   `const bee = new Bee("http://localhost:1633")`

2. Buy storage (postage stamp batch) by specifying storage size and duration:

   `const batchId = await bee.buyStorage(Size.fromGigabytes(1), Duration.fromDays(1))`

3. Read the file from disk:

   `const data = await readFile("./hello.txt")`

4. Upload bytes with filename & content type ‚Üí get reference:

   `const { reference } = await bee.uploadFile(batchId, data, "hello.txt", { contentType: "text/plain" })`

5. Download the file by reference:

   `const file = await bee.downloadFile(reference)`

6. Log the downloaded file‚Äôs title and metadata:

   `console.log(file.name)`

   `console.log(file.contentType)`

   `console.log(file.data.toUtf8())`

**Full example:**

```js
import { Bee, Size, Duration } from "@ethersphere/bee-js";
import { readFile } from "node:fs/promises";

// 1) Connect to your Bee node HTTP API
const bee = new Bee("http://localhost:1633");

// 2) Buy storage (postage stamp batch) for this session
const batchId = await bee.buyStorage(
  Size.fromGigabytes(1),
  Duration.fromDays(1)
);

// 3) Read the file from disk as bytes
const data = await readFile("./hello.txt");

// 4) Upload the bytes with a filename and content type; capture the reference
const { reference } = await bee.uploadFile(batchId, data, "hello.txt", {
  contentType: "text/plain",
});
console.log("Uploaded reference:", reference.toHex());

// 5) Download the file back using the reference
const file = await bee.downloadFile(reference);

// 6) Log the file's metadata and contents to the terminal
console.log(file.name); // "hello.txt"
console.log(file.contentType); // "text/plain"
console.log(file.data.toUtf8()); // Prints file content
```

### Single file ‚Äî Browser

:::info
When working with browsers you can use the [`File` interface](https://developer.mozilla.org/en-US/docs/Web/API/File). The filename is taken from the `File` object itself, but can be overwritten through the second argument of the `uploadFile` function.
:::

**Walkthrough**

1. Initialize a Bee object using the API endpoint of a Bee node:

   `const bee = new Bee("http://localhost:1633")`

2. Buy storage and get postage stamp batch ID:

   `const batchId = await bee.buyStorage(Size.fromGigabytes(1), Duration.fromDays(1))`

3. Create a `File` object:

`const file = new File(["Hello Swarm!"], "hello.txt", { type: "text/plain" })`

4. Use batch ID to upload ‚Üí get reference:

   `const { reference } = await bee.uploadFile(batchId, file)`

5. Download by reference:

   `const downloaded = await bee.downloadFile(reference)`

6. Log the downloaded file‚Äôs title and metadata:

   `console.log(downloaded.name)   // "hello.txt"`

   `console.log(file.contentType) // "text/plain"`

   `console.log(downloaded.data.toUtf8()) // prints file content`

```js
import { Bee, Size, Duration } from "@ethersphere/bee-js";

// 1) Connect to your Bee node HTTP API
const bee = new Bee("http://localhost:1633");

// 2) Buy storage (postage stamp batch) for this session
const batchId = await bee.buyStorage(
  Size.fromGigabytes(1),
  Duration.fromDays(1)
);
console.log("Batch ID:", String(batchId));

// 3) Upload a single file created in code
const file = new File(["Hello Swarm!"], "hello.txt", { type: "text/plain" });
const { reference } = await bee.uploadFile(batchId, file);
console.log("Reference:", String(reference));

// 4) Download and print name + contents
const downloaded = await bee.downloadFile(reference);
console.log(downloaded.name); // "hello.txt"
console.log(file.contentType); // "text/plain"
console.log(downloaded.data.toUtf8()); // prints file content
```

### Multiple files ‚Äî Browser

Use **`uploadFiles`** for multi-file upload in the browser. It accepts `File[]`/`FileList`. When using `<input type="file" webkitdirectory multiple>`, each file‚Äôs **relative path** is preserved. To download a specific file later, pass the **collection reference** plus the **same relative path**.

1. Initialize a Bee object using the API endpoint of a Bee node:
   `const bee = new Bee("http://localhost:1633")`

2. Buy storage and get postage stamp batch ID:
   `const batchId = await bee.buyStorage(Size.fromGigabytes(1), Duration.fromDays(1))`

3. Create files for upload:

```js
const files = [
  new File(["Hello Swarm"], "index.html", { type: "text/html" }),
  new File(["body{font-family:sans-serif}"], "assets/main.css", {
    type: "text/css",
  }),
];
```

4. Upload multiple files (collection) ‚Üí get collection reference:
   `const res = await bee.uploadFiles(batchId, files)`

5. Download files by relative paths:
   `const logo = await bee.downloadFile(res.reference, "images/logo.png")`

6. Log the downloaded file‚Äôs title and contents:

   `console.log(page.name)          // "index.html"`

   `console.log(page.data.toUtf8()) // prints file content`

```js
import { Bee, Size, Duration } from "@ethersphere/bee-js";

// 1. Initialize a Bee object
const bee = new Bee("http://localhost:1633");

// 2. Buy storage and get batch ID
const batchId = await bee.buyStorage(
  Size.fromGigabytes(1),
  Duration.fromDays(1)
);
console.log("Batch ID:", String(batchId));

// 3. Create files for upload
const files = [
  new File(["Hello Swarm"], "index.html", { type: "text/html" }),
  new File(["body{font-family:sans-serif}"], "assets/main.css", {
    type: "text/css",
  }),
];

//  4. Upload multiple files (collection) ‚Üí get collection reference
const res = await bee.uploadFiles(batchId, files);
console.log("Collection ref:", String(res.reference));

// 5. Download files by relative path
const page = await bee.downloadFile(res.reference, "index.html");
console.log(page.name); // "index.html"
console.log(page.data.toUtf8()); // prints file content

const style = await bee.downloadFile(res.reference, "assets/main.css");
console.log(style.name); // "main.css"
console.log(style.data.toUtf8()); // prints file content
```

### Multiple files ‚Äî Node.js

**Step-by-step Walkthrough:**

1. Initialize a Bee object using the API endpoint of a Bee node:

   `const bee = new Bee("http://localhost:1633")`

2. Buy storage and get postage stamp batch ID:

   `const batchId = await bee.buyStorage(Size.fromGigabytes(1), Duration.fromDays(1))`

3. Recursively upload a local directory ‚Üí get collection reference:

   `const res = await bee.uploadFilesFromDirectory(batchId, "./site")`

4. Download one file by its relative path:

   `const page = await bee.downloadFile(res.reference, "index.html")`

5. Log the downloaded file name and contents:

   `console.log(page.name ?? "index.html")`

   `console.log(page.data.toUtf8())`

**Full example:**

```js
import { Bee, Size, Duration } from "@ethersphere/bee-js";

// 1) Connect to your Bee node HTTP API
const bee = new Bee("http://localhost:1633");

// 2) Buy storage (postage stamp batch)
const batchId = await bee.buyStorage(
  Size.fromGigabytes(1),
  Duration.fromDays(1)
);

// 3) Upload all files under ./files (relative paths preserved); get reference
const res = await bee.uploadFilesFromDirectory(batchId, "./files");
console.log("Directory uploaded. Collection reference:", res.reference.toHex());

// 4) Download files from the collection by original relative paths
const page = await bee.downloadFile(res.reference, "root.txt");
const stylesheet = await bee.downloadFile(
  res.reference,
  "subdirectory/example.txt"
);

// 5) Log the file name and contents to the terminal
console.log(page.name); // "root.txt"
console.log(page.data.toUtf8()); // prints file content

console.log(stylesheet.name); // "example.txt"
console.log(stylesheet.data.toUtf8()); // prints file content
```

## Upload & Download with Swarm CLI

The `swarm-cli` tool offers a convenient command-line interface for Bee node interaction. It's a convenient tool for node management or one-off uploads and downloads.

Refer to [the official README](https://github.com/ethersphere/swarm-cli/blob/master/README.md) for a more complete usage guide.

Buy storage via an interactive prompt (capacity + TTL), then upload:

```bash
swarm-cli stamp create
```

Follow the interactive prompts:

```bash
For swarm cli, use "stamp create", which looks like this:

PS C:\Users\noahm> swarm-cli stamp create
Please provide the total capacity of the postage stamp batch
This represents the total size of data that can be uploaded
Example: 1GB

Please provide the time-to-live (TTL) of the postage stamps
Defines the duration after which the stamp will expire
Example: 1d, 1w, 1month

You have provided the following parameters:
Capacity: 1.074 GB
TTL: 7 days

Cost: 0.6088166475825152 xBZZ
Available: 10000.0000000000000000 xBZZ
Type: Immutable
? Confirm the purchase Yes
... Creating postage batch. This may take up to 5 minutes.
```

Once you have a valid stamp batch, you can find it using `swarm-cli stamp list`

```bash
swarm-cli stamp list
```

```bash
Stamp ID: 6dd0c4bbb6d62ba6c5fae3b000301c961ee584dd32846291821d789d7582ae36
Usage: 0%
Capacity (immutable): 2.380 GB remaining out of 2.380 GB
TTL: A few seconds (2025-09-21)
------------------------------------------------------------------------------------------------------------------------
Stamp ID: d13210952ec60b01a3c0027602743921736d6b277e9e70dd00d0d95fd878acbc
Usage: 0%
Capacity (immutable): 2.380 GB remaining out of 2.380 GB
TTL: A few seconds (2025-09-21)
```

Use `swarm-cli upload` along with a valid batch ID to upload a file:

```bash
swarm-cli upload test.txt --stamp <BATCH_ID>
```

You can also simply use:

```bash
swarm-cli upload <PATH_TO_FILE>
```

And an interactive prompt will walk your through stamp selection and the rest of the upload.

Upon upload, a Swarm reference hash will be returned which can then be used to download content:

```bash
swarm-cli download <REFERENCE> ./output/
```

## Upload & Download with the Bee API (advanced)

The **Bee HTTP API** offers the **lowest-level access** to a Bee node. However, it is **more complex and difficult to use** than **bee-js** or **swarm-cli** because you must manage headers, content types, and postage parameters yourself. **Unless you specifically require raw HTTP control**, we **do not recommend** using the Bee API directly. Instead use **bee-js** for application development and **swarm-cli** for command-line interaction.

Refer to the [Bee API reference specification](https://docs.ethswarm.org/api/) for detailed usage information.

The Bee API exposes three HTTP endpoints:

- **`/bzz`** ‚Äî upload & download files/directories (most common)
- **`/bytes`** ‚Äî upload & download raw data
- **`/chunks`** ‚Äî upload & download individual chunks

#### Upload with **/bzz**

While both `swarm-cli` and `bee-js` allow for postage stamp batches to be purchased by specifying the storage duration and data size, the actual call to the Bee API requires an `amount` and `depth` parameters. The relationship between these parameters and the storage size and duration of the batch is complex. Therefore `bee-js` and `swarm-cli` (which allow batches to be purchased by data size/duration which are then converted to `depth`/`amount`) are strongly encouraged for newcomers to development on Swarm. [Learn more](/docs/develop/tools-and-features/buy-a-stamp-batch).

1. Buy a postage batch:

```bash
curl -s -X POST http://localhost:1633/stamps/<amount>/<depth>
```

2. Upload a file with the returned `batchID`:

```bash
curl -X POST \
  -H "Swarm-Postage-Batch-Id: <BATCH_ID>" \
  -H "Content-Type: text/plain" \
  --data-binary "@test.txt" \
  http://localhost:1633/bzz
```

Response:

```json
{
  "reference": "22cbb9cedca08ca8d50b0319a32016174ceb8fbaa452ca5f0a77b804109baa00"
}
```

3. Download with `/bzz`

```bash
curl http://localhost:1633/bzz/<REFERENCE> -o output.txt
```

---

## Awesome Swarm

*Contribute to the Awesome Swarm list on [GitHub](https://github.com/ethersphere/awesome-swarm).*

[Swarm](https://www.ethswarm.org/) is an incentivized peer-to-peer storage and communication system. [Join the decentralized network with a Bee node](https://docs.ethswarm.org/docs/installation/quick-start), the basic building block of Swarm.

This is a list of free and open source projects related to Swarm and its growing ecosystem.

See [CONTRIBUTING.md](https://github.com/ethersphere/awesome-swarm/blob/master/CONTRIBUTING.md) to learn how to create your submission.

## Table of contents

Click on the menu icon next to [README.md](#readme) for a list of sections

## Projects

### Services

**[`^ back to top ^`](#)**

[Bee](https://github.com/ethersphere/bee) - Also referred to as the _node_ or the _client_, this service allows you to join the Swarm network.

### Libraries

**[`^ back to top ^`](#)**

[Bee-JS](https://github.com/ethersphere/bee-js) - A high-level Javascript library to interact with Bee through its REST API.

[Mantaray-js](https://github.com/ethersphere/mantaray-js) - A low-level Swarm manifest manipulation library.

[Mantaray-py](https://github.com/Ankvik-Tech-Labs/mantaray-py/) - Allows you to manipulate and interpret mantaray data via MantarayNode and MantarayFork abstractions.

### CI/CD

**[`^ back to top ^`](#)**

[Beekeeper](https://github.com/ethersphere/beekeeper) - Orchestrate and test Bee clusters through Kubernetes.

[Swarm Actions](https://github.com/ethersphere/swarm-actions) - GitHub Actions workflow for uploading data to the Swarm network.  

### UI

**[`^ back to top ^`](#)**

[Bee Dashboard](https://github.com/ethersphere/bee-dashboard) - React project to troubleshoot and interact with your Bee node.

[Gateway](https://github.com/ethersphere/gateway) - Gateway to the Swarm project, for uploading, downloading and sharing assets on the network.

[Swarmy](https://swarmy.cloud/) - Swarm as a service, makes it simple to store and retrieve data on Swarm. 

[Swarm Desktop App](https://www.ethswarm.org/build/desktop) -  By running a lightweight Swarm node on your computer, you get direct access to the Swarm peer-to-peer network, without the need for centralized gateways.    

[Etherjot](https://github.com/ethersphere/etherjot) - Bring your web3 blog live in minutes with Etherjot, a graphical blogging application natively supporting Swarm.

[buzzMint](https://github.com/ethersphere/awesome-swarm/blob/master/buzz-mint.eth.limo) - A decentralised NFT creator.

[Bchan](https://bchan.bzz.limo/) - A private message board allowing users to post images, text, and links across various threads.

### Tools

**[`^ back to top ^`](#)**

[Swarm MCP](https://github.com/ethersphere/swarm-mcp) - A Model Context Protocol (MCP) server implementation that uses Ethereum Swarm's Bee API for storing and retrieving data.

[Swarm CLI](https://github.com/ethersphere/swarm-cli) - No more copy-pasting curl commands, with `swarm-cli` you can do everything on Swarm with simple commands straight from the terminal.

[Swarm Extension](https://github.com/ethersphere/swarm-extension) - Official extension that adds Swarm support and injects Bee library to the browser.

[Swarm CID Converter](https://github.com/agazso/swarm-cid-converter) - Convert Swarm hashes or links to CID and vice versa.

[Bee-AFS](https://github.com/aloknerurkar/bee-afs) - FUSE filesystem for Bee.

[Nextcloud Swarm Plugin](https://github.com/MetaProvide/nextcloud-swarm-plugin) - Plugin for bridging Nextcloud and Swarm.

[Beest](https://github.com/w3rkspacelabs/beest) -  An interactive CLI toolkit that simplifies the management of multiple Bee nodes.

[Doctor Bee](https://github.com/w3rkspacelabs/doctor-bee) - A simple python script to check up a Bee node's health status.

[IPFS to Swarm](https://github.com/Solar-Punk-Ltd/ipfs-to-swarm) - Migrate data from IPFS to Swarm.

### Smart Contracts

**[`^ back to top ^`](#)**

[Swap, Swear and Swindle](https://github.com/ethersphere/swap-swear-and-swindle) - Protocols for peer-to-peer accounting.

[Storage Incentives](https://github.com/ethersphere/storage-incentives) - Smart contracts providing the basis for Swarm's storage incentivization model.

### Documentation

**[`^ back to top ^`](#)**

[The Book of Swarm](https://docs.ethswarm.org/the-book-of-swarm.pdf) - Storage and communication infrastructure for self-sovereign digital society back-end stack for the decentralised web.

[Bee Docs](https://github.com/ethersphere/bee-docs) - Documentation for the Swarm Bee Client. View at [docs.ethswarm.org](https://docs.ethswarm.org/docs/).

[Bee-JS Docs](https://github.com/ethersphere/bee-js-docs) - Documentation for the Swarm Bee-js javascript library. View at [bee-js.ethswarm.org](https://bee-js.ethswarm.org/docs/).

[Swarm Specification](https://papers.ethswarm.org/p/swarm-specification/) - The Swarm specification document is an essential resource for developers and software engineers seeking to build their own Swarm client or integrate Swarm's functionalities into their applications.

[Swarm Erasure Coding paper](https://papers.ethswarm.org/p/erasure/) - The erasure coding paper provides a technical exploration of erasure coding in the Swarm network, focusing on ensuring data integrity and resilience.

[Swarm Papers](https://papers.ethswarm.org/) - Swarm‚Äôs documentation includes a variety of papers from technical specifications to in-depth explorations of the network's architecture and functionalities.

[Bee API Reference](https://docs.ethswarm.org/api/) - Bee API Documentation.

### Community / Ecosystem

**[`^ back to top ^`](#)**

[Fair data society](https://fairdatasociety.org/) - Ecosystem initiative for ethical Web3.

[FairOS](https://github.com/fairDataSociety/fairOS-dfs) - Distributed file system, key-value store and nosql store on Swarm (for developers).

[The Fair Data Protocol (FDP)](https://fdp.fairdatasociety.org/) - A data interoperability protocol for dApps that use personal data.
 
[FDP play](https://github.com/fairDataSociety/fdp-play) - CLI tool to spin up local development FDP environment and Bee cluster with Docker.

[Fairdrive](https://fairdrive.fairdatasociety.org/) - Decentralised and unstoppable "Dropbox" for end-users and developers using Fair Data Protocol.

[Fairdrive code](https://github.com/fairDataSociety/fairdrive-theapp) - Code for decentralised and unstoppable "Dropbox" for end-users and developers using Fair Data Protocol.

[Galileo](https://app.galileo.fairdatasociety.org/) - Open Street Maps on Swarm.

[SwarmScan](https://swarmscan.resenje.org/) - Get network insights.

[Etherna.io](https://etherna.io/) - Decentralised media platform on Swarm.

[SwarmNFT library](https://github.com/igar1991/SwarmNFT) - JavaScript library for creating NFTs on Ethereum-compatible blockchains and storing content on Swarm.

[videoNFT](https://github.com/pabloVoorvaart/videoNFT/) - NFT live streaming with Swarm (winner of EthBerlin3 2022 Freedom to Transact Track).

[DeBoot](https://github.com/awmacpherson/deboot) - DeBoot is a project to research and implement approaches to bootloading OS images from a decentralized storage network such as Swarm or IPFS.

[Swarm DAppNode Package](https://github.com/rndlabs/dappnodepackage-swarm) - Swarm DAppNode package for Swarm Mainnet with multi-platform (x86_64 and arm64) support. Testnet DAppNode packages can be found [here](https://github.com/rndlabs/dappnodepackage-swarm-testnet).

[Mipasa Swarm Connector](https://github.com/MiPasa/mipasa-swarm-connector/) - MiPasa connector for Swarm (BZZ) distributed storage network.

[Export Webpage on Swarm](https://github.com/ethersphere/devcon-swarm-exporter) - CLI tool to build an optimized static export of devcon app frontend.

[Blob Storage on Swarm](https://github.com/Blobscan/blobscan) - The pioneer blockchain explorer dedicated to navigate and visualize shard blob transactions.

[SWIPs](https://github.com/ethersphere/SWIPs) - The Swarm Improvement Proposal repository.

### Miscellaneous

**[`^ back to top ^`](#)**

[Swarm Bot](https://github.com/ethersphere/swarm-bot) - Discord bot handling commands related to Swarm and its community.

[ethersphere/bee DeepWiki](https://deepwiki.com/ethersphere/bee) - The DeepWiki for the Bee client GitHub repository. DeepWiki is a tool which provides autogenerated documentation (using LLM ai agents such as ChatGPT or Google Gemini) based directly on code from a GitHub repository. It also has a question box where any question can be asked about the Bee codebase.

*As with all LLMs, DeepWiki may sometimes be confidently wrong. Make sure to always double check (either by inspecting the code yourself, or confirming with a Bee team core developer) before assuming its answers are correct.*

---

## Community

## Official Links

[Twitter](https://twitter.com/ethswarm)  
[Discord server](https://discord.gg/wdghaQsGq5)  
[Reddit](https://www.reddit.com/r/ethswarm/)  
[GitHub](https://github.com/ethersphere)  
[Blog](https://blog.ethswarm.org)  
[Homepage](https://www.ethswarm.org/)  

## Awesome Swarm

An [awesome list](https://awesome.re) on anything awesome related to the Swarm platform. üêù üêù üêù

To see the most up to date list or submit an addition to it, make sure to check the [awesome-swarm repo](https://github.com/ethersphere/awesome-swarm).

## Grants and Bounties

Swarm grants support many interesting projects that are already building their products on top of Swarm. Swarm bounties extend the ecosystem with tooling and infrastructure. 

If you have an idea for a project which uses Swarm's technology we welcome you to [apply for a grant](https://grants.ethswarm.org).

Learn more about grants for building on Swarm at the [EthSwarm homepage](https://www.ethswarm.org/grants).

## Fellowships

[Swarm fellows](https://www.ethswarm.org/fellowships) work on items identified as needs for the Swarm network to evolve and grow but are not part of core Swarm development. Fellows are expected to pursue the goals supported by the fellowship in the long term as part of their career path. A fellowship helps them achieve results to a certain degree, but afterwards, the project should be sustainable and able to continue on its own.

Current Swarm fellows include both [Datafund](https://datafund.io/) and [Solar Punk](https://solarpunk.buzz/).

---

## Fair Data Society

The [Fair Data Society (FDS)](https://fairdatasociety.org/) is a coordinated network developing infrastructure and dApps for a fairer data economy and promoting human rights through digital sovereignty. It is a movement and vision aimed at promoting a decentralized, equitable, and sustainable digital ecosystem that respects individual privacy and data ownership. Its core goal is to empower individuals with control over their data, ensuring that data is used ethically and transparently, while fostering a more balanced relationship between individuals, organizations, and governments.

While FDS is an independent organization, it shares Swarm's vision for the future of data and the decentralized web. FDS uses Swarm's technology as the foundation for the software it develops and incubates for the purpose of realizing its goals.

The suite of Swarm based FDS software provides a wide range of functionalities for a variety of users and use cases. The suite currently consists of:

:::caution
FDS's software is currently in beta or earlier and has no guarantees of file integrity, persistence, or security.
:::

1. [The Fair Data Protocol (FDP)](https://fdp.fairdatasociety.org/) - A data interoperability protocol for dApps that use personal data.
1. [Fairdrive](https://fairdrive.fairdatasociety.org/) - Decentralised storage on Swarm.
1. [Fairdrop](https://fairdrop.fairdatasociety.org/) - An easy and secure way to send your files. No central server. No tracking. No backdoors. 
1. [FairOS](https://docs.fairos.fairdatasociety.org/docs/) - The operating system for the decentralised web.
1. [Galileo](https://app.galileo.fairdatasociety.org/) - An open source project that allows you to use, create and modify maps of various participants.

## Links

* [FDS YouTube](https://www.youtube.com/@fairdatasociety8412)
* [FDS Homepage](https://fairdatasociety.org/)
* [FDS Discord](https://discord.com/invite/vw3PmWf2rE)
* [FDS Twitter](https://twitter.com/fairdatasociety)
* [FDS GitHub](https://github.com/fairDataSociety)

---

## FAQ

# Swarm FAQ

## Community

### What are the Swarm Foundation's official channels?

- Website: [https://ethswarm.org/](https://ethswarm.org/)
- Blog:[https://blog.ethswarm.org/](https://blog.ethswarm.org/)
- Github: [https://github.com/ethersphere](https://github.com/ethersphere)
- e-mail: info@ethswarm.org
- Discord: [https://discord.ethswarm.org/](https://discord.ethswarm.org/)
- Twitter: [https://twitter.com/ethswarm](https://twitter.com/ethswarm)
- Reddit: [https://www.reddit.com/r/ethswarm](https://www.reddit.com/r/ethswarm)
- Youtube: [https://www.youtube.com/channel/UCu6ywn9MTqdREuE6xuRkskA](https://www.youtube.com/channel/UCu6ywn9MTqdREuE6xuRkskA)

### Where can I find technical support and get answers to my other questions?

The Swarm community is centered around our Discord server where you will find many people willing and able to help with your every need! [https://discord.ethswarm.org/](https://discord.ethswarm.org/)

### Where can I find support for running Bee node on Dappnode?

You can find support for running Bee on Dappnode on the Dappnode Discord server: [https://discord.gg/dRd5CrjF](https://discord.gg/dRd5CrjF)

### Who can I contact for other inquiries?

For any other inquiries, you can contact us at [info@ethswarm.org](mailto:info@ethswarm.org)

### What's the relationship between Swarm and Ethereum?

Swarm started in the first days of Ethereum as part of the original "world computer" vision, consisting of Ethereum (the processor), Whisper (messaging) and Swarm (storage). The project is the result of years of research and work by the Ethereum Foundation, the Swarm Foundation, teams, individuals across the ecosystem and the community.

The conceptual idea for Swarm was started in the Ethereum team at the beginning, and the Ethereum Foundation incubated Swarm. After five years of research, Swarm and Ethereum are now two separate entities.

## BZZ Token

### What is BZZ Token?

Swarm's native token BZZ, was initially issued on Ethereum. It has been bridged over to Gnosis where it is referred to as xBZZ for differentiation, and serves as a means of accessing the platform's data relay and storage services, while also providing compensation for node operators who provide these services. 

### What is PLUR?

1 PLUR is the atomic unit of xBZZ, where xBZZ then has 16 decimals (ie. 1 PLUR = 1e-16 xBZZ)

### Where can I buy BZZ tokens?

There are many ways to acquire BZZ tokens, either on custodial centralised exchanges where you can trade traditional currencies and cryptocurrency or through decentralised exchanges and protocols where you can trade between cryptocurrencies. For more information please visit the [Get BZZ](https://www.ethswarm.org/get-bzz) page on the Ethswarm.org homepage. 

*Note that for use on Swarm for staking or purchasing postage stamps, you need the Gnosis Chain version of BZZ, commonly referred to as xBZZ.*

### What is the BZZ token address?

See [this page](/docs/references/smart-contracts) for a list of relevant token addresses. 

### What is the BZZ token supply?

With the [shutdown of the bonding curve](https://blog.ethswarm.org/foundation/2024/bonding-curve-shutdown/) as a result of a [community vote](https://blog.ethswarm.org/foundation/2024/announcing-the-outcome-of-swarms-bonding-curve-vote/), the BZZ supply is now fixed at [63,149,437](https://etherscan.io/token/0x19062190b1925b5b6689d7073fdfc8c2976ef8cb).

### BZZ token tokenomics

More about BZZ token tokenomics: https://blog.ethswarm.org/hive/2021/bzz-tokenomics/

### What is the bonding curve?

A bonding curve is a mathematical function in the form of y=f(x) that determines the price of a single token, depending on the number of tokens currently in existence, or the market supply. The key difference is that with a traditional exchange platform market makers are required to provide liquidity to the market, whereas a bonding curve takes over the role of providing liquidity, negating the need for market makers. 

### What is the "Bzzaar" bonding curve?

During the first several years of the life of the BZZ token, the bonding curve mechanism played a critical role in maintaining liquidity and setting a transparent pricing model for BZZ tokens. 

On May 4th of 2024, as a result of a [community vote](https://blog.ethswarm.org/foundation/2024/announcing-the-outcome-of-swarms-bonding-curve-vote/), the bonding curve was [shut down](https://blog.ethswarm.org/foundation/2024/bonding-curve-shutdown/) and the BZZ supply is now fixed at [63,149,437](https://etherscan.io/token/0x19062190b1925b5b6689d7073fdfc8c2976ef8cb).

---

## Glossary

## Swarm

The Swarm network consists of a collection of [Bee nodes](#bee) which work together to enable decentralised data storage for the next generation of censorship-resistant, unstoppable, serverless dapps. 

Swarm is also the name of the core organization that oversees the development and success of the Bee Swarm as a whole. They can be found at [ethswarm.org](https://www.ethswarm.org/).

## Gnosis Chain

[Gnosis Chain](https://www.gnosis.io/) (previously known as xDai chain) is a [PoS](https://www.gnosis.io/validators), [EVM](https://ethereum.org/en/developers/docs/evm/) compatible Ethereum [sidechain](https://ethereum.org/en/developers/docs/scaling/sidechains/) which uses the same addressing scheme as Ethereum. Swarm's smart contracts have been issued on Gnosis Chain.

## Smart Contracts

Smart contracts are automatically executable code which can be published on a blockchain to ensure immutability. Swarm uses smart contracts on Gnosis Chain for a variety of key aspects of the network including [incentivization](#xbzz-token), [inter-node accounting](#swap), and [payments for storage](#postage-stamps).

## Bee

Swarm nodes are referred to as "Bee" nodes. Bee nodes can run on a wide variety of computer types including desktop computers, hobbyist computers like Raspberry Pi 4 (for light or ultralight nodes), remotely hosted virtual machines, and much more. When running, Bee nodes interact with Swarm smart contracts on Gnosis Chain and connect with other Bee nodes to form the Swarm network.

Bee nodes can act as both client and service provider, or solely as client or service provider, depending on the needs of the node operator. Bee nodes pay each other for services on the Swarm network with the xBZZ token.

## Overlay

An overlay network is a virtual or logical network built on top of some lower level "underlay" network. Examples include the Internet as an overlay network built on top of the telephone network, and the p2p Bittorent network built on top of the Internet. 

With Swarm, the overlay network is based on a [Kademlia DHT](https://en.wikipedia.org/wiki/Kademlia) with overlay addresses derived from each node's [Gnosis](#gnosis-chain) address. Swarm's overlay network addresses are permanent identifiers for each node and do not change over time.

## Overlay Address

Overlay addresses are a Keccak256 hash of a node‚Äôs Gnosis Chain address and the Swarm network ID (the Swarm network ID is included to prevent address collisions). The overlay address for a node does not change over time and is a permanent identifier for the node. Overlay addresses are used to group nodes into neighborhoods which are responsible for storing the same chunks of data. If not otherwise specified, when referring to a "node address", it typically is referring to the overlay address, not the underlay address. The overlay address is the address used to determine which nodes connect to each other and which chunks nodes are responsible for.

## Neighborhood

[Neighborhoods](/docs/concepts/DISC/neighborhoods) are nodes which are grouped together based on their overlay addresses and are responsible for storing the same chunks of data. The chunks which each neighborhood are responsible for storing are defined by the proximity order of the nodes and the chunks. 

## Sister Neighborhood

A sister neighborhood is composed of nodes in the other half of an old neighborhood after a neighborhood split. 

## Parent Neighborhood

A parent neighborhood is the neighborhood one proximity order shallower than the two sister neighborhoods it contains. For example, given a neighborhood at depth 5 of 01011 and its sister neighborhood of 01010, their parent neighborhood is 0101 at depth 4.

## Underlay

An underlay network is the low level network on which an overlay network is built. It allows nodes to find each other, communicate, and transfer data. Swarm's underlay network is a p2p network built with [libp2p](https://libp2p.io/). Nodes are assigned underlay addresses which in contrast to their overlay addresses are not permanent and may change over time. 

## Swap

Swap is the p2p accounting protocol used for Bee nodes. It allows for the automated accounting and settlement of services between Bee nodes in the Swarm network. In the case that services exchanged between nodes is balanced equally, no settlement is necessary. In the case that one node is unequally indebted to another, settlement is made to clear the node's debts. Two key elements of the Swap protocol are [cheques and the chequebook contract](#cheques--chequebook).   

## Cheques & Chequebook

Cheques are the off-chain method of accounting used by the Swap protocol where the issuing node signs a cheque specifying a beneficiary, a date, and an amount, and gives it to the recipient node as a token of promise to pay at a later date. 

The chequebook is the smart contract where the cheque issuer's funds are stored and where the beneficiary can cash the cheque received. 

The cheque and chequebook system reduces the number of required on-chain transactions by allowing multiple cheques to accumulate and be settled together as a group, and in the case that the balance of cheques between nodes is equal, no settlement transaction is required at all. 

## Postage Stamps

Postage stamps can be purchased with [xBZZ](#xbzz-token) and represent the right to store data on the Swarm network. In order to upload data to Swarm, a user must purchase a batch of stamps which they can then use to upload an equivalent amount of data to the network. 

## Kademlia

Kademlia is a distributed hash table (DHT) which is commonly used in distributed peer-to-peer networks. A distributed hash table is a type of hash table which is designed to be stored across a decentralized group of nodes in order to be persistent and fault tolerant. It is designed so that each node is only required to store a subset of the total set of key / value pairs. One of the unique features of the Kademlia DHT design is a distance metric based on the XOR bitwise operation. It is referred to as "Kademlia distance" or just "distance". Swarm‚Äôs DISC uses a modified version of Kademlia which has been specialized for storage purposes, and understanding the concepts behind Kademlia is necessary for understanding Swarm.

## Kademlia distance

Kademlia introduces an XOR based distance metric to define the relatedness of two addresses. In Kademlia nodes have numeric ids with the same length and format taken from the same namespace as the keys of the key/value pairs. Kademlia distance between node ids and keys is calculated through the XOR bitwise operation done over any ids or keys. 

Note: For a Kademlia DHT, any standardized numerical format can be used for ids. However, within Swarm, ids are derived from a Keccak256 digest and are represented as 256 bit hexadecimal numbers. They are referred to as addresses or hashes.

Swarm hash: 

> eada6722670c6de6da7d0470167bf14f6e4dc1b98476da94a7330041adec26a3

In the examples which follow, we use short binary numbers to increase example clarity rather than the actual Swarm hash format.

Example: We have a Kademlia DHT consisting of only ten nodes with ids of 1 - 10. We want to find the distance between node 4 and 7. In order to do that, we perform the XOR bitwise operation:

4 | 0100   
7 | 0111  
‚Äî‚Äî‚Äî‚ÄîXOR  
3 | 0011  

And we find that the distance between the two nodes is 3. 

## Chunk

When data is uploaded to Swarm, it is broken down into 4kb sized pieces which are each assigned an address in the same format as node‚Äôs overlay addresses. Chunk addresses are formed by taking the BMT hash of the chunk content along with an 8 byte measure of the number of the chunk‚Äôs child chunks, the `span`. The BMT hashing algorithm is based on the Keccac256 hashing algorithm, so it produces an address with the same format as that for the node overlay addresses.

## Proximity Order (PO)

Proximity Order is a concept defined in The Book of Swarm and is closely related to Kademlia distance. In contrast to distance which is an exact measure of the relatedness of two nodes, PO is a discrete measure relatedness between two nodes. By "discrete", we mean that PO is a general measure of relatedness rather than an exact measure of relatedness like the XOR distance metric of Kademlia.

Proximity order is defined as the number of shared prefix bits of any two addresses. It is found by performing the XOR bitwise operation on the two addresses and counting how many leading 0 there are before the first 1. 

Taking the previous example used in the Kademlia distance definition:

4 | 0100   
7 | 0111  
‚Äî‚Äî‚Äî‚ÄîXOR  
3 | 0011  

In the result we find that the distance is 3, and that there are two leading zeros. Therefore for the PO of these two nodes is 2.

Both Proximity Order and distance are measures of the relatedness of ids, however Kademlia distance is a more exact measurement. 

Taking the previous example used in the Kademlia distance definition:  

5 | 0101   
7 | 0111  
‚Äî‚Äî‚Äî‚ÄîXOR  
2 | 0010  

Here we find that the distance between 5 and 7 is 2, and the PO is also two. Although 5 is closer to 7 than 4 is to 7, they both fall within the same PO, since PO is only concerned with the shared leading bits. PO is a fundamental concept to Swarm‚Äôs design and is used as the basic unit of relatedness when discussing the addresses of chunks and nodes. PO is also closely related to the concept of depth.   

## Depth types

There are three fundamental categories of depth:
 
### 1. Topology related depth

This depth is defined in relation to the connection topology of a single node as the subject in relation to all the other nodes it is connected to. It is referred to using several different terms which all refer to the same concept (Connectivity depth / Kademlia depth / neighborhood depth / physical depth)

Connectivity depth refers to the saturation level of the node‚Äôs topology - the level to which the topology of a node‚Äôs connections has Kademlia connectivity. Defined as one level deeper than the deepest fully saturated level. A PO is defined as saturated if it has at least the minimum required level of connected nodes, which is set at 8 nodes in the current implementation of Swarm. 

The output from the Bee API's `topology` endpoint:

![](/img/depths1.png)

Here we can see the depth is 8, meaning that PO bin 7 is the deepest fully saturated PO bin:

![](/img/depths2.png) 

Here we can confirm that at least 8 nodes are connected in bin 7.

Connectivity depth is defined from the point of view of individual nodes, it is not defined as characteristic of the entire network. However, given a uniform distribution of node ids within the namespace and given enough nodes, all nodes should converge towards the same connectivity depth. 

While this is sometimes referred to as Kademlia depth, the term ‚ÄúKademlia depth‚Äù is not defined within the original Kademlia paper, rather it refers to the depth at which the network in question (Swarms) has the characteristics which fulfill the requirements described in the Kademlia paper.
 
### 2. Area of responsibility related depths

Area of responsibility refers to which chunks a node is responsible for storing. There are two concepts of depth related to a node‚Äôs area of responsibility - storage depth and reserve depth. Both reserve depth and storage depth are measures of PO which define the chunks a node is responsible for storing.  
 
### 2a. Reserve Depth

The PO which measures the node‚Äôs area of responsibility based on the theoretical 100% utilisation of all postage stamp batches (all the chunks which are eligible to be uploaded and stored are uploaded and stored). Has an inverse relationship with area of responsibility - as depth grows, area of responsibility gets smaller.

### 2b. Storage Depth

The PO which measures the node‚Äôs effective area of responsibility. Storage depth will equal reserve depth in the case of 100% utilisation - however 100% utilisation is uncommon. If after syncing all the chunks within the node‚Äôs area of responsibility at its reserve depth and the node still has sufficient space left, then the storage depth will decrease so that the area of responsibility doubles. 

### 3. Postage stamp batch and chunk related depths

### 3a. Batch depth
Batch depth is the value `d` which is defined in relation to the size of a postage stamp batch. The size of a batch is defined as the number of chunks which can be stamped by that batch (also referred to as the number of slots per batch, with one chunk per slot). The size is calculated by:

* $$2^{d}$$
* $$d$$ is a value selected by the batch issuer which determines how much data can be stamped with the batch

### 3b. Bucket depth 

Bucket depth is the constant value which defines how many buckets the address space for chunks is divided into for postage stamp batches. Bucket depth is set to 16, and the number of buckets is defined as $$2^{bucket depth}$$

## PLUR

PLUR (name inspired by the [PLUR principles](https://en.wikipedia.org/wiki/PLUR)) is the smallest denomination of BZZ. 1 PLUR is equal to 1e-16 BZZ.

## Bridged Tokens

Bridged tokens are tokens from one blockchain which have been _bridged_ to another chain through a smart contract powered bridge. For example, xDAI and xBZZ on Gnosis Chain are the bridged version of DAI and BZZ on Ethereum. 

## BZZ Token

BZZ is Swarm's [ERC-20](https://ethereum.org/en/developers/docs/standards/tokens/erc-20/) token issued on Ethereum.   

## xBZZ Token

xBZZ is BZZ bridged to the [Gnosis Chain](https://www.gnosis.io/) using the [Gnosis Chain Bridge](https://bridge.gnosischain.com/).

It is used as payment for [postage stamps](#postage-stamps) and as the unit of accounting between the nodes. It is used to incentivize nodes to provide resources to the Swarm.

## DAI Token

[DAI](https://developer.makerdao.com/dai/1/) is an [ERC-20](https://ethereum.org/en/developers/docs/standards/tokens/erc-20/) stable token issued on the Ethereum blockchain, tracking USD.

## xDAI Token

xDAI is [DAI](https://developer.makerdao.com/dai/1/) [bridged](#bridged-tokens) to the [Gnosis Chain](https://www.gnosis.io) using [xDai Bridge](https://bridge.gnosischain.com/). It is also the native token of the Gnosis Chain, i.e. transaction fees are paid in xDai.

## Sepolia

Sepolia is an Ethereum testnet. It is an environment where smart contracts can be developed and tested without spending cryptocurrency with real value, and without putting valuable assets at risk. Tokens on Sepolia are often prefixed with a lower-case 's', example: 'sBZZ' and because this is a test network carry no monetary value. It is an environment where Bee smart contracts can be tested and interacted with without any risk of monetary loss.

## Faucet

A cryptocurrency faucet supplies small amounts of cryptocurrency to requestors (typically for testing purposes).

Check out the [Fund Your Node](In order to acquire sETH and sBZZ, refer to the [Fund Your Node](/docs/bee/installation/fund-your-node) page.) page for more information.

## RPC Endpoint

An RPC (Remote Procedure Call) endpoint is a URL that allows applications to communicate with a remote server by sending requests and receiving responses. It is commonly used to interact with decentralized networks, enabling applications to query data or send transactions without running a full node.

In the context of Swarm, a Blockchain RPC endpoint refers specifically to a connection to Gnosis Chain, which is required for transactions such as purchasing postage stamps and staking xBZZ. Bee nodes rely on an RPC endpoint to facilitate these blockchain interactions.

---

## Smart Contracts

import { globalVariables } from '/src/config/globalVariables';

### Token Contracts
|Contract|Blockchain | Address             | 
| ---------------------- | ------------------------------- |--------- |
|BZZ token| Ethereum          | [`0x19062190b1925b5b6689d7073fdfc8c2976ef8cb`](https://ethplorer.io/address/0x19062190b1925b5b6689d7073fdfc8c2976ef8cb)                |
|xBZZ token | Gnosis Chain    | [`0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da`](https://gnosisscan.io/token/0xdbf3ea6f5bee45c02255b2c26a16f300502f68da) |
|sBZZ token| Sepolia (Ethereum testnet) | [`0x543dDb01Ba47acB11de34891cD86B675F04840db`](https://sepolia.etherscan.io/address/0x543dDb01Ba47acB11de34891cD86B675F04840db)         |

### Storage Incentives Contracts

You can find the Solidity source code for each contract in the [storage incentives Github repo](https://github.com/ethersphere/storage-incentives 
). 

For a list of the current smart contract addresses, see the [storage incentives ABI repo](https://github.com/ethersphere/go-storage-incentives-abi).

For a history of smart contract addresses, see the [storage incentives ABI repo history](https://github.com/ethersphere/go-storage-incentives-abi/commits/master/abi/abi_mainnet.go).

| Contract        | Blockchain   | Address                                                                                                        |
| --------------- | ------------ | -------------------------------------------------------------------------------------------------------------- |
| Postage Stamp   | Gnosis Chain | {globalVariables.postageStampContract} |
| Staking         | Gnosis Chain | {globalVariables.stakingContract} |
| Redistribution  | Gnosis Chain | {globalVariables.redistributionContract} |
| Price Oracle    | Gnosis Chain | {globalVariables.priceOracleContract} |

---

## Tokens

## Swarm Ecosystem Tokens

### BZZ

:::info
On May 4th of 2024, as a result of a [community vote](https://blog.ethswarm.org/foundation/2024/announcing-the-outcome-of-swarms-bonding-curve-vote/), the bonding curve was [shut down](https://blog.ethswarm.org/foundation/2024/bonding-curve-shutdown/) and the BZZ supply is now fixed at [63,149,437](https://etherscan.io/token/0x19062190b1925b5b6689d7073fdfc8c2976ef8cb).
:::

BZZ is the original token issued from the [Ethswarm Bonding Curve contract](https://etherscan.io/address/0x4f32ab778e85c4ad0cead54f8f82f5ee74d46904) on the Ethereum blockchain. 

BZZ Ethereum address: [0x19062190b1925b5b6689d7073fdfc8c2976ef8cb](
https://etherscan.io/address/0x19062190b1925b5b6689d7073fdfc8c2976ef8cb)

PLUR is the smallest denomination of BZZ. 1 PLUR is equal to 1e-16 BZZ.

### xBZZ

"xBZZ" is the term used to indicate BZZ on Gnosis Chain. It is the bridged version of the original Ethereum BZZ token issued on Gnosis Chain. xBZZ is the token used for staking and to pay for storage fees on Swarm.

xBZZ Gnosis Chain address: [0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da](https://gnosisscan.io/address/0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da)

:::info
Note that the ticker symbol is the same BZZ for both Gnosis Chain and Ethereum versions of the token. xBZZ is term of convenience used to differentiate the tokens within the Swarm community.
:::

As with BZZ, PLUR is the smallest denomination of xBZZ. 1 PLUR is equal to 1e-16 xBZZ.

### sBZZ

sBZZ is the testnet version of BZZ on the Sepolia Ethereum testnet. 

Sepolia testnet address: [0x543dDb01Ba47acB11de34891cD86B675F04840db](https://sepolia.etherscan.io/address/0x543dDb01Ba47acB11de34891cD86B675F04840db)

### DAI 

DAI is the popular decentralized stablecoin from [MakerDAO](https://makerdao.com/en/).

### xDAI

xDAI is the bridged version of DAI on Gnosis Chain and also serves as the native gas token for Gnosis Chain and is used to pay transaction fees on Gnosis Chain in the same way ETH is used to pay for transactions on Ethereum. It is required by Bee nodes to pay for transaction fees when interacting with Swarm smart contracts on Gnosis Chain.

### Getting BZZ / xBZZ

The Swarm official website has a page with [a list of resources for getting BZZ tokens](https://www.ethswarm.org/get-bzz). Be careful to check whether it is BZZ on Ethereum or Gnosis Chain.

### Bridging BZZ to xBZZ or DAI to xDAI

If you already have DAI or BZZ on Ethereum then you can use the [Gnosis Chain Bridge](https://bridge.gnosischain.com/) for swapping between DAI and xDAI or BZZ and xBZZ.
