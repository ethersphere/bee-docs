[{"title":"API and Debug API","type":0,"sectionRef":"#","url":"/docs/api-reference/","content":"","keywords":""},{"title":"API‚Äã","type":1,"pageTitle":"API and Debug API","url":"/docs/api-reference/#api","content":"The API-endpoint exposes all functionality to upload and download content to and from the Swarm network. By default, it runs on port :1633. Detailed information about Bee API endpoint can be found here: "},{"title":"API Reference‚Äã","type":1,"pageTitle":"API and Debug API","url":"/docs/api-reference/#api-reference","content":""},{"title":"Debug API‚Äã","type":1,"pageTitle":"API and Debug API","url":"/docs/api-reference/#debug-api","content":"The Debug API is disabled by default but can be enabled by setting the debug-api-enable configuration option to true. The Debug API exposes functionality to inspect the state of your Bee node while it is running, as well as some other features that should not be exposed to the public Internet. The Debug API runs on port :1635 by default. info For a new installation of Bee, the Debug API endpoint is not yet exposed for security reasons. To enable the Debug API endpoints, set--debug-api-enable to true in your configuration file and restart your Bee. "},{"title":"Debug API Reference‚Äã","type":1,"pageTitle":"API and Debug API","url":"/docs/api-reference/#debug-api-reference","content":"danger Your Debug API should not be exposed to the public Internet, make sure that your network has a firewall which blocks port 1635, or bind the Debug API to localhost "},{"title":"Build from Source","type":0,"sectionRef":"#","url":"/docs/bee/installation/build-from-source","content":"","keywords":""},{"title":"Build from Source‚Äã","type":1,"pageTitle":"Build from Source","url":"/docs/bee/installation/build-from-source#build-from-source","content":"Clone the repository: git clone https://github.com/ethersphere/bee cd bee Use git to find the latest release: git describe --tags Checkout the required version: git checkout v1.17.4 Build the binary: make binary Check you are able to run the bee command. Success can be verified by running: dist/bee version 1.17.4 (optional) Additionally, you may also like to move the Bee binary to somewhere in your $PATH sudo cp dist/bee /usr/local/bin/bee  "},{"title":"Connectivity","type":0,"sectionRef":"#","url":"/docs/bee/installation/connectivity","content":"","keywords":""},{"title":"Networking Basics‚Äã","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#networking-basics","content":"In a network, each computer is assigned an IP address. Each IP address is then subdivided into thousands of sockets or ports, each of which has an incoming and outgoing component. In a completely trusted network of computers, any connections to or from any of these ports are allowed. However, to protect ourselves from nefarious actors when we join the wider Internet, it is sometimes important to filter this traffic so that some of these ports are off limits to the public. In order to allow messages to our p2p port from other Bee nodes that we have previously not connected, we must ensure that our network is set up to receive incoming connections (on port 1634 by default). danger There are also some ports which you should never expose to the outside Internet. Make sure that your api-addr (default 1633) is only ever exposed in Gateway Mode and your debug-api-addr (default 1635) is never exposed to the Internet. It is good practice to employ one or more firewalls that block traffic on every port except for those you are expecting to be open. "},{"title":"Your IP Address‚Äã","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#your-ip-address","content":"When you connect to the Internet, you are assigned a unique number called an IP Address. IP stands for Internet Protocol. The most prevalent IP version used is still the archaicIPv4 which was invented way back in 1981. IPv6 is available but not well used. Due to the mitigation of the deficiencies inherent in the IPv4 standard, we may encounter some complications. "},{"title":"Datacenters and Computers Connected Directly to the Internet‚Äã","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#datacenters-and-computers-connected-directly-to-the-internet","content":"If you are renting space in a datacenter, the chances are that your computer will be connected directly to the real Internet. This means that the IP of your networking interface will be directly set to be the same as your public IP. You can investigate this by running: ifconfig  or ip address  Your output should contain something like: eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 178.128.196.191 netmask 255.255.240.0 broadcast 178.128.207.255  Here we can see our computer's public IP address178.128.196.191. This is the address that is used by other computers we connect to over the Internet. We can verify this using a third party service such as icanhazip or ifconfig. curl icanhazip.com --ipv4  or curl ifconfig.co --ipv4  The response something contain something like: 178.128.196.191  With Bee running, try to connect to your Bee's p2p port using the public IP adddress from another computer: nc -zv 178.128.196.191 1634  If you have success, congratulations! If this still doesn't work for you, see the last part of Manual: Configure Your Router and Bee section below, as you may need to configure your nat-addr. "},{"title":"Home, Commercial and Business Networks and Other Networks Behind NAT‚Äã","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#home-commercial-and-business-networks-and-other-networks-behind-nat","content":"To address thescarcity of IP numbers, Network Address Translation (NAT) was implemented. This approach creates a smaller, private network which many devices connect to in order to share a public IP address. Traffic destined for the Internet at large is then mediated by another specialised computer. In the cases of the a home network, this computer is the familiar home router, normally also used to provide a WiFi network. If we run the above commands to find the computer's IP in this scenario, we will see a different output. ip address  en0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ... inet 192.168.0.10 netmask 0xffffff00 broadcast 192.168.0.255 ...  Here we can see that, instead of the public IP address, we can see that our computer's IP address is 192.168.0.10. This is part of the IP address space that the Internet Engineering Task Force has designated forprivate networks. As this IP won't work on the global Internet, our router remembers that our computer has been assigned this IP. It then uses Network Address Translation (NAT) to modify all requests from our computer to another computer somewhere in the Internet. As the requests pass through the router it changes our local IP to the public IP of the router, and vice versa when the responses are sent back, from the public IP to the local one. Navigating Through the NAT‚Äã The presence of NAT presents two problems for p2p networking. The first is that it can be difficult for programs running on our computer to know our real public IP as it is not explicitly known by our computer's networking interface, which is configured with a private network IP. This is a relatively easy problem to solve as we can simply discover our public IP and then specify it in Bee's configuration, or indeed determine it using other means. The second issue is that our router has only 65535 ports to expose to the public network, however, each device on your private network is capable of exposing 65535 each. To the global Internet, it appears that there is only one set of ports to connect to, whereas, in actual fact, there is a full set of ports for each of the devices which are connected to the private network. To solve this second problem, routers commonly employ an approach known as port forwarding. Bee's solution to these problems come in two flavours, automatic and manual. Automatic: Universal Plug and Play (UPnP)‚Äã UPnP is a protocol designed to simplify the administration of NAT and port forwarding for the end user by providing an API from which software running within the network can use to ask the router for the public IP and to request for ports to be forwarded to the private IP of the computer running the software. UPnP is a security risk! UPnP is a security risk as it allows any host or process inside (sometimes also outside) your network to open arbitrary ports which may be used to transfer malicious traffic, for example aRAT. UPnP can also be used to determine your IP, and in the case of using ToR or a VPN, your real public IP. We urge you to disable UPnP on your router and use manual port forwarding as described below. Bee will use UPnP to determine your public IP, which is required for various internal processes. In addition to this, a request will be sent to your router to ask it to forward a random one of its ports, which are exposed directly to the Internet, to the Bee p2p port (default 1634) which your computer is exposing only to the private network. Doing this creates a tunnel through which other Bee's may connect to your computer safely. If you start your Bee node in a private network with UPnP available, the output of the addresses endpoint of your Debug API will look something like this: [ &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP&quot;, &quot;/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP&quot;, &quot;/ip6/::1/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP&quot;, &quot;/ip4/86.98.94.9/tcp/20529/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP&quot; ]  Note that the port in the externalmultiaddress is the router's randomly selected 20529 which is forwarded by the router to192.168.0.10:1634. These addresses in this multiaddress are also known as the underlay addresses. Manual: Configure Your Router and Bee‚Äã Inspecting the underlay addresses in the output of the addresses endpoint of our Debug API, we can see addresses only for localhost127.0.0.1 and our private network IP 192.168.0.10. Bee must be having trouble navigating our NAT. [ &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip6/::1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot; ]  To help fix the first problem, let's determine our public IP address. curl icanhazip.com  86.98.94.9  Now we can simply supply this IP in our Bee configuration on startup. Solving our second problem is a little more difficult as we will need to interact with our router's firmware, which is a little cranky. Each router is different, but the concept is usually the same. Log in to your router by navigating your browser to your router's configuration user interface, usually at http://192.168.0.1. You will need to log in with a password. Sadly, passwords are often left to be the defaults, which can be found readily on the Internet. Once logged in, find the interface to set up port forwarding. The Port Forward website provides some good information, or you may refer to your router manual or provider. Here, we will then set up a rule that forwards port 1634 of our private IP address 192.168.0.10 to the same port 1634 of our public IP. Now, when requests arrive at our public address 86.98.94.9:1634 they are modified by our router and forwarded to our private IP and port192.168.0.10:1634. Sometimes this can be a little tricky, so let's verify we are able to make a TCP connection using netcat. First, with Bee not running, let's set up a simple TCP listener using Netcat on the same machine we would like to run Bee on. nc -l 0.0.0.0 1634  nc -zv 86.98.94.9 1634  Connection to 86.98.94.9 port 1834 [tcp/*] succeeded!  Success! ‚ú® If this didn't work for you, check out our Debugging Connectivity guide below. If it did, let's start our Bee node with the --nat-addr configured. bee start --nat-addr 86.98.94.9:1634  Checking our addresses endpoint again, we can now see that Bee has been able to successfully assign a public address! Congratulations, your Bee is now connected to the outside world! [ &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip6/::1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip4/86.98.94.9/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot; ]  info If you are regularly connecting and disconnecting to a network, you may also want to use your router's firmware to configure the router to reserve and only assign the same local network IP from its DHCP pool to your computer's MAC address. This will ensure that your Bee seamlessly connects when you rejoin the network! "},{"title":"Debugging Connectivity‚Äã","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#debugging-connectivity","content":"The above guide navigates your NAT, but there are still a few hurdles to overcome. To make sure there is a clear path from your computer to the outside world, let's follow our Bee's journey from the inside out. Let's set up a netcat listener on all interfaces on the computer we'd like to run Bee on as we have above. nc -l 0.0.0.0 1634  Now, let's verify we're able to connect to netcat by checking the connection from our local machine. nc -zv 127.0.0.1 1634  Connection to 127.0.0.1 port 1634 [tcp/*] succeeded!  This should be a no brainer, the connection between localhost in not normally mediated. If there is a problem here, the problem is with some other software running on your operating system or your operating system itself. Try a different port, such as 1734 and turning off any unneccesary software. If this doesn't work, you may need to try a different operating system environment. Please get in touch and we'll try to help! If we were successful, let's move on to the next stage. info If you are not able to get access to some firewall settings, or otherwise debug incoming connectivity, don't worry! All is not lost. Bee can function just fine with just outgoing connections. However, if you can, it is worth the effort to allow incoming connections, as the whole swarm will benefit from the increased connectivity. Let's find out what our IP looks like to the Internet. curl icanhazip.com  86.98.94.9  Now try to connect to your port using the global IP. nc -zv 86.98.94.9 1634  If this is successful, our Bee node's path is clear! If not, we can try a few things to make sure there are no barriers stopping us from getting through. Check your computer's firewall. Sometimes your computer is configured to prevent connections. If you are on a private network mediated by NAT, you can check if this is the problem by trying to connect from another device on your network using the local IP nc -zv 192.168.0.10 1634. Ubuntu uses UFW, MacOS can be configured using the Firewall tab in the Security &amp; Privacysection of System Preferences. Windows usesDefender Firewall. For each of these firewalls, set a special rule to allow UDP and TCP traffic to pass through on port 1634. You may want to limit this traffic to the Bee application only. Check your ingress' firewall. For a datacenter hired server, this configuration will often take place in somewhere in the web user interface. Refer to your server hosting provider's documentation to work out how to open ports to the open Internet. Ensure that both TCP and UDP traffic are allowed. Similarly, if you are connecting from within a private network, you may find that the port is blocked by the router. Each router is different, so consult your router's firware documentation to make sure there are no firewalls in place blocking traffic on your Bee's designated p2p port. You may check this using netcat by trying to connect using your computer's public IP, as above nc -zv 86.98.94.9 1634. Docker Docker adds another level of complexity. To debug docker connectivity issues, we may use netcat as above to check port connections are working as expected. Double check that you are exposing the right ports to your local network, either by using the command line flags or in your docker-compose.yaml. You should be able to successfully check the connection locally using eg. nc -zv localhost 1634 then follow instructions above to make sure your local network has the correct ports exposed to the Internet. Something else entirely? Networking is a complex topic, but it keeps us all together. If you still can't connect to your Bee, get in touch via The Beehive and we'll do our best to get you connected. In the swarm, no Bee is left behind. "},{"title":"Bee Using Docker","type":0,"sectionRef":"#","url":"/docs/bee/installation/docker","content":"","keywords":""},{"title":"Quick Start‚Äã","type":1,"pageTitle":"Bee Using Docker","url":"/docs/bee/installation/docker#quick-start","content":"Try Bee out by simply running the following command in your terminal. docker run \\ -p 1635:1635 \\ -p 1634:1634 \\ -p 1633:1633 \\ --rm -it ethersphere/bee:stable \\ start \\ --welcome-message=&quot;Bzzzz bzzz bzz bzz. üêù&quot; \\ --blockchain-rpc-endpoint http://localhost:8545 \\ --debug-api-enable  info If starting your node for the first time, you will need to deploy a chequebook contract. See installation section for more info. To persist files, mount a local directory as follows and enter the password used to encrypt your keyfiles. Note, docker insists on absolute paths when mounting volumes, so you must replace /path/to/.bee-docker with a valid path from your local filesystem. docker run \\ -v /path/to/.bee-docker:/home/bee/.bee \\ -p 1635:1635 \\ -p 1634:1634 \\ -p 1633:1633 \\ --rm -it ethersphere/bee:stable \\ start \\ --welcome-message=&quot;Bzzzz bzzz bzz bzz. üêù&quot; \\ --blockchain-rpc-endpoint https://gno.getblock.io/&lt;&lt;your-api-key&gt;&gt;/mainnet/ \\ --debug-api-enable  Once you have generated your keys, use the -d flag to run in detached mode and leave Bee running in the background: docker run \\ -d -v /path/to/.bee-docker:/home/bee/.bee\\ -p 1635:1635 \\ -p 1634:1634 \\ -p 1633:1633 \\ --rm -it ethersphere/bee:stable \\ start \\ --welcome-message=&quot;Bzzzz bzzz bzz bzz. üêù&quot; \\ --blockchain-rpc-endpoint https://gno.getblock.io/&lt;&lt;your-api-key&gt;&gt;/mainnet/ \\ --debug-api-enable  "},{"title":"Versions‚Äã","type":1,"pageTitle":"Bee Using Docker","url":"/docs/bee/installation/docker#versions","content":"In order to avoid accidentally upgrading your Bee containers, or deadlocks resulting from Docker caching solutions, it is recommended to use best practices and pin the specific version of Bee that you want to run. Specific Versions‚Äã docker pull ethersphere/bee:1.17.4  Using Tags‚Äã docker pull ethersphere/bee:beta  You may use the tags beta, latest, and stable, or find out more at the Docker Hub repository. "},{"title":"Docker Compose‚Äã","type":1,"pageTitle":"Bee Using Docker","url":"/docs/bee/installation/docker#docker-compose","content":"Configuration files for Bee are provided to enable quick and easy installation with persistent storage and secure secret management. Setup‚Äã First, retrieve the current docker-compose.yaml file. wget -q https://raw.githubusercontent.com/ethersphere/bee/v1.4.1/packaging/docker/docker-compose.yml  Next, create a .env file using the example file provided. This file will be responsible for storing configuration and secrets for our Bee node(s). wget -q https://raw.githubusercontent.com/ethersphere/bee/v1.4.1/packaging/docker/env -O .env  There are some important configuration parameters which must be set in order for our projects to work. To affect configuration in the .env file, we first remove the # at the beginning of the line and then change the value after = to our desired config. For Bee, amend the following parameters: BEE_BLOCKCHAIN_RPC_ENDPOINT=https://gno.getblock.io/&lt;&lt;your-api-key&gt;&gt;/mainnet/ BEE_PASSWORD=my-password   With the configuration settings complete, you can start your Bee node(s) by running: ```bash docker-compose up -d  tip By specifying the -d flag to docker-compose we run Bee in detached mode so that it continues running in the background. danger Docker Compose will create a Docker volume called bee containing important key material. Make sure to backup the contents of your Docker volume! To determine the Bee node's address to fund, we can check the logs for our Bee container: docker-compose logs -f bee-1  bee_1 | time=&quot;2020-12-15T18:43:14Z&quot; level=warning msg=&quot;cannot continue until there is sufficient ETH (for Gas) and at least 1 xBZZ available on 7a977fa660e2e93e1eba40030c6b8da68d01971e&quot; time=&quot;2020-12-15T18:43:14Z&quot; level=warning msg=&quot;learn how to fund your node by visiting our docs at https://docs.ethswarm.org/docs/bee/installation/fund-your-node&quot;  Once you have determined your Bee's Ethereum addresses,fund your node. After your transaction has been completed, your node should recognise that your wallet has been funded, and begin to deploy and fund your Bee chequebook! Once Bee has completed this procedure, you may query the Bee HTTP API at http://localhost:1633. curl localhost:1633  Ethereum Swarm Bee  Once you start seeing messages in the docker-compose logs -f bee-1like: successfully connected to peer 7fa40ce124d69ecf14d6f7806faaf9df5d639d339a9d343aa7004373f5c46b8f (outbound)  You're connected to the Swarm. Let's do a quick check to find out how many peers we have using the curl command line utility: curl localhost:1635/peers  { &quot;peers&quot;: [ { &quot;address&quot;: &quot;339cf2ca75f154ffb8dd13de024c4a5c5b53827b8fd21f24bec05835e0cdc2e8&quot; }, { &quot;address&quot;: &quot;b4e5df012cfc281e74bb517fcf87fc2c07cd787929c332fc805f8124401fabae&quot; } ] }  If you see peers listed here - congratulations! You have joined the swarm! Welcome! üêù "},{"title":"Gateway","type":0,"sectionRef":"#","url":"/docs/bee/installation/gateway","content":"Gateway","keywords":""},{"title":"Fund Your Node","type":0,"sectionRef":"#","url":"/docs/bee/installation/fund-your-node","content":"","keywords":""},{"title":"A node's wallet‚Äã","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#a-nodes-wallet","content":"When your Bee node is installed, an Ethereum wallet is also created. This wallet is used by Bee to interact with the blockchain (e.g. for sending and receiving cheques, or for making purchases of postage stamps, etc.). "},{"title":"Chequebook‚Äã","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#chequebook","content":"When your node has downloaded enough content to exceed the free tier threshold, then cheques are sent to peers to provide payment in return for their services. In order to send these cheques, a chequebook must be deployed on the blockchain for your node, and for full speed operation it can be funded with BZZ. This deployment happens when a node initialises for the first time. Your Bee node will warn you in its log if there aren't enough funds in its wallet for deploying the chequebook. You can configure the amount of xBZZ to be sent from the node's wallet. It is 1 xBZZ by default, but it can be set to zero. "},{"title":"Joining the swarm (mainnet)‚Äã","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#joining-the-swarm-mainnet","content":""},{"title":"Basic deployment‚Äã","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#basic-deployment","content":"If you want to get your Bee node up and running as easily as possible, then you can set its--swap-initial-depositvalue to zero. This means that your node's chequebook will not get funded with xBZZ, meaning that other nodes will only serve it within the free tier bandwidth threshold. Since gas fees on the Gnosis Chain are very low, you won't need much xDAI either to get started. You may acquire a small amount for free by using the official Gnosis Chain xDAI faucet xDAI Faucet. The required amount is a function of the current transaction fee on chain, but 0.01 xDAI should be more than enough to start up your node. You can use the Blockscout block explorer to inspect what's going on with your wallet by searching for its Ethereum address. "},{"title":"Full performance node‚Äã","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#full-performance-node","content":"If you want to run a full node, or upload a lot of content, then you may need more xDAI for gas. To acquire this, you may convert DAI on the main Ethereum network to xDAI using thexDAI bridge, or buy xDAIdirectly using fiat. You will also need to fund your node with more xBZZ for full speed access, or to purchase postage stamps to upload content. To bridge BZZ from the Ethereum mainet to the Gnosis Chain, you may useOmniBridge. To find out what your node's Ethereum address is, please consult your relevant installation guide or check your logs! Configure Your Wallet App To interact with the BZZ ecosystem, you will need to make a couple of small configuration additions to your wallet software. In the case of e.g. MetaMask, you'll need toadd the Gnosis Chain network, and thenadd a custom token. The canonical addresses for the BZZ token on the various blockchains are as follows: Blockchain\tContract addressEthereum, BZZ\t0x19062190b1925b5b6689d7073fdfc8c2976ef8cb Gnosis Chain, xBZZ\t0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da Goerli (testnet), gBZZ\t0x2ac3c1d3e24b45c6c310534bc2dd84b5ed576335 Accessing Your Node's Wallet If you wish to interact with the node's wallet directly then you can import it into a wallet app like MetaMask. To do that you will need the wallet file and its password. A Bee node's wallet key is stored within the keys/ folder in its datadir, in JSON format, and its password should be in a file nearby it. For example on Debian or Ubuntu: sudo cat /var/lib/bee/keys/swarm.key sudo cat /var/lib/bee/password  Testnet A Bee node needs gETH and gBZZ in its wallet to be able to properly interact with the test network. One way to acquire these funds is to sign into our Discord and request gETH and gBZZ test tokens from thefaucet bot to your node's Ethereum address. To find out what your node's Ethereum address is, please consult the installation guide or check the logs! Once you have the address: join our Discord servernavigate to the #faucet channelverify your usernamerequest test tokens from the faucet bot To request the tokens you must type (not copy paste) the following, replacing the address with your own: /faucet sprinkle 0xabeeecdef123452a40f6ea9f598596ca8556bd57  If you have problems, please let us know by making a post in the #faucet channel, we will do our best to provide tokens to everyone. Note that you should use a Chromium-based client (e.g., Chrome, native Discord client) to type the faucet command, as support for other browsers is spotty. It's reported to not work on Firefox, for example. Transactions may take a while to complete, please be patient. We're also keen for you to join us in the swarm, and indeed you soon will! üêù üêù üêù "},{"title":"Hive","type":0,"sectionRef":"#","url":"/docs/bee/installation/hive","content":"","keywords":""},{"title":"Docker‚Äã","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#docker","content":"Up to date Docker images for Bee are provided. "},{"title":"Docker-Compose‚Äã","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#docker-compose","content":"It becomes easier to run multiple Bee nodes withdocker-compose. Check out the Docker compose section of theDocker README. "},{"title":"Helm‚Äã","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#helm","content":"If you really want to run a lot of Bee nodes and you have experience using Kubernetes with Helm, you can have a look at how we manage our cluster under Ethersphere/helm. "},{"title":"Manually‚Äã","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#manually","content":"If you just want to run a handful of bee nodes, you can run multiple bee nodes by creating separate configuration files. Create your first configuration file by running bee printconfig &amp;&gt; bee-config-1.yaml  Make as many copies of bee-config-1.yaml as you want to run bee nodes. Increment the number in the name (bee-config-1 to bee-config-2) for each new configuration file. Configure your nodes as desired, but ensure that the values api-addr, data-dir, debug-api-addr, and p2p-addr are unique for each configuration. "},{"title":"Monitoring‚Äã","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#monitoring","content":"See the monitoring section on how to access Bee's internal metrics! Share your community creations (like swarmMonitor - thanks doristeo!) in the #node-operators channel of our Discord server so we can add you to our list of all things that are awesome and Swarm. üß° "},{"title":"Quick Start","type":0,"sectionRef":"#","url":"/docs/bee/installation/quick-start","content":"","keywords":""},{"title":"Comparison of Node Types‚Äã","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#comparison-of-node-types","content":"Feature\tFull Node\tLight Node\tUltra-light NodeDownloading\t‚úÖ\t‚úÖ\t‚úÖ Uploading\t‚úÖ\t‚úÖ\t‚ùå Can exceed free download limits by paying xBZZ\t‚úÖ\t‚úÖ\t‚ùå Storing &amp; sharing data\t‚úÖ\t‚ùå\t‚ùå Storage incentives\t‚úÖ\t‚ùå\t‚ùå SWAP incentives\t‚úÖ\t‚úÖ\t‚ùå PSS messaging\t‚úÖ\t‚úÖ\t‚úÖ Gnosis Chain Connection\t‚úÖ\t‚úÖ\t‚ùå info The Swarm network includes two incentives protocols which each give Bee nodes incentives to participate in maintaining the network in a healthy way. Storage incentives:By participating in the storage incentives protocol, full nodes which store and share data chunks with the network have a chance to earn xBZZ. Staked xBZZ is required to earn storage incentives. Learn more in the staking section.SWAP incentives:The SWAP incentives protocol encourages full or light (but not ultra-light) nodes to share bandwidth with other nodes in exchange for payments from other nodes either in-kind or as a cheque to be settled at a future date. SWAP requires a chequebook contract to be set up on Gnosis Chain for each participating node.  "},{"title":"Which type of node is the right choice?‚Äã","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#which-type-of-node-is-the-right-choice","content":"Different node types best suit different use cases: "},{"title":"Interact with the Swarm network‚Äã","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#interact-with-the-swarm-network","content":"If you want to interact with the Bee ecosystem in a decentralised way, but not earn xBZZ by storing or forwarding chunks, simply run a Beelight node in the background on your laptop or desktop computer. This will enable direct access to the Swarm network from your web browser and other applications. If you only need to download a small amount of data from the Swarm network an ultra light node could be the right choice for you. This will allow you to download a limited amount of data but does not support uploading data. To run a light or ultra-light node install Bee with the recommended configuration settings for your chosen node type. info The Swarm Desktop app offers an easy way to automatically set up a light or ultra-light node and interact with it through a graphical user interface. "},{"title":"Support the Network and Earn xBZZ by Running a Full Node‚Äã","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#support-the-network-and-earn-xbzz-by-running-a-full-node","content":"Earn xBZZ and help keep the swarm strong by running your own full node. It's easy to set up your own Bee on a small computer like a Raspberry Pi 4, cloud host, or any home computer that's connected to the internet. To run a full node install Bee with the recommended configuration settings for a full node. info Staking is not required to run a full node, but is necessary to earn storage incentives. An altruistic person may want to run a full node without putting up any stake, and in fact, could possibly earn enough xBZZ from bandwidth (swap/cheque) compensation to be able to stake at some point in the future. Learn more in the staking section "},{"title":"Run Your Own Hive of Nodes‚Äã","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#run-your-own-hive-of-nodes","content":"Take it to the next level by keeping a whole hive of Bees! We provide tooling and monitoring to help you manage large deployments of multiple Bee nodes: Bee Hives. "},{"title":"verify","type":0,"sectionRef":"#","url":"/docs/bee/installation/verify","content":"verify verify.md https://github.com/ethersphere/bee/pull/1581","keywords":""},{"title":"Backups","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/backups","content":"","keywords":""},{"title":"Files‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#files","content":"A full Bee node backup includes the keys, localstore, statestore, and password files. The node should be stopped before taking a backup and not restarted until restoring the node from the backup to prevent the node from getting out of sync with the network. Node key and state data is found in the data directory specified in its configuration. Key data in backup files allows access to Bee node's Gnosis account. If lost or stolen it could lead to the loss of all assets in that account. Multiple backups should be kept in secure locations. info Don't forget - it's not a backup until you're sure the backup files work! Make sure to test restoring from backup files to prevent loss of assets due to data loss or corruption. "},{"title":"Ubuntu / Debian / Raspbian / CentOS package managers‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#ubuntu--debian--raspbian--centos-package-managers","content":"For Linux installations from package managers yum or apt, the data directory is located at: /var/lib/bee  It may also be useful to include the bee.yaml config file in a backup so that configuration can be easily restored. The default location of the config file is: /etc/bee  "},{"title":"Binary package install‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#binary-package-install","content":"If you installed Bee using the automated shell script or by building Bee from source, your data directory will typically be located at: /home/&lt;user&gt;/.bee  "},{"title":"Docker Compose‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#docker-compose","content":"When using Docker Compose configuration files to run a node, Docker will create a volume for Bee. Use docker cp to retrieve the contents of these folders: docker cp bee_bee_1:/home/bee/.bee/ bee  "},{"title":"Data types‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#data-types","content":"The data directory contains three directories. Its default location depends on the node install method used. Shell script install: /home/&lt;user&gt;/.bee ‚îú‚îÄ‚îÄ keys ‚îÇ ‚îú‚îÄ‚îÄ libp2p.key ‚îÇ ‚îú‚îÄ‚îÄ pss.key ‚îÇ ‚îî‚îÄ‚îÄ swarm.key ‚îú‚îÄ‚îÄ localstore ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îî‚îÄ‚îÄ statestore ‚îî‚îÄ‚îÄ ...  Package manager install: /var/lib/bee ‚îú‚îÄ‚îÄ keys ‚îÇ ‚îú‚îÄ‚îÄ libp2p.key ‚îÇ ‚îú‚îÄ‚îÄ pss.key ‚îÇ ‚îî‚îÄ‚îÄ swarm.key ‚îú‚îÄ‚îÄ localstore ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îî‚îÄ‚îÄ statestore ‚îî‚îÄ‚îÄ ...  "},{"title":"Keys‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#keys","content":"The keys directory contains three key files: libp2p_v2.key, pss.key, and swarm.key. These keys are generated during the Bee node's initialisation and are the most important data to retain for a backup. danger The swarm.key file allows access to Bee node's Gnosis account. If the key is lost or stolen it could lead to the loss of all assets secured by that key. Multiple backups should be kept in secure locations to prevent loss of assets or unauthorized access. info To use swarm.key to manage the Gnosis account for a node through Metamask or other wallets, exportSwarmKeys can be used to convert swarm.key to a compatible format. "},{"title":"Statestore‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#statestore","content":"The statestore directory retains information related to the node, including SWAP balances, info on peers, block list, postage stamps, and more. info As the data in statestore and localstore continually changes during normal operation of a node, when taking a backup the node should first be stopped and not re-connected to the Swarm network until restoring from the backup (otherwise the statestore and localstore files will get out of sync with the network). It is possible to restore using out of sync statestore and localstore files, however it may lead to data loss or unexpected behavior related to chunk uploads, postage stamps, and more. "},{"title":"Localstore‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#localstore","content":"The localstore directory contains chunks locally which are frequently requested, pinned in the node, or are in the node's neighbourhood of responsibility. "},{"title":"Backup Your node‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#backup-your-node","content":"Copy entire bee data folder to fully backup node. This will do a full backup of statestore. localstore, and key files into the newly created /backup directory. Make sure to save the backup directory to a safe location. mkdir backup sudo cp -r /var/lib/bee/ backup  "},{"title":"Export keys‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#export-keys","content":"If you only need to export your node's blockchain keys, you need to export the swarm.key UTC / JSON keystore file and the password file used to encrypt it. First create a directory for your keys and then export, make sure to save the newly created keystore directory in a safe location. mkdir keystore sudo cp -r /var/lib/bee/keys/swarm.key /var/lib/bee/password keystore  "},{"title":"View key and password for wallet import‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#view-key-and-password-for-wallet-import","content":"sudo cat /var/lib/bee/keys/swarm.key sudo cat /var/lib/bee/password  info Note that swarm.key is in UTC / JSON keystores format and is encrypted by default by your password file inside the /bee directory. Make sure to export both the swarm.key file and the password file in order to secure your wallet. If you need your private key exported from the keystore file, you may use one of a variety of Ethereum wallets which support exporting private keys from UTC files (such as Metamask, however we offer no guarantees for any software, make sure you trust it completely before using it). "},{"title":"Get private key from keystore and password‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#get-private-key-from-keystore-and-password","content":"There are many tools and wallets you may use to get your private key from your keystore and password. Most Ethereum wallets which support importing accounts by keystore also include the option to export your private key, and Metamask is one of the most popular wallets for doing so. To import to Metamask: Get your swarm.key and keystore as described in the section above.Go to Metamask and click &quot;Account 1&quot; --&gt; &quot;Import Account&quot;Choose the &quot;Select Type&quot; dropdown menu and choose &quot;JSON file&quot;Paste the password (Make sure to do this first)Upload exported JSON file Click &quot;Import&quot; To export your private key: Go to Metamask and click &quot;Account 1&quot; to view the dropdown menu of all accountsClick the three dots next to the account you want to exportClick &quot;Account details&quot;Click &quot;Show private key&quot;Enter your Metamask password (not your keystore password)Copy your private key to a safe location "},{"title":"Restore from backup‚Äã","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#restore-from-backup","content":"danger Before restoring, make sure to check for any old node data at /var/lib/bee from a previous node which has not yet been backed up, and back it up if needed. Install Bee. See install page for more info. Change ownership of bee data folder. sudo chown -R /var/lib/bee Delete statestore, keys, localstore, and password files. sudo rm -r /var/lib/bee Navigate to backup directory and copy files to data folder. cp -r /&lt;path-to-backup&gt;/bee /var/lib/ Revert ownership of the data folder. sudo chown -R bee:bee /var/lib/bee Start bee service and check logs to see if Bee node is running properly. sudo systemctl restart bee sudo journalctl --lines=100 --follow --unit bee  "},{"title":"Install Bee","type":0,"sectionRef":"#","url":"/docs/bee/installation/install","content":"","keywords":""},{"title":"Recommended Hardware Specifications‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#recommended-hardware-specifications","content":""},{"title":"Full Nodes‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#full-nodes","content":"Minimum recommended specifications for each full node: Dual core, recent generation, 2ghz processor 8gb RAM30gb SSDStable internet connection HDD drives are very strongly discouraged for full nodes due to their low speeds. Note that there are additional hardware requirements if you choose to run your own Gnosis Chain node in order to provide your Bee node(s) with the required RPC endpoint. See configuration step for more details. In order to test whether a set of hardware specs is sufficient for running a full node and participating in the storage incentives redistribution, see this guide on the staking page. "},{"title":"Light and UltraLight Nodes‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#light-and-ultralight-nodes","content":"The minimum required hardware specifications for light and ultralight nodes are very low, and can be run on practically any commercially available computer or microcomputer such as a Raspberry Pi. "},{"title":"Note on Startup Methods‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#note-on-startup-methods","content":"caution The bee start startup method may not be used interchangeably with running Bee as a service using systemctl or brew services. It is strongly advised to use run Bee using a service manager such as systemctl. Bee will be set up to run as a service automatically as part of the installation process if using one of the official Debian or RPM packages. Bee may be operated either by using the bee start command within a terminal session or by running Bee as a service in the background using systemctl (Linux) and brew services (MacOS) commands. While the Bee service does use the bee start command under the hood, there are two important differences between these modes of operation in practice: When starting a node by directly using bee start after starting up a terminal session, the Bee node process is bound to that terminal session. When the session ends due to closing the terminal window or logging out from a remote ssh session, the node will stop running. When running bee as a service on the other hand, the node can continue to operate in the background even after the terminal session ends. When running a Bee node using the bee start command, a separate instance of Bee using different default locations for the config and data folders from the Bee service is used. The bee start command uses ~/.bee.yaml as the default config directory and ~/.bee as the default data directory, while systemctl uses /etc/bee/bee.yaml as the default config directory and /var/lib/bee as the default data directory. See the configuration page for more details. In general bee start may not be the best option for most users - especially if operating a full node. "},{"title":"Installation Steps‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#installation-steps","content":"Install Bee Configure BeeFind Bee AddressFund node (Not required for ultra-light nodes) Wait for InitialisationCheck Bee StatusBack Up KeysDeposit Stake (Full node only, optional) "},{"title":"1. Install Bee‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#1-install-bee","content":""},{"title":"Package manager install‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#package-manager-install","content":"Bee is available for Linux in .rpm and .deb package format for a variety of system architectures, and is available for MacOS through Homebrew. See the releases page of the Bee repo for all available packages. One of the advantages of this method is that it automatically sets up Bee to run as a service as a part of the install process. DebianRPMMacOS Get GPG key: curl -fsSL https://repo.ethswarm.org/apt/gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/ethersphere-apt-keyring.gpg Set up repo inside apt-get sources: echo \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ethersphere-apt-keyring.gpg] https://repo.ethswarm.org/apt \\ * *&quot; | sudo tee /etc/apt/sources.list.d/ethersphere.list &gt; /dev/null Install package: sudo apt-get update sudo apt-get install bee  You should see the following output to your terminal after a successful install: Reading package lists... Done Building dependency tree... Done Reading state information... Done The following NEW packages will be installed: bee 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 0 B/27.2 MB of archives. After this operation, 50.8 MB of additional disk space will be used. Selecting previously unselected package bee. (Reading database ... 82381 files and directories currently installed.) Preparing to unpack .../archives/bee_1.17.4_amd64.deb ... Unpacking bee (1.17.4) ... Setting up bee (1.17.4) ... Logs: journalctl -f -u bee.service Config: /etc/bee/bee.yaml Bee requires a Gnosis Chain RPC endpoint to function. By default this is expected to be found at ws://localhost:8546. Please see https://docs.ethswarm.org/docs/installation/install for more details on how to configure your node. After you finish configuration run 'sudo bee-get-addr' and fund your node with XDAI, and also XBZZ if so desired. Created symlink /etc/systemd/system/multi-user.target.wants/bee.service ‚Üí /lib/systemd/system/bee.service.  "},{"title":"Shell script install (Alternate method)‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#shell-script-install-alternate-method","content":"The Bee install shell script for Linux automatically detects its execution environment and installs the latest stable version of Bee. info Note that this install method copies precompiled binaries directly to the /usr/local/bin directory, so Bee installed through this method cannot be managed or uninstalled with package managers such as dpkg and rpm. Also note that unlike the package install method, this install method will not set up Bee to run as a service (such as with systemctl or brew services). Use either of the following commands to run the script and install Bee: wget‚Äã wget -q -O - https://raw.githubusercontent.com/ethersphere/bee/master/install.sh | TAG=v1.17.4 bash  curl‚Äã curl -s https://raw.githubusercontent.com/ethersphere/bee/master/install.sh | TAG=v1.17.4 bash  "},{"title":"Build from source‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#build-from-source","content":"If neither of the above methods works for your system, you can see our guide for building directly from source. "},{"title":"2. Configure Bee‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#2-configure-bee","content":"Bee is a versatile piece of software with diverse use cases. Before starting Bee for the first time you will need to configure it to suit your needs. The installation script should have generated a config file at/etc/bee/bee.yaml populated by the default configuration for the Bee service. See the configuration section for more details. Check that the file was successfully generated and contains the default configuration:  test -f /etc/bee/bee.yaml &amp;&amp; echo &quot;$FILE exists.&quot; cat /etc/bee/bee.yaml  The output should match the default bee.yaml values. If your bee.yaml file is missing, create a new one and fill it in with the default configuration copied from the Ethswarm GitHub Bee repo. sudo touch /etc/bee/bee.yaml sudo vi /etc/bee/bee.yaml  "},{"title":"Set node type‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#set-node-type","content":"Full Node, Light Node, Ultra-light Node‚Äã See the quick start guide if you're not sure which type of node to run. To run Bee as a full node both full-node and swap-enable must be set to true, and a valid and stable Gnosis Chain RPC endpoint must be specified with blockchain-rpc-endpoint. ## bee.yaml full-node: true  To run Bee as a light node full-node must be set to false and swap-enable must both be set to true, and a valid and stable Gnosis Chain RPC endpoint must be specified with blockchain-rpc-endpoint. ## bee.yaml full-node: false  To run Bee as an ultra-light node full-node and swap-enable must both be set to false. No Gnosis Chain endpoint is required, and blockchain-rpc-endpoint can be left to its default value of an empty string. ## bee.yaml full-node: false swap-enable: false  "},{"title":"Set blockchain RPC endpoint‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#set-blockchain-rpc-endpoint","content":"Full and light Bee nodes require a Gnosis Chain RPC endpoint so they can interact with and deploy their chequebook contract, see the latest view of the current postage stamp batches, and interact with and top-up postage stamp batches. A blockchain RPC endpoint is not required for nodes running in ultra-light mode. We strongly recommend you run your own Gnosis Chain node if you are planning to run a full node, and especially if you plan to run a hive of nodes. If you do not wish to run your own Gnosis Chain node and are willing to trust a third party, you may also consider using an RPC endpoint provider such as GetBlock. For running a light node or for testing out a single full node you may also consider using one of the free public RPC endpoints listed in the Gnosis Chain documentation. However the providers of these endpoints make no SLA or availability guarantees, and is therefore not recommended for full node operators. To set your RPC endpoint provider, specify it in configuration for the blockchain-rpc-endpoint value, which is set to an empty string by default. ## bee.yaml blockchain-rpc-endpoint: https://gno.getblock.io/&lt;&lt;your-api-key&gt;&gt;/mainnet/  "},{"title":"Configure Swap Initial Deposit (Optional)‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#configure-swap-initial-deposit-optional","content":"When running your Bee node with SWAP enabled for the first time, your Bee node will deploy a 'chequebook' contract using the canonical factory contract which is deployed by Swarm. A factory is used to ensure every node is using legitimate and verifiable chequebook contracts. Once the chequebook is deployed, Bee will (optionally) deposit a certain amount of xBZZ in the chequebook contract so that it can pay other nodes in return for their services. The amount of xBZZ transferred to the chequebook is set by the swap-initial-deposit configuration setting (it may be left at the default value of zero or commented out). "},{"title":"NAT address‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#nat-address","content":"Swarm is all about sharing and storing chunks of data. To enable other Bees (also known as peers) to connect to your Bee, you must broadcast your public IP address in order to ensure that Bee is reachable on the correct p2p port (default 1634). We recommend that you manually configure your external IP and check connectivity to ensure your Bee is able to receive connections from other peers. First determine your public IP address: curl icanhazip.com  123.123.123.123  Then configure your node, including your p2p port (default 1634). ## bee.yaml nat-addr: &quot;123.123.123.123:1634&quot;  "},{"title":"ENS Resolution (Optional)‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#ens-resolution-optional","content":"The ENS domain resolution system is used to host websites on Bee, and in order to use this your Bee must be connected to a mainnet Ethereum blockchain node. We recommend you run your own ethereum node. An option for resource restricted devices is geth+nimbus and a guide can be found here. Other options include dappnode, nicenode, stereum and avado. If you do not wish to run your own Ethereum node you may use a blockchain API service provider such as Infura. After signing up for Infura's API service, simply set your --resolver-options to https://mainnet.infura.io/v3/your-api-key. ## bee.yaml resolver-options: [&quot;https://mainnet.infura.io/v3/&lt;&lt;your-api-key&gt;&gt;&quot;]  "},{"title":"3. Find Bee address‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#3-find-bee-address","content":"As part of the process of starting a Bee full node or light node the node must issue a Gnosis Chain transaction which is paid for using xDAI. We therefore need to find our node's Gnosis Chain address. We can find it by reading it directly from our key file: sudo cat /var/lib/bee/keys/swarm.key  Output from cat /var/lib/bee/keys/swarm.key: {&quot;address&quot;:&quot;215693a6e6cf0a27441075fd98c31d48e3a3a100&quot;,&quot;crypto&quot;:{&quot;cipher&quot;:&quot;aes-128-ctr&quot;,&quot;ciphertext&quot;:&quot;9e2706f1ce135dde449af5c529e80d560fb73007f1edb1636efcf4572eed1265&quot;,&quot;cipherparams&quot;:{&quot;iv&quot;:&quot;64b6482b8e04881446d88f4f9003ec78&quot;},&quot;kdf&quot;:&quot;scrypt&quot;,&quot;kdfparams&quot;:{&quot;n&quot;:32768,&quot;r&quot;:8,&quot;p&quot;:1,&quot;dklen&quot;:32,&quot;salt&quot;:&quot;3da537f2644274e3a90b1f6e1fbb722c32cbd06be56b8f55c2ff8fa7a522fb22&quot;},&quot;mac&quot;:&quot;11b109b7267d28f332039768c4117b760deed626c16c9c1388103898158e583b&quot;},&quot;version&quot;:3,&quot;id&quot;:&quot;d4f7ee3e-21af-43de-880e-85b6f5fa7727&quot;}  The address field contains the Gnosis Chain address of the node, simply add the 0x prefix. danger Do not share the contents of your swarm.key or any other keys with anyone, this example is for a throwaway account. "},{"title":"4. Fund Node‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#4-fund-node","content":"info We recommend not holding a high value of xBZZ or xDAI in your nodes' wallet. Please consider regularly removing accumulated funds. Before funding your node, you first need to get some xDAI. You can send it either from your own Gnosis Chain compatible wallet such as Metamask, or from a centralized exchange which supports xDAI withdrawals to Gnosis Chain. If you already have some DAI on Ethereum, you can use the xDAI bridge to mint xDAI on Gnosis Chain. Once you have some xDAI ready, you're ready to fund your Bee node. Send at least 1 xDAI to the address you found in the previous step to fund your node. You can optionally also send some xBZZ to your node which you can use to pay for storage on Swarm. While depositing xBZZ is optional, node operators who intend to download or upload large amounts of data on Swarm may wish to deposit some xBZZ in order to pay for SWAP settlements. See the section on node funding for more information. For nodes which stake xBZZ and participate in the storage incentives system, very small amounts of xDAI will be used regularly to pay for staking related transactions on Gnosis Chain, so xDAI may need to be periodically topped up. See the staking section for more information. After sending xDAI and optionally xBZZ to the Gnosis Chain address collected in the previous step, restart the node: LinuxMacOS Linux‚Äã sudo systemctl restart bee  "},{"title":"5. Wait for Initialisation‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#5-wait-for-initialisation","content":"When first started in full or light mode, Bee must deploy a chequebook to the Gnosis Chain blockchain, and sync the postage stamp batch store so that it can check chunks for validity when storing or forwarding them. This can take a while, so please be patient! Once this is complete, you will see Bee starting to add peers and connect to the network. You can keep an eye on progress by watching the logs while this is taking place. LinuxMacOS Linux‚Äã sudo journalctl --lines=100 --follow --unit bee  If all goes well, you will see your node automatically begin to connect to other Bee nodes all over the world. INFO[2020-08-29T11:55:16Z] greeting &lt;Hi I am a very buzzy bee bzzzz bzzz bzz. üêù&gt; from peer: b6ae5b22d4dc93ce5ee46a9799ef5975d436eb63a4b085bfc104fcdcbda3b82c  Now your node will begin to request chunks of data that fall within your radius of responsibilty - data that you will then serve to other p2p clients running in the swarm. Your node will then begin to respond to requests for these chunks from other peers. Incentivisation In Swarm, storing, serving and forwarding chunks of data to other nodes can earn you rewards! Follow this guide to learn how to regularly cash out cheques other nodes send you in return for your services so that you can get your xBZZ! Your Bee client has now generated an elliptic curve key pair similar to an Ethereum wallet. These are stored in your data directory, in the keys folder. Keep Your Keys and Password Safe! Your keys and password are very important, back up these files and store them in a secure place that only you have access to. With great privacy comes great responsibility - while no-one will ever be able to guess your key - you will not be able to recover them if you lose them either, so be sure to look after them well and keep secure backups. "},{"title":"6. Check if Bee is Working‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#6-check-if-bee-is-working","content":"First check that the correct version of Bee is installed: bee version  1.17.4  Once the Bee node has been funded, the chequebook deployed, and postage stamp batch store synced, its HTTP APIwill start listening at localhost:1633. To check everything is working as expected, send a GET request to localhost port 1633. curl localhost:1633  Ethereum Swarm Bee  Great! Our API is listening! Next, let's see if we have connected with any peers by querying ourDebug API. Note that the debug api listens at port 1635 by default (localhost:1635). info Here we are using the jq utility to parse our javascript. Use your package manager to install jq, or simply remove everything after and including the first | to view the raw json without it. curl -s localhost:1635/peers | jq &quot;.peers | length&quot;  87  Perfect! We are accumulating peers, this means you are connected to the network, and ready to start using Bee to upload and download content or host and browse websites hosted on the Swarm network. Welcome to the swarm! üêù¬†üêù¬†üêù¬†üêù¬†üêù "},{"title":"7. Back Up Keys‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#7-back-up-keys","content":"Once your node is up and running, make sure to back up your keys. "},{"title":"8. Deposit Stake (Optional)‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#8-deposit-stake-optional","content":"While depositing stake is not required to run a Bee node, it is required in order for a node to receive rewards for sharing storage with the network. You will need to deposit xBZZ to the staking contract for your node. To do this, send a minimum of 10 xBZZ to your nodes' wallet and run: curl -XPOST localhost:1635/stake/100000000000000000  This will initiate a transaction on-chain which deposits the specified amount of xBZZ into the staking contract. Storage incentive rewards are only available for full nodes which are providing storage capacity to the network. Note that SWAP rewards are available to all full and light nodes, regardless of whether or not they stake xBZZ in order to participate in the storage incentives system. "},{"title":"Getting help‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#getting-help","content":"The CLI has documentation built-in. Running bee gives you an entry point to the documentation. Running bee start -h or bee start --help will tell you how you can configure your Bee node via the command line arguments. You may also check out the configuration guide, or simply run your Bee terminal command with the --help flag, eg. bee start --help or bee --help. "},{"title":"Next Steps to Consider‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#next-steps-to-consider","content":""},{"title":"Access the Swarm‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#access-the-swarm","content":"If you'd like to start uploading or downloading files to Swarm, start here. "},{"title":"Explore the API‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#explore-the-api","content":"The Bee API and Debug API are the primary methods for interacting with Bee and getting information about Bee. After installing Bee and getting it up and running, it's a good idea to start getting familiar with the APIs. "},{"title":"Run a hive!‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#run-a-hive","content":"If you would like to run a hive of many Bees, check out the hive operators section for information on how to operate and monitor many Bees at once. "},{"title":"Start building DAPPs on Swarm‚Äã","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#start-building-dapps-on-swarm","content":"If you would like to start building decentralised applications on Swarm, check out our section for developing with Bee. "},{"title":"Bcrypt hashing utility","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/bcrypt","content":"Bcrypt hashing utility In order to generate a valid admin password hash you can use any available bcrypt compatible tools, both online and offline (htpasswd). For convenience Bee also provides a method to generate and validate password hashes: $ bee bcrypt super$ecret $2a$10$eZP5YuhJq2k8DFmj9UJGWOIjDtXu6NcAQMrz7Zj1bgIVBcHA3bU5u $ bee bcrypt --check super$ecret '$2a$10$eZP5YuhJq2k8DFmj9UJGWOIjDtXu6NcAQMrz7Zj1bgIVBcHA3bU5u' OK: password hash matches provided plain text info When validating a hash don't forget about quotes - the ($) hash prefix might interfere with your terminal.","keywords":""},{"title":"Bee Tools","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/bee-tools","content":"","keywords":""},{"title":"Bee Dashboard‚Äã","type":1,"pageTitle":"Bee Tools","url":"/docs/bee/working-with-bee/bee-tools#bee-dashboard","content":"Our wonderful community (shout out to matmertz25!) has teamed up with our inimitable javascript team, the Bee Gees üï∫ , to create Bee Dashboard a graphical user interface for your Bee. Use this tool to make sure your Bee is functioning correctly, keep an eye on cheques as they accumulate, and cash them out, withdraw your earned xBZZ, and much more! Head over to the Github repo for more information on how to install and use Bee Dashboard. "},{"title":"Swarm CLI‚Äã","type":1,"pageTitle":"Bee Tools","url":"/docs/bee/working-with-bee/bee-tools#swarm-cli","content":"If you're comfortable with nodejs and the command line, we recommend you try interacting with your Bee using the mighty swarm-cli. Swarm CLI is a javascript based companion for your Bee node that can maintain multiple identities, makes it super easy to host your websites and will allow you to interact with some of Swarm's more advanced features such as feeds. Instructions on how to install and use swarm-cli are maintained at the Github repository. "},{"title":"Cashing Out","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/cashing-out","content":"Cashing Out As your Bee forwards and serves chunks to its peers, it is rewarded in BZZ in the form of cheques. Once these cheques accumulate sufficient value, you may cash them out using Bee's API. This process transfers money from your peer's chequebooks into your own, which you can then withdraw to your wallet to do with as you please! info Do not cash out your cheques too regularly! Once a week is more than sufficient! Besides the transaction costs, this prevents and relieves unnecessary congestion on the blockchain. üí© info Learn more about how SWAP and other accounting protocols work by reading The Book of Swarm . Bee contains a rich set of features to enable you to query the current accounting state of your node. First, let's query our node's current balance by sending a POST request to the balances endpoint. curl localhost:1635/chequebook/balance | jq { &quot;totalBalance&quot;: &quot;10000000&quot;, &quot;availableBalance&quot;: &quot;9640360&quot; } It is also possible to examine your per-peer balances. curl localhost:1635/balances | jq { &quot;balances&quot;: [ //... { &quot;peer&quot;: &quot;d0bf001e05014fa036af97f3d226bee253d2b147f540b6c2210947e5b7b409af&quot;, &quot;balance&quot;: &quot;-85420&quot; }, { &quot;peer&quot;: &quot;f1e2872581de18bdc68060dc8edd3aa96368eb341e915aba86b450486b105a47&quot;, &quot;balance&quot;: &quot;-75990&quot; } //... ] } In Swarm, these per-peer balances represent trustful agreements between nodes. Tokens only actually change hands when a node settles a cheque. This can either be triggered manually or when a certain threshold is reached with a peer. In this case, a settlement takes place. You may view these using the settlements endpoint. More info can be found by using the chequebook API. curl localhost:1635/settlements| jq { &quot;totalreceived&quot;: &quot;718030&quot;, &quot;totalsent&quot;: &quot;0&quot;, &quot;settlements&quot;: [ //... { &quot;peer&quot;: &quot;dce1833609db868e7611145b48224c061ea57fd14e784a278f2469f355292ca6&quot;, &quot;received&quot;: &quot;8987000000000&quot;, &quot;sent&quot;: &quot;0&quot; } //... ] } More information about the current received or sent cheques can also be found using the chequebook api. curl localhost:1635/chequebook/cheque | jq { &quot;lastcheques&quot;: [ { &quot;peer&quot;: &quot;dce1833609db868e7611145b48224c061ea57fd14e784a278f2469f355292ca6&quot;, &quot;lastreceived&quot;: { &quot;beneficiary&quot;: &quot;0x21b26864067deb88e2d5cdca512167815f2910d3&quot;, &quot;chequebook&quot;: &quot;0x4A373Db93ba54cab999e2C757bF5ca0356B42a3f&quot;, &quot;payout&quot;: &quot;8987000000000&quot; }, &quot;lastsent&quot;: null } //... ] } As our node's participation in the network increases, we will begin to see more and more of these balances arriving. In the case that we have received a settlement from another peer, we can ask our node to perform the relevant transactions on the blockchain, and cash our earnings out. To do this, we simply POST the relevant peer's address to the cashout endpoint. curl -XPOST http://localhost:1635/chequebook/cashout/d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b { &quot;transactionHash&quot;: &quot;0xba7b500e21fc0dc0d7163c13bb5fea235d4eb769d342e9c007f51ab8512a9a82&quot; } You may check the status of your transaction using the xDAI Blockscout. Finally, we can now see the status of the cashout transaction by sending a GET request to the same URL. curl http://localhost:1635/chequebook/cashout/d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b | jq { &quot;peer&quot;: &quot;d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b&quot;, &quot;chequebook&quot;: &quot;0xae315a9adf0920ba4f3353e2f011031ca701d247&quot;, &quot;cumulativePayout&quot;: &quot;179160&quot;, &quot;beneficiary&quot;: &quot;0x21b26864067deb88e2d5cdca512167815f2910d3&quot;, &quot;transactionHash&quot;: &quot;0xba7b500e21fc0dc0d7163c13bb5fea235d4eb769d342e9c007f51ab8512a9a82&quot;, &quot;result&quot;: { &quot;recipient&quot;: &quot;0x312fe7fde9e0768337c9b3e3462189ea6f9f9066&quot;, &quot;lastPayout&quot;: &quot;179160&quot;, &quot;bounced&quot;: false } } Success, we earned our first xBZZ! üêù Now we have earned tokens, to withdraw our xBZZ from the chequebook contract back into our node's own wallet, we simply POST a request to the chequebook withdraw endpoint. curl -XPOST http://localhost:1635/chequebook/withdraw\\?amount\\=1000 | jq And conversely, if we have used more services than we have provided, we may deposit extra xBZZ into the chequebook contract by sending a POST request to the deposit endpoint. curl -XPOST http://localhost:1635/chequebook/deposit\\?amount\\=1000 | jq { &quot;transactionHash&quot;: &quot;0xedc80ebc89e6d719e617a50c6900c3dd5dc2f283e1b8c447b9065d7c8280484a&quot; } You may then use Blockscout to track your transaction and make sure it completed successfully. Managing uncashed cheques For the Bee process, the final step of earning xBZZ is cashing a cheque. It is worth noting that a cheque is not yet actual xBZZ. In Bee, a cheque, just like a real cheque, is a promise to hand over money upon request. In real life, you would present the cheque to a bank. In swarm life, we present the cheque to a smart-contract. Holding on to a swap-cheque is risky; it is possible that the owner of the chequebook has issued cheques worth more xBZZ than is contained in their chequebook contract. For this reason, it is important to cash out your cheques every so often. With the set of API endpoints, as offered by Bee, it is possible to develop a script that fully manages the uncashed cheques for you. As an example, we offer you a very basic script, where you can manually cash out all cheques with a worth above a certain value. To use the script: Download and save the script: wget -O cashout.sh https://gist.githubusercontent.com/ralph-pichler/3b5ccd7a5c5cd0500e6428752b37e975/raw/cashout.sh Make the file executable: chmod +x cashout.sh List all uncashed cheques and cash out your cheques above a certain value: List: ./cashout.sh info If running ./cashout.sh returns nothing, you currently have no uncashed cheques. Cashout all cheques: ./cashout.sh cashout-all info Are you a Windows-user who is willing to help us? We are currently missing a simple cashout script for Windows. Please see theissue. info You can find the officially deployed smart-contract by the Swarm team in the swap-swear-and-swindle repository.","keywords":""},{"title":"Debug API","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/debug-api","content":"Debug API Now that you have created your Swarm wallet and your Bee node has begun to participate in the global Swarm network, we can use the Debug API to take a closer look at what's happening with your node. The Debug API provides a privileged environment where you are able to interact with your Bee node to get more information about the status of your node. danger Never expose your Debug API to the public Internet, make sure to use a firewall or bind to localhost, as we have in the example below. To use the Debug API we must first configure Bee to enable it, as it is disabled by default. bee start --debug-api-enable --debug-api-addr=localhost:1635 Checking Connectivity‚Äã First, let's check how many nodes we are currently connected to. curl -s http://localhost:1635/peers | jq '.peers | length' 23 Great! We can see that we are currently connected and sharing data with 23 other nodes! info Here we are using the jq command line utility to count the amount of objects in the peers array in the JSON response we have received from our Debug API, learn more about how to install and use jq here. Inspect Network Topology‚Äã We can gain even more insight into how your Bee is becoming a part of the global network using the topology endpoint. curl -X GET http://localhost:1635/topology | jq In this example, our node is beginning to form a healthy network. We hope to see our node adding and connecting to nodes in as many bins as possible. Learn more about proximity order bins and how your Bee node becomes part of the ordered p2p network in The Book of Swarm . { &quot;baseAddr&quot;: &quot;793cdae71d51b0ffc09fecd1c5b063560150cf2e1d55058bad4a659be5894ab1&quot;, &quot;population&quot;: 159, &quot;connected&quot;: 19, &quot;timestamp&quot;: &quot;2020-08-27T19:24:16.451187+01:00&quot;, &quot;nnLowWatermark&quot;: 2, &quot;depth&quot;: 4, &quot;bins&quot;: { &quot;bin_0&quot;: { &quot;population&quot;: 77, &quot;connected&quot;: 4, &quot;...&quot;: &quot;...&quot; }, &quot;bin_1&quot;: { &quot;population&quot;: 37, &quot;connected&quot;: 4, } } } } Find out more about what you can do with the Debug API here.","keywords":""},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/introduction","content":"","keywords":""},{"title":"Configuration‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#configuration","content":"Learn how to configure your node, and the details behind all the configuration options Bee provides. "},{"title":"Debug API‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#debug-api","content":"Access the HTTP Debug API directly for detailed information about your Bee. "},{"title":"Logs and Files‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#logs-and-files","content":"Find out where Bee stores your logs and files. "},{"title":"Bee Dashboard and Swarm CLI‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#bee-dashboard-and-swarm-cli","content":"Try out our brand new Bee Dashboard app and swarm-cli tool to monitor your Bee's status, cash out your cheques, upload data to the swarm and more! "},{"title":"Cashing Out‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#cashing-out","content":"Get your cheques cashed and bank your xBZZ. See this guide to receiving payments from your peers. "},{"title":"Monitoring and Metrics‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#monitoring-and-metrics","content":"There is a lot going on inside Bee, we provide tools and metrics to help you find out what's going on. "},{"title":"Backups‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#backups","content":"Keep your important data safe, Bee stores important state and key information on your hardrive, make sure you keep a secure copy in case of disaster. "},{"title":"Upgrading‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#upgrading","content":"Find out how to keep your Bee up to date with the latest and greatest releases, and make sure you're tuned into our release announcements. "},{"title":"Uninstalling Bee‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#uninstalling-bee","content":"We hope you won't need to remove Bee. If you do, please let us know if you had issues so we can help resolve them for our beloved network. Here's the guide to removing Bee from your system. "},{"title":"Light Nodes","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/light-nodes","content":"Light Nodes info When a light node is requesting data from the network - it will not benefit from plausible deniability. This is because a light node does not forward on behalf of other nodes, and so it is always the originator of the request. Configuration‚Äã To run Bee as a light node full-node must be set to false and swap-enable must be set to true, and a stable Gnosis Chain RPC endpoint URL must be specified with blockchain-rpc-endpoint in the configuration. Mode of Operation‚Äã At present, light mode represents a pragmatic and elegant approach to improving network stability, reliability and resilience. In general, light mode may be thought of as simply not participating in the activity of forwarding or storing chunks for other members of the swarm, these nodes are strictly consumers, who will pay xBZZ in return for services rendered by full nodes - those contributing towards moving data around the network. This means that although the node will participate in the pull syncing protocol by filling up its local storage with the chunks closest to its overlay address, the node will not serve these chunks to other peers. Additionally, a light node will not participate in the forwarding protocol, as it will not forward chunks to peers closer to the destination address.","keywords":""},{"title":"Logs and Files","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/logs-and-files","content":"","keywords":""},{"title":"Linux‚Äã","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#linux","content":"If you have installed Bee on Linux using a package manager you will now be able to manage your Bee service using systemctl. systemctl status bee  ‚óè bee.service - Bee - Ethereum Swarm node Loaded: loaded (/lib/systemd/system/bee.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2020-11-20 23:50:15 GMT; 6s ago  Logs are available using the journalctl command: journalctl --lines=100 --follow --unit bee  INFO[2021-02-09T18:55:11Z] swarm public key 03379f7aa673b7f03737064fd23ba1453619924a4602e70bbccc133ba67d0968bd DEBU[2021-02-09T18:55:11Z] using existing libp2p key DEBU[2021-02-09T18:55:11Z] using existing pss key INFO[2021-02-09T18:55:11Z] pss public key 03bae655ce94431e1f2c2de8d017f88c8c5c293ef0057379223084aba9e318596e INFO[2021-02-09T18:55:11Z] using ethereum address 99c9e7868d22244106a5ffbc2f5d6b7c88e2c85a INFO[2021-02-09T18:55:14Z] using default factory address for chain id 5: f0277caffea72734853b834afc9892461ea18474 INFO[2021-02-09T18:55:14Z] no chequebook found, deploying new one. WARN[2021-02-09T18:55:15Z] cannot continue until there is sufficient ETH (for Gas) and at least 10 BZZ available on 99c9e7868d22244106a5ffbc2f5d6b7c88e2c85a  "},{"title":"MacOS‚Äã","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#macos","content":"Services are managed using Homebrew services. brew services restart swarm-bee  Logs are available at /usr/local/var/log/swarm-bee/bee.log tail -f /usr/local/var/log/swarm-bee/bee.log  "},{"title":"Data Locations‚Äã","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#data-locations","content":""},{"title":"Bee‚Äã","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#bee","content":"Configuration files are stored in /etc/bee/. State, chunks and other data are stored in /var/lib/bee/ "},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/configuration","content":"","keywords":""},{"title":"Configuration for Bee Service‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#configuration-for-bee-service","content":"Note that Bee is only set up to run as a service by default when it is installed using one of the officially supported Linux Debian or RPM packages or the Homebrew installer for MacOS. "},{"title":"Default Data and Config Directories‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#default-data-and-config-directories","content":"When running Bee as a service /etc/bee/bee.yaml is used as the default config directory and /var/lib/bee as the default data directory. "},{"title":"Change Default Config Directory‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#change-default-config-directory","content":"Add the --config flag to bee start to specify a config file with another location or file name. bee start --config /&lt;path-to-config&gt;/&lt;config-filename&gt;.yaml  "},{"title":"Change Default Config‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#change-default-config","content":"Configuration for the Bee service should not be set through command line arguments and environment variables as it is with the bee start command. To change configuration, simply edit the yaml file and restart Bee: Linux‚Äã sudo vi /etc/bee/bee.yaml sudo systemctl restart bee  MacOS‚Äã vi /usr/local/etc/swarm-bee/bee.yaml brew services restart swarm-bee  "},{"title":"Manually generate config for Bee service‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#manually-generate-config-for-bee-service","content":"If the config file is accidentally deleted or missing, it can be manually generated: bee printconfig  Copy the output and save in /etc/bee/bee.yaml: sudo vi /etc/bee/bee.yaml  "},{"title":"Configuration for bee start‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#configuration-for-bee-start","content":""},{"title":"Default Data and Config Directories‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#default-data-and-config-directories-1","content":"The bee start command uses ~/.bee.yaml as the default config directory and ~/.bee as the default data directory. "},{"title":"Manually generate config for Bee start‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#manually-generate-config-for-bee-start","content":"No configuration file is generated automatically at the default config directory for bee start during the Bee installation process so it must be manually generated. It can be easily generated with the following command: For bee start: bee printconfig &amp;&gt; $HOME/.bee.yaml  This produces the following file contents, showing the default configuration of Bee: ## Bee configuration - https://docs.ethswarm.org/docs/bee/working-with-bee/configuration ## Bee configuration - https://docs.ethswarm.org/docs/working-with-bee/configuration ## HTTP API listen address (default &quot;:1633&quot;) # api-addr: :1633 ## chain block time (default 15) # block-time: 15 ## initial nodes to connect to (default [/dnsaddr/mainnet.ethswarm.org]) # bootnode: [/dnsaddr/mainnet.ethswarm.org] ## cause the node to always accept incoming connections # bootnode-mode: false ## config file (default is /home/&lt;user&gt;/.bee.yaml) config: /etc/bee/bee.yaml ## origins with CORS headers enabled # cors-allowed-origins: [] ## data directory (default &quot;/home/&lt;user&gt;/.bee&quot;) data-dir: /var/lib/bee ## cache capacity in chunks, multiply by 4096 to get approximate capacity in bytes # cache-capacity: 1000000 ## number of open files allowed by database # db-open-files-limit: 200 ## size of block cache of the database in bytes # db-block-cache-capacity: 33554432 ## size of the database write buffer in bytes # db-write-buffer-size: 33554432 ## disables db compactions triggered by seeks # db-disable-seeks-compaction: false ## debug HTTP API listen address (default &quot;:1635&quot;) debug-api-addr: 127.0.0.1:1635 ## enable debug HTTP API debug-api-enable: true ## cause the node to start in full mode # full-node: false ## NAT exposed address # nat-addr: &quot;&quot; ## ID of the Swarm network (default 1) # network-id: 1 ## P2P listen address (default &quot;:1634&quot;) # p2p-addr: :1634 ## enable P2P WebSocket transport # p2p-ws-enable: false ## password for decrypting keys # password: &quot;&quot; ## path to a file that contains password for decrypting keys password-file: /var/lib/bee/password ## percentage below the peers payment threshold when we initiate settlement (default 50) # payment-early-percent: 50 ## threshold in BZZ where you expect to get paid from your peers (default 100000000) # payment-threshold: 100000000 ## excess debt above payment threshold in percentages where you disconnect from your peer (default 25) # payment-tolerance-percent: 25 ## postage stamp contract address # postage-stamp-address: &quot;&quot; ## ENS compatible API endpoint for a TLD and with contract address, can be repeated, format [tld:][contract-addr@]url # resolver-options: [] ## enable swap (default true) # swap-enable: true ## swap blockchain endpoint (default &quot;&quot;) [deprecated] # swap-endpoint: &quot;&quot; ## blockchain endpoint (default &quot;&quot;) # blockchain-rpc-endpoint: &quot;&quot; ## swap factory address # swap-factory-address: &quot;&quot; ## legacy swap factory addresses # swap-legacy-factory-addresses: &quot;&quot; ## initial deposit if deploying a new chequebook (default 0) # swap-initial-deposit: 0 ## gas price in wei to use for deployment and funding (default &quot;&quot;) # swap-deployment-gas-price: &quot;&quot; ## enable tracing # tracing-enable: false ## endpoint to send tracing data (default &quot;127.0.0.1:6831&quot;) # tracing-endpoint: 127.0.0.1:6831 ## service name identifier for tracing (default &quot;bee&quot;) # tracing-service-name: bee ## proof-of-identity transaction hash # transaction: &quot;&quot; ## log verbosity level 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace (default &quot;info&quot;) # verbosity: info ## send a welcome message string during handshakes # welcome-message: &quot;&quot; ## triggers connection to main network # mainnet: true  "},{"title":"Configuration Priority‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#configuration-priority","content":"Configuration is processed in the following ascending order of preference when using bee start to run a Bee node: Command Line ArgumentsEnvironment VariablesConfiguration File "},{"title":"Command Line Arguments‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#command-line-arguments","content":"Run bee start --help in your Terminal to list the available command line arguments as follows: Start a Swarm node Usage: bee start [flags] Flags: --admin-password string bcrypt hash of the admin password to get the security token --allow-private-cidrs allow to advertise private CIDRs to the public network --api-addr string HTTP API listen address (default &quot;:1633&quot;) --block-time uint chain block time (default 15) --blockchain-rpc-endpoint string rpc blockchain endpoint --bootnode strings initial nodes to connect to --bootnode-mode cause the node to always accept incoming connections --cache-capacity uint cache capacity in chunks, multiply by 4096 to get approximate capacity in bytes (default 1000000) --cache-retrieval enable forwarded content caching (default true) --chequebook-enable enable chequebook (default true) --cors-allowed-origins strings origins with CORS headers enabled --data-dir string data directory (default &quot;/home/noah/.bee&quot;) --db-block-cache-capacity uint size of block cache of the database in bytes (default 33554432) --db-disable-seeks-compaction disables db compactions triggered by seeks --db-open-files-limit uint number of open files allowed by database (default 200) --db-write-buffer-size uint size of the database write buffer in bytes (default 33554432) --debug-api-addr string debug HTTP API listen address (default &quot;:1635&quot;) --debug-api-enable enable debug HTTP API --full-node cause the node to start in full mode -h, --help help for start --mainnet triggers connect to main net bootnodes. (default true) --nat-addr string NAT exposed address --network-id uint ID of the Swarm network (default 1) --p2p-addr string P2P listen address (default &quot;:1634&quot;) --p2p-ws-enable enable P2P WebSocket transport --password string password for decrypting keys --password-file string path to a file that contains password for decrypting keys --payment-early-percent int percentage below the peers payment threshold when we initiate settlement (default 50) --payment-threshold string threshold in BZZ where you expect to get paid from your peers (default &quot;13500000&quot;) --payment-tolerance-percent int excess debt above payment threshold in percentages where you disconnect from your peer (default 25) --postage-stamp-address string postage stamp contract address --postage-stamp-start-block uint postage stamp contract start block number --pprof-mutex enable pprof mutex profile --pprof-profile enable pprof block profile --price-oracle-address string price oracle contract address --redistribution-address string redistribution contract address --resolver-options strings ENS compatible API endpoint for a TLD and with contract address, can be repeated, format [tld:][contract-addr@]url --restricted enable permission check on the http APIs --resync forces the node to resync postage contract data --staking-address string staking contract address --static-nodes strings protect nodes from getting kicked out on bootnode --storage-incentives-enable enable storage incentives feature (default true) --swap-deployment-gas-price string gas price in wei to use for deployment and funding --swap-enable enable swap (default true) --swap-endpoint string swap blockchain endpoint --swap-factory-address string swap factory addresses --swap-initial-deposit string initial deposit if deploying a new chequebook (default &quot;0&quot;) --swap-legacy-factory-addresses strings legacy swap factory addresses --token-encryption-key string admin username to get the security token --tracing-enable enable tracing --tracing-endpoint string endpoint to send tracing data (default &quot;127.0.0.1:6831&quot;) --tracing-host string host to send tracing data --tracing-port string port to send tracing data --tracing-service-name string service name identifier for tracing (default &quot;bee&quot;) --use-postage-snapshot bootstrap node using postage snapshot from the network --verbosity string log verbosity level 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace (default &quot;info&quot;) --warmup-time duration time to warmup the node before some major protocols can be kicked off. (default 5m0s) --welcome-message string send a welcome message string during handshakes Global Flags: --config string config file (default is $HOME/.bee.yaml)  "},{"title":"Environment variables‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#environment-variables","content":"Bee config may also be passed using environment variables. Environment variables are set as variables in your operating system's session or systemd configuration file. To set an environment variable, type the following in your terminal session. export VARIABLE_NAME=variableValue  Verify if it is correctly set by running echo $VARIABLE_NAME. All available configuration options are available as BEE prefixed, capitalised, and underscored environment variables, e.g. --api-addr becomes BEE_API_ADDR. "},{"title":"Configuration Options‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#configuration-options","content":"Bee provides the following options to customise your node. "},{"title":"Global‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#global","content":"--config‚Äã default /home/&lt;user&gt;/.bee.yaml The location of a YAML configuration file containing configuration options. See configuration. "},{"title":"Start‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#start","content":"--admin-password‚Äã When the permission checks for the API is enabled then this option configure admin password that is used to generate Bearer tokens. Be aware that you need to pass a bcrypt hash of the password here not the actual plaintext password! default &quot;&quot; --allow-private-cidrs: false‚Äã default &quot;&quot; --api-addr‚Äã default :1633 The IP and port the API will serve HTTP requests from. Omitting the IP part of the address will cause the server to listen to all interfaces. Argument values are of the form '132.132.132.132:1633'. --block-time‚Äã default 15 The expected block time of the attached SWAP endpoint. --block-hash‚Äã default &quot;&quot; The block hash of the block whose parent is the block that contains the transaction hash --blockchain-rpc-endpoint‚Äã default &quot;&quot; Gnosis Chain (mainnet) or Goerli (testnet) blockchain endpoint. Leave unset to boot in ultra-light (chainless) mode. --bootnode‚Äã default /dnsaddr/mainnet.ethswarm.org This is a multiaddrspecifying the Bee bootnodes used for bootstrapping the network. It can be multiple values. By default a node connects to the Swarm mainnet. When using a private or test network, network specific bootnodes must be set. Any Bee node in a network can act as a bootnode. --bootnode-mode‚Äã default false Cause the node to always accept incoming connections --cache-capacity‚Äã default 1000000 The amount of disk space, in chunks, that is used for forwarding and uploading chunks. --cache-retrieval‚Äã default true Enable the caching of forwarded content. --chequebook-enable‚Äã default true Enable chequebook. --cors-allowed-origins‚Äã default [] HTTP/WS origin domains or wildcards defining these, which the API will allow responses to, e.g. bee start --cors-allowed-origins=&quot;*&quot; bee start --cors-allowed-origins=&quot;https://website.ethswarm.org&quot;  --data-dir‚Äã default /home/&lt;user&gt;/.bee The location on your disk where Bee stores its data. Data in this directory will be required to restore a node state using the same key. This consists of the following three types of data. 1. Chunk Data (localstore)‚Äã This consists of chunks and files that you have pinned locally, cached chunks you have requested, or chunks within your radius of responsibility which you are responsible for serving to your peers. 2. State Data (statestore)‚Äã This is information about the local state of your Bee node and should be backed up. 3. Keystore Data (keys)‚Äã These files contain encrypted versions of your private key and should be backed up and kept private. danger Keep the key files in your keystore data directory safe! They are the cryptographic proof of your network identity and cannot be recovered. The next four options expose low-level configurations forLevelDB's Openfilemethod. Please let us know how you get on with tweaking these settings on your hardware in the#node-operators channel on ourDiscord server --db-block-cache-capacity‚Äã default 33554432 Corresponds to LevelDB BlockCacheCapacity (see above) --db-disable-seeks-compaction‚Äã default false Corresponds to LevelDB DisableSeeksCompaction (see above) --db-open-files-limit‚Äã default 200 info To accommodate less powerful hardware and operating systems, the db-open-files-limit is set deliberately low. We recommend that you try to increase it to nearer to 10000 or more if this is possible when using your hardware. Please let us know how you get on with tweaking these settings on your hardware in the #node-operators channel on our Discord server Corresponds to LevelDB OpenFilesCacheCapacity (see above) --db-write-buffer-size‚Äã default 33554432 Corresponds to LevelDB WriteBuffer (see above) --debug-api-addr‚Äã default :1635 The IP and port the Debug APIwill serve HTTP requests from. Omitting the IP part of the address will cause the server to listen to all requests. --debug-api-enable must be set to true. --debug-api-enable‚Äã default false Set this to true to enable access to the Debug API --full-node‚Äã default false Enable this by setting it to true to fully participate in serving and forwarding chunks to the network. --mainnet‚Äã default true Set to false to connect to the Swarm testnet, or other networks. Note that if you do so then you'll need to specify some bootnodes using the --bootnode argument. --nat-addr‚Äã default &quot;&quot; Sets the expected public IP. Normally this is generated automatically, but in certain circumstances it may be desirable to set it manually. Format is 123.123.123.123:1634 where the port number is your Bee p2p port. --network-id‚Äã default 1 if --mainnet=truedefault 10 if --mainnet=false The network ID for which to accept new connections. Set to 1 for mainnet, 10 for testnet. --p2p-addr‚Äã default :1634 The IP and port to listen for p2p protocol messages. --p2p-ws-enable‚Äã default false Enables web-sockets transport for p2p communications. --password‚Äã default &quot;&quot; Password used to decrypt Swarm identity keys. danger Passing passwords as command line arguments is insecure. Use a password file or environment variable in production environments. --password-file‚Äã default &quot;&quot; The path to a file that contains password for decrypting keys. The empty string assumes no file is presented. --payment-early-percent‚Äã default 50 Percentage below the peers payment threshold when we initiate settlement. --payment-threshold‚Äã default 13500000 The threshold in BZZ where you expect to get paid from your peers. --payment-tolerance-percent‚Äã default 25 Excess debt above payment threshold as a percentage where you disconnect from your peer (default 25). --postage-stamp-start-block‚Äã default 0 The block number of the deployed postage stamp contract. --postage-stamp-address‚Äã default automatically configured depending on network The address of the postage stamp contract on the Ethereum blockchain, used for buying batches of stamps. --resolver-options‚Äã default eth:0x00000000000C2E074eC69A0dFb2997BA6C7d2e1e@localhost:8545 ENS API endpoint for a TLD, with contract address. Multiple values can be provided. Settings should be provided in the format [tld:][contract-addr@]url A default top level domain and resolver contract address are provided, but an ENS/Geth endpoint must be provided to enable this functionality. --restricted‚Äã default false Enable permission check on certain http APIs. More information on how to restrict the access to the APIs is available here. If enabled - you must specify an admin password using the --admin-password option and a --token-encryption-key string value. To generate a valid admin password use the provided bcrypt utility --resync‚Äã default false Forces the node to resync postage contract data. --static-nodes‚Äã default [] Protect nodes from getting kicked out on bootnode. --storage-incentives-enable‚Äã default false Enables the storage incentives feature. --swap-deployment-gas-price‚Äã default determined automatically Gas price in wei to use for deployment and funding --swap-enable‚Äã default true --blockchain-endpoint‚Äã default &quot;&quot; Deprecated, use --blockchain-rpc-endpoint instead. SWAP Gnosis Chain (mainnet) or Goerli (testnet) blockchain endpoint. Leave unset to boot in ultra-light (chainless) mode. --swap-factory-address‚Äã default anointed contract for the current blockchain id --swap-initial-deposit‚Äã default 0 --token-encryption-key‚Äã default Admin username to get the security token. --tracing-enable‚Äã default false Send tracing spans to the tracing service. More information on how to configure and visualize tracing data is availablehere. --tracing-endpoint‚Äã default 127.0.0.1:6831 The URL where the tracing service listens for Thrift protocol UDP messages. --tracing-host‚Äã default &quot;&quot; Host to send tracing data. --tracing-port‚Äã default &quot;&quot; Port to send tracing data. --tracing-service-name‚Äã default bee Bee service identifier in tracing spans. --transaction‚Äã default &quot;&quot; As a spam prevention measure, for nodes which deployed their chequebook with v0.5.0 or before, specify transaction - the transaction hash of any Ethereum transaction on the xDAI networksent from the Bee node's Ethereum address. --verbosity‚Äã default info 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace --warmup-time‚Äã default 5m0s Time to warmup the node before pull/push protocols can be kicked off. --welcome-message‚Äã default &quot;&quot; Custom welcome message to be displayed to peers on succesful connection. "},{"title":"Monitoring Your Node","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/monitoring","content":"Monitoring Your Node Your Bee node is equipped with tools to help you understand what your Bee has been up to! Navigate to http://localhost:1635/metrics. This is the current state of Bee's metrics as they stand at this moment. In order to use these metrics and view, we need to keep a record of these metrics over time. To do this we will use Prometheus. Simply install, configure as follows, and restart! For Ubuntu and other Debian based Linux distributions install using apt: sudo apt install prometheus And configure localhost:1635 as a target in the static_configs. sudo vim /etc/prometheus/prometheus.yml static_configs: - targets: [&quot;localhost:9090&quot;, &quot;localhost:1635&quot;] Navigate to http://localhost:9090 to see the Prometheus user interface. Now that our metrics are being scraped into Prometheus' database, we can use it as a data source which is used by Grafana to display the metrics as a time series graph on the dashboard. Type bee_ in the 'expression' or 'metrics' field in Prometheus or Grafana respectively to see the list of metrics available. Here's a few to get you started! rate(bee_swap_cheques_received[1d]) rate(bee_swap_cheques_sent[1d]) rate(bee_swap_cheques_rejected[1d]) Share your creations in the #node-operators channel of our Discord server!","keywords":""},{"title":"Staking","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/staking","content":"","keywords":""},{"title":"Adding stake‚Äã","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#adding-stake","content":"Bee has builtin endpoints for depositing the stake. Currently the minimum staking requirement is 10 xBZZ, so make sure that there is enough tokens in the node's wallet and you must have some native token as well for paying the gas. Then you can run the following command to stake 10 xBZZ. The amount is given in PLUR which is the smallest denomination of xBZZ and 1 xBZZ == 1e16 PLUR. curl -XPOST localhost:1635/stake/100000000000000000  If the command executed successfully it returns a transaction hash that you can use to verify on a block explorer. It is possible to deposit more by repeatedly using the command above. You can also check the amount staked with the following command: curl localhost:1635/stake  "},{"title":"Check redistribution status‚Äã","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#check-redistribution-status","content":"Use the RedistributionState endpoint of the API to get more information about the redistribution status of the node. curl -X GET http://localhost:1635/redistributionstate | jq  { &quot;minimumFunds&quot;: &quot;18750000000000000&quot;, &quot;hasSufficientFunds&quot;: true, &quot;isFrozen&quot;: false, &quot;isFullySynced&quot;: true, &quot;phase&quot;: &quot;commit&quot;, &quot;round&quot;: 176319, &quot;lastWonRound&quot;: 176024, &quot;lastPlayedRound&quot;: 176182, &quot;lastFrozenRound&quot;: 0, &quot;block&quot;: 26800488, &quot;reward&quot;: &quot;10479124611072000&quot;, &quot;fees&quot;: &quot;30166618102500000&quot; }  &quot;minimumFunds&quot;: &lt;integer&gt; - The minimum xDAI needed to play a single round of the redistribution game (the unit is 1e-18 xDAI).&quot;hasSufficientFunds&quot;: &lt;bool&gt; - Shows whether the node has enough xDAI balance to submit at least five storage incentives redistribution related transactions. If false the node will not be permitted to participate in next round.&quot;isFrozen&quot;: &lt;bool&gt; - Shows node frozen status.&quot;isFullySynced&quot;: &lt;bool&gt; - Shows whether node's localstore has completed full historical syncing with all connected peers.&quot;phase&quot;: &lt;string&gt; - Current phase of redistribution game (commit, reveal, or claim).&quot;round&quot;: &lt;integer&gt; - Current round of redistribution game. The round number is determined by dividing the current Gnosis Chain block height by the number of blocks in one round. One round takes 152 blocks, so using the &quot;block&quot; output from the example above we can confirm that the round number is 176319 (block 26800488 / 152 blocks = round 176319). &quot;lastWonRound&quot;: &lt;integer&gt; - Number of round last won by this node.&quot;lastPlayedRound&quot;: &lt;integer&gt; - Number of the last round where node's neighborhood was selected to participate in redistribution game.&quot;lastFrozenRound&quot;: &lt;integer&gt; The number the round when node was last frozen. &quot;block&quot;: &lt;integer&gt; - Gnosis block of the current redistribution game.&quot;reward&quot;: &lt;string (BigInt)&gt; - Record of total reward received in PLUR.&quot;fees&quot;: &lt;string (BigInt)&gt; - Record of total spent in 1E-18 xDAI on all redistribution related transactions. danger Nodes should not be shut down or updated in the middle of a round they are playing in as it may cause them to lose out on winnings or become frozen. To see if your node is playing the current round, check if lastPlayedRound equals round in the output from the /redistributionstate endpoint. "},{"title":"Check node performance‚Äã","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#check-node-performance","content":"One of the most common issues affecting staking is the sampler process failing. The sampler is a resource intensive process which is run by nodes which are selected to take part in redistribution. The process may fail or time out if the node's hardware specifications aren't high enough. To check a node's performance the /rchash/{depth}/{anchor} endpoint of the API may be used. Before proceeding, first check that the node is fully synced, is not frozen, and has sufficient funds to participate in staking. To check node sync status, call the redistributionstate endpoint: curl -X GET http://localhost:1635/redistributionstate | jq  Response: { &quot;minimumFunds&quot;: &quot;18750000000000000&quot;, &quot;hasSufficientFunds&quot;: true, &quot;isFrozen&quot;: false, &quot;isFullySynced&quot;: true, &quot;phase&quot;: &quot;commit&quot;, &quot;round&quot;: 176319, &quot;lastWonRound&quot;: 176024, &quot;lastPlayedRound&quot;: 176182, &quot;lastFrozenRound&quot;: 0, &quot;block&quot;: 26800488, &quot;reward&quot;: &quot;10479124611072000&quot;, &quot;fees&quot;: &quot;30166618102500000&quot; }  Confirm that hasSufficientFunds is true, and isFullySynced is true before moving to the next step. If hasSufficientFunds if false, make sure to add at least the amount of xDAI shown in minimumFunds (unit of 1e-18 xDAI). If the node was recently installed and isFullySynced is false, wait for the node to fully sync before continuing. After confirming the node's status, continue to the next step. The {anchor} value can be set to any random string. To get the {depth} value, first call the topology debug API endpoint using jq to filter for the depth `value: sudo curl -sX GET http://localhost:1635/topology | jq '.depth'  Output: 8  Call the endpoint using the value returned (8 in our example) like so: sudo curl -X GET http://localhost:1633/rchash/8/randomstring | jq  If the sampler runs successfully, you should see output like this:  % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 659 100 659 0 0 3 0 0:03:39 0:02:54 0:00:45 161 { &quot;Sample&quot;: { &quot;Items&quot;: [ &quot;000003dac2b2f75842e410474dfa4c1e6e0b9970d81b57b33564c5620667ba96&quot;, &quot;00000baace30916f7445dbcc44d9b55cb699925acfbe157e4498c63bde834f40&quot;, &quot;0000126f48fb1e99e471efc683565e4b245703c922b9956f89cbe09e1238e983&quot;, &quot;000012db04a281b7cc0e6436a49bdc5b06ff85396fcb327330ca307e409d2a04&quot;, &quot;000014f365b1a381dda85bbeabdd3040fb1395ca9e222e72a597f4cc76ecf6c2&quot;, &quot;00001869a9216b3da6814a877fdbc31f156fc2e983b52bc68ffc6d3f3cc79af0&quot;, &quot;0000198c0456230b555d5261091cf9206e75b4ad738495a60640b425ecdf408f&quot;, &quot;00001a523bd1b688472c6ea5a3c87c697db64d54744829372ac808de8ec1d427&quot; ], &quot;Hash&quot;: &quot;7f7d93c6235855fedc34e32c6b67253e27910ca4e3b8f2d942efcd758a6d8829&quot; }, &quot;Time&quot;: &quot;2m54.087909745s&quot; }  If the Time value is higher than 6 minutes, then the hardware specifications for the node may need to be upgraded. If there is an evictions related error such as the one below, try running the call to the /rchash/{depth}/{anchor} endpoint again: error: &quot;level&quot;=&quot;error&quot; &quot;logger&quot;=&quot;node/storageincentives&quot; &quot;msg&quot;=&quot;make sample&quot; &quot;error&quot;=&quot;sampler: failed creating sample: sampler stopped due to ongoing evictions&quot;  While evictions are a normal part of Bee's standard operation, the event of an eviction will interrupt the sampler process. If you are still experiencing problems, you can find more help in the node-operators Discord channel (for your safety, do not accept advice from anyone sending a private message on Discord). "},{"title":"Security","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/security","content":"","keywords":""},{"title":"Calling the restricted APIs‚Äã","type":1,"pageTitle":"Security","url":"/docs/bee/working-with-bee/security#calling-the-restricted-apis","content":""},{"title":"Getting the security token‚Äã","type":1,"pageTitle":"Security","url":"/docs/bee/working-with-bee/security#getting-the-security-token","content":"In order to call any of the restricted APIs the requests have to have a &quot;Bearer&quot; Authorization header with a valid security token. The security token can be acquired by calling the /auth HTTP endpoint that is protected using basic authentication To call this endpoint you need to generate an authorization header based on our password. That is achieved by base64 encoding our password prepended with a colon &quot;:&quot; character (since username is empty in our case). So if our password is s3cret then you need to base64(:s3cret) The payload is in a json format containing two fields: roleexpiry The role field can have one of the four values: maintainercreatorauditorconsumer The expiry field is a numeric value representing the lifetime of the token (in seconds) info There is an inheritance relationship between roles: consumer ‚äÜ creator ‚äÜ accountant ‚äÜ maintainer Maintainer role is universal - it's the superset of all permissions belonging to other roles. Example‚Äã Let's say our password is hello By base64 encoding :hello we get the value OmhlbGxv this is our basic authorization header value that we can use in our payload: POST {{api}}/auth Content-Type: application/json Authorization: Basic OmhlbGxv { &quot;role&quot;: &quot;maintainer&quot;, &quot;expiry&quot;: 3600 }  Sample response: { &quot;key&quot;:&quot;A1UQrbNUK22otp50EsESoJNYkJfrK9H1D4ex4gSWUddx3H/A9VCu8ltS8lVmvSTzoNA==&quot; }  "},{"title":"Refreshing the security token‚Äã","type":1,"pageTitle":"Security","url":"/docs/bee/working-with-bee/security#refreshing-the-security-token","content":"In order to refresh the token we need to call the /refresh endpoint in the same manner as when we are calling any other restricted API endpoint - by putting valid existing token in the http request: POST {{api}}/refresh Content-Type: application/json Authorization: Bearer A1UQrbNUK22otp50EsESoJNYkJfrK9H1D4ex4gSWUddx3H/A9VCu8ltS8lVmvSTzoNA== { &quot;role&quot;: &quot;maintainer&quot;, &quot;expiry&quot;: 86400 }  In the payload you can specify a different role and token lifetime. "},{"title":"A note on the HTTP endpoints‚Äã","type":1,"pageTitle":"Security","url":"/docs/bee/working-with-bee/security#a-note-on-the-http-endpoints","content":"There are three major groups: technical debug endpoints /readiness/node/addresses/chainstate/debug/pprof/debug/vars/health business related endpoints residing on the debug port /peers/pingpong/{peer-id}/reservestate/connect/{address}/blocklist/peers/{address}/chunks/{address}/topology/welcome-message/balances/balances/{peer}/consumed/consumed/{peer}/timesettlements/settlements/settlements/{peer}/chequebook/cheque/{peer}/chequebook/cheque/chequebook/cashout/{peer}/chequebook/balance/chequebook/address/chequebook/deposit/chequebook/withdraw/wallet/stamps/stamps/{id}/stamps/{id}/buckets/stamps/{amount}/{depth}/stamps/topup/{id}/{amount}/stamps/dilute/{id}/{depth}/batches/tags/{id} API endpoints /bytes/bytes/{address}/chunks/chunks/stream/chunks/{address}/soc/{owner}/{id}/feeds/{owner}/{topic}/bzz/bzz/{address}/bzz/{address}/{path}/pss/send/{topic}/{targets}/pss/subscribe/{topic}/tags/tags/{id}/pins/pins/{reference}/stewardship/{address}/auth/refresh The user can toggle the debug port by setting the appropriate boolean value on debug-api-enable configuration parameter. If the value is false (default) then the first two groups of endpoints will not be available. Once we toggle the restricted flag to true - two things are going to happen: it will expose the debug business related endpoints on the API portit will restrict the access to them based on the security configuration provided. info Toggling the restricted flag ON will not remove the business related endpoints from the debug port, nor will it restrict them there. "},{"title":"The order in which HTTP endpoints become available‚Äã","type":1,"pageTitle":"Security","url":"/docs/bee/working-with-bee/security#the-order-in-which-http-endpoints-become-available","content":"The technical debug endpoints group will be the first to become available - as soon as its dependencies come online (within seconds). The other two groups will become available at a later stage, specifically after the postage syncing is done. Requests to a non technical debug endpoint before it becomes available will be rejected with a 404 http response code. "},{"title":"Ultra Light Nodes","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/ultra-light-nodes","content":"Ultra Light Nodes info When running without a blockchain connection, bandwidth incentive payments (SWAP) cannot be made so there is a risk of getting blocklisted by other peers for unpaid services. Configuration‚Äã In order to run an ultra-light node use the same configuration as for the light node but leave the blockchain-rpc-endpoint parameter value to empty (or just comment it out). caution Make sure you set the swap-enable configuration parameter to false, otherwise you will get an error. Mode of Operation‚Äã The target audience for this mode of operations are users who want to try out running a node but don't want to go through the hassle of blockchain onboarding. Ultra-light nodes will be able to download data as long as the data consumed does not exceed the payment threshold (payment-threshold in configuration) set by peers they connect to. Running Bee without a connected blockchain backend, however, imposes some limitations: Can't do overlay verificationCan't do SWAP settlements Since we can't buy postage stamps: Can't send PSS messagesCan't upload data to the network","keywords":""},{"title":"Uninstalling Bee","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/uninstalling-bee","content":"","keywords":""},{"title":"Uninstalling Bee‚Äã","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#uninstalling-bee","content":"Choose the appropriate uninstall method based on the install method used: "},{"title":"Package Manager Install‚Äã","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#package-manager-install","content":"This method can be used for package manager based installs of the official Debian, RPM, and Homebrew packages. Debian‚Äã sudo apt-get remove bee  RPM‚Äã sudo yum remove bee  "},{"title":"Binary Install‚Äã","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#binary-install","content":"If Bee is installed using the automated shell script or by building from source, Bee can be uninstalled by directly removing the installed file. sudo rm `/usr/local/bin/bee`  "},{"title":"Remove Bee Data Files‚Äã","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#remove-bee-data-files","content":"To completely remove all Bee files from your system you will also need to remove the config and data files. danger Node keys, password, chunks and state files are stored in the data folder. Make backups of your data folder to prevent losing keys and data. "},{"title":"Bee‚Äã","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#bee","content":"Config folder: Configuration file is stored in /etc/bee/ Data folder: State, keys, chunks, and other data are stored in /var/lib/bee/ "},{"title":"Access Content","type":0,"sectionRef":"#","url":"/docs/desktop/access-content","content":"Access Content Accessing content on Swarm using Swarm Desktop is easy. All you need to get started is the Swarm hash for the content you wish to access. Whenever content is uploaded to Swarm a Swarm hash is generated as a reference to that content. To access content on Swarm go to the Files tab and click Download: From there, paste the Swarm hash for the content you want to access, and click Find. We'll use the hash for a Swarm blog post explaining how to upload a website to Swarm: 6843d3be17364ea0620011430e4db2a26ff781da478493a02d6eb5aae886b8ae On the following screen you will see the data associated with the Swarm hash and see options for downloading (or browsing if it is a hash for a website): Click View Website to see the site in your browser, or Download to download the files:","keywords":""},{"title":"Upgrading Bee","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/upgrading-bee","content":"","keywords":""},{"title":"Upgrade Procedure‚Äã","type":1,"pageTitle":"Upgrading Bee","url":"/docs/bee/working-with-bee/upgrading-bee#upgrade-procedure","content":"danger Bee sure to back up your keys and cash out your cheques to make sure your xBZZ is safe before applying updates. danger Nodes should not be shut down or updated in the middle of a round they are playing in as it may cause them to lose out on winnings or become frozen. To see if your node is playing the current round, check if lastPlayedRound equals round in the output from the /redistributionstate endpoint. See staking section for more information on staking and troubleshooting. "},{"title":"Ubuntu / Debian / Raspbian‚Äã","type":1,"pageTitle":"Upgrading Bee","url":"/docs/bee/working-with-bee/upgrading-bee#ubuntu--debian--raspbian","content":"To upgrade Bee, simply stop the Bee service. sudo systemctl stop bee  Now follow the installation instructions to download the new package and install the new version, as you would during a new installation. You will be greeted by the following prompt: Configuration file '/etc/bee/bee.yaml' ==&gt; Modified (by you or by a script) since installation. ==&gt; Package distributor has shipped an updated version. What would you like to do about it ? Your options are: Y or I : install the package maintainer's version N or O : keep your currently-installed version D : show the differences between the versions Z : start a shell to examine the situation The default action is to keep your current version. *** bee.yaml (Y/I/N/O/D/Z) [default=N] ?  Select N to keep your current data and keys. You may now start your node again: sudo systemctl start bee  Manual Installations‚Äã To upgrade your manual installation, simply stop Bee, replace the Bee binary and restart. Docker‚Äã To upgrade your docker installation, simply increment the version number in your configurations and restart. "},{"title":"Upgrading from a mainnet v1.5.x series to a mainnet v1.6.x series‚Äã","type":1,"pageTitle":"Upgrading Bee","url":"/docs/bee/working-with-bee/upgrading-bee#upgrading-from-a-mainnet-v15x-series-to-a-mainnet-v16x-series","content":"Bee v1.6.x contains a completely new data storage format called Sharky. As part of these changes, existing data must be migrated to the new data structure expected by the 1.5.x client. This will happen automatically, but may require extra space and cause a spike in cpu requirements for the duration of the migration. If you can not accommodate approximately 3x (2x might even be enough) as much disk space as is currently being used by your Bee datadir, you may want to run bee db nuke before upgrading (but after stopping the Bee service) to resync your nodes content from the network. If you have locally pinned content please ensure you have a local backup so that you can restamp and restore it to the network in case of disaster. "},{"title":"Backup and Restore","type":0,"sectionRef":"#","url":"/docs/desktop/backup-restore","content":"","keywords":""},{"title":"Create a Backup‚Äã","type":1,"pageTitle":"Backup and Restore","url":"/docs/desktop/backup-restore#create-a-backup","content":"To create a backup of your Bee node in Swarm Desktop, begin by navigating to the Settings tab in the app and copying the location of the data directory as indicated in the Data DIR field:  Navigate to the directory you just copied and create copies of the localstore, statestore, and keys folders and store them in a secure and private location. The keys folder is the most important folder as it contains your Bee node's private keys, and you will lose access to your Bee node and its assets if those keys are lost.  In addition to the data folders, you will also need the password found in the config.yaml file in order to restore a Bee node from backup. Move up one directory from Data DIR to the Data directory, and create a copy of the config.yaml file and save it along with the other folders you just backed up:  Alternatively you may open the config.yaml and save the password as a text file along with the rest of your backup files:  Your completed backup should have three folders and one file (localstore, statestore, keys and config.yaml/password.txt ) and look like this:  "},{"title":"Restore from Backup‚Äã","type":1,"pageTitle":"Backup and Restore","url":"/docs/desktop/backup-restore#restore-from-backup","content":"To restore from backup, begin with a new install of Swarm Desktop. Once the installation process is finished, navigate to the Settings tab in the app and copy the install file directory as indicated in the Data DIR field:  Before navigating to the directory you just copied, right click the Bee icon in the System tray and select Stop Bee and then Quit to close and exit from Swarm Desktop:  Next open your file explorer and navigate to the directory you just copied. Delete the localstore, statestore, and keys (if present) folders, and replace them with your own backup copies of these folders:  Move up one directory from Data DIR to Data, and open the config.yaml file in a text editor such as VS Code or a plaintext editor:   Replace the password string with your own password from the config.yaml backup. Restart Swarm Desktop and check to see if the backup was restored successfully:  "},{"title":"Install","type":0,"sectionRef":"#","url":"/docs/desktop/install","content":"","keywords":""},{"title":"Download and Install Swarm Desktop‚Äã","type":1,"pageTitle":"Install","url":"/docs/desktop/install#download-and-install-swarm-desktop","content":"Installing the Swarm Desktop app takes only a few clicks. To get started, simply download and install the Swarm Desktop app for your operating system. Installers are available for Windows, Linux, and OSX. You can find download links for Swarm Desktop at the Swarm homepage and you can find installers for specific operating systems at the releases page of the Swarm Desktop GitHub repo. caution Swarm Desktop is in Beta and currently includes the Sentry application monitoring and bug reporting software which automatically collects data in order to help improve the software. caution This project is in beta state. There might (and most probably will) be changes in the future to its API and working. Also, no guarantees can be made about its stability, efficiency, and security at this stage. Ethswarm.org Swarm Desktop Page Swarm Desktop GitHub Releases Page After running the installer, a window will pop up and display the installation status:  Once the installation is complete, Swarm Desktop will open up in your default browser in a new window to the &quot;Info&quot; tab of the app:  If the installation went smoothly, you should see the message &quot;Your node is connected&quot; above the &quot;Access Content&quot; button along with a status message of &quot;Node OK&quot;. What Just Happened?‚Äã Running the Swarm Desktop app for the first time set up a new Bee node on your system. The installation process generated and saved private keys for your node in the Swarm Desktop's data directory. Those keys were used to start up a new Bee node in ultra-light mode. danger If your Swarm Desktop files are accidentally deleted or become corrupted you will lose access to any assets or data which are secured using those keys. Make sure to backup your keys. "},{"title":"\"Ultra-light\" and \"Light\" Nodes‚Äã","type":1,"pageTitle":"Install","url":"/docs/desktop/install#ultra-light-and-light-nodes","content":"Swarm Desktop by default starts up a node in &quot;ultra-light&quot; mode. When running in ultra-light mode Swarm Desktop limited to only downloading data from Swarm. Moreover, it's limited to downloading only within the free threshold allowed by other nodes. For instructions on switching to light mode see the configuration section. "},{"title":"Tour of Swarm Desktop‚Äã","type":1,"pageTitle":"Install","url":"/docs/desktop/install#tour-of-swarm-desktop","content":""},{"title":"Info Tab‚Äã","type":1,"pageTitle":"Install","url":"/docs/desktop/install#info-tab","content":"The &quot;Info&quot; tab gives you a quick view of your Swarm Desktop's status. From here we can quickly see if the node is connected to Swarm, whether the node is funded, and whether its chequebook contract is set up. On a new install of Swarm Desktop, the node should be connected, but the wallet and chequebook will not have been set up yet.  "},{"title":"Files Tab‚Äã","type":1,"pageTitle":"Install","url":"/docs/desktop/install#files-tab","content":"From &quot;Files&quot; tab you can input a Swarm hash in order to download the file associated with the hash. See this full guide for downloading using Swarm Desktop.  "},{"title":"Account Tab‚Äã","type":1,"pageTitle":"Install","url":"/docs/desktop/install#account-tab","content":"From the &quot;Account&quot; tab you can view your Swarm Desktop node's Gnosis Chain address and associated xBZZ and xDAI balances.  "},{"title":"Settings Tab‚Äã","type":1,"pageTitle":"Install","url":"/docs/desktop/install#settings-tab","content":"From the &quot;Settings&quot; tab you can view important settings values. Note that the Blockchain RPC URL and ENS resolver URL are already filled in, and only the Blockchain RPC URL is modifiable through this tab. If you wish to modify other settings see the configuration page for detailed instructions.  "},{"title":"Status Tab‚Äã","type":1,"pageTitle":"Install","url":"/docs/desktop/install#status-tab","content":"From the &quot;Status&quot; tab you can see a quick overview of the health of your Swarm Desktop's Bee node.  "},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/desktop/configuration","content":"","keywords":""},{"title":"Setting RPC Endpoint‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#setting-rpc-endpoint","content":"In order to interact with the Gnosis Chain to buy stamps, participate in staking, and manage assets such as xBZZ, Bee nodes require a valid Gnosis Chain RPC endpoint. By default the RPC endpoint is set to https://xdai.fairdatasociety.org, however any valid Gnosis Chain RPC endpoint may be used. To modify the RPC endpoint, first navigate to the Settings tab:  From the Settings tab, expand the API Settings section and click the pen button next to Blockchain RPC URL to edit the default RPC. You can choose any valid Gnosis Chain RPC, either from your own Gnosis node or a service provider. You can find a list of paid and free RPC options from the Gnosis Chain docs. For this example we will use the free endpoint - https://rpc.gnosischain.com/.  Click Save and Restart to finish changing the RPC endpoint. "},{"title":"Upgrading from an Ultra-light to a Light Node‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#upgrading-from-an-ultra-light-to-a-light-node","content":"Bee ultra-light nodes are limited to only downloading small amounts of data from Swarm. In order to download greater amounts of data or to upload data to Swarm you must upgrade to a light node. To do this we need to first fund our Swarm Desktop Bee node with some xDAI (DAI bridged from Ethereum to Gnosis Chain which serves as Gnosis Chain's native token for paying transaction fees) in order to pay for the Gnosis Chain transactions required for setting up a light node. "},{"title":"Bridging Ethereum DAI to Gnosis Chain as xDAI‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#bridging-ethereum-dai-to-gnosis-chain-as-xdai","content":"If you already have some xDAI on a Gnosis Chain address, skip to the next step Funding Node with xDAI. If you have DAI on Ethereum and need to swap it for xDAI, you can use one of the two Gnosis Chain bridges described in the Gnosis Chain documentation: OmnibridgexDAI Bridge See this guide to Gnosis Chain bridges from the Honeyswap DEX for more detailed instructions on using each of these bridges. Five to ten xDAI is plenty to get started. "},{"title":"Funding Node with xDAI‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#funding-node-with-xdai","content":"Once you have a few xDAI in your Gnosis Chain address, to fund your Bee node you need to send it from your wallet to your Swarm Desktop wallet. You can find your address from the Account tab of the app.  Next simply send your xDAI to that address. Before sending, make sure you have set your wallet to use the Gnosis Chain network and not the Ethereum mainnet. If Gnosis Chain is not included as default selectable network in your wallet, you may need to add the network manually. You can use this configuration to add Gnosis Chain: Field\tValueNetwork name:\tGnosis New RPC URL:\thttps://rpc.gnosischain.com Chain ID:\t100 Symbol:\txDai Block Explorer URL (Optional):\thttps://blockscout.com/xdai/mainnet  The transaction should be confirmed in under a minute. We can check on the Account page to see when the xDAI has been received:  "},{"title":"Set Up Wallet‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#set-up-wallet","content":"Now with some xDAI in the Swarm Desktop wallet, we can upgrade our Bee node from ultra-light to a light node. Completing the setup process will swap xDAI for some xBZZ at the current price, and will issue the transactions needed to set up the chequebook contract. To get started, navigate to the Info tab and click the Setup wallet button.Click Use xDAI.Confirm that you have sufficient xDAI balance and click Proceed.Click Swap Now and Upgrade.Wait for the upgrade to complete.After the upgrade is complete, you will see several new sections within the Account tab: Chequebook, Stamps, and Feeds. "},{"title":"Fund Chequebook‚Äã","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#fund-chequebook","content":"After setting up your wallet you will have access to the Chequebook section from the Accounts tab. From here you can manage your chequebook for your Swarm Desktop Bee node. "},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/desktop/introduction","content":"Introduction While running Bee from the terminal is a powerful and flexible approach for developers and node operators, it may not be the best option for more simple use cases. The Swarm Desktop app is an alternative which provides an easy-to-use graphical user interface for running a Bee node and interacting seamlessly with the Swarm network. The app automatically spins up a Bee ultra-light node in order to allow anyone to use Swarm to upload and download files securely, send encrypted files to others, and host HTML websites with ease. The Swarm Desktop App was designed to simplify the Swarm onboarding process so that anyone can benefit from decentralized storage while maintaining privacy and control over their data. Available for Windows, Mac, and Linux operating systems, the Swarm Desktop App serves as a personal gateway to the Swarm network.","keywords":""},{"title":"Postage Stamps","type":0,"sectionRef":"#","url":"/docs/desktop/postage-stamps","content":"","keywords":""},{"title":"How to Buy a Postage Stamp Batch‚Äã","type":1,"pageTitle":"Postage Stamps","url":"/docs/desktop/postage-stamps#how-to-buy-a-postage-stamp-batch","content":"Stamps can be purchased by selecting Stamps from the Account tab:  And then clicking the Buy New Postage Stamp button:  "},{"title":"Depth and Amount‚Äã","type":1,"pageTitle":"Postage Stamps","url":"/docs/desktop/postage-stamps#depth-and-amount","content":"Batch depth and amount are the two required parameters which must be set when purchasing a postage stamp batch. Depth determines how many chunks can be stamped with a batch while amount determines how much xBZZ is assigned per chunk.  Inputting a value for depth allows you to preview the upper limit of data which can be uploaded for that depth. info Note that a batch will become fully utilized before the upper limit has been reached, so the actual amount of data which can be uploaded is lower than the limit. At higher depth values, this becomes less of a problem as batch utilization will come closer to the upper limit on average as the depth increases. For this reason, Swarm Desktop requires a minimum batch depth of 17. Inputting a value for amount and depth together will allow you to also preview the total cost of the postage stamp batch as well as the TTL (time to live - how long the batch can store data on Swarm). Click the Buy New Stamp button to purchase the stamp batch.  After purchasing stamps you can view stamp details from the Postage Stamps drop down menu:  "},{"title":"Publish a Website","type":0,"sectionRef":"#","url":"/docs/desktop/publish-a-website","content":"","keywords":""},{"title":"Publish a Website on Swarm - Step by Step Guide‚Äã","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#publish-a-website-on-swarm---step-by-step-guide","content":""},{"title":"Install Swarm Desktop and Deposit Funds‚Äã","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#install-swarm-desktop-and-deposit-funds","content":"First, download and install the Swarm Desktop App. Next, add xDAI (transaction fees) to your Node Wallet address. If you possess xBZZ (storage fees), you can deposit it alongside the xDAI. Otherwise, you can exchange your xDAI for xBZZ using the Swarm Desktop app. Follow these steps to deposit funds: Launch the Swarm Desktop App and go to the Account section in the left menu.Transfer xDAI to your node wallet address. For safety, we suggest sending no more than 5 to 10 xDAI.After funding your wallet, click the Top Up Wallet button on the right side of the screen.Select the Use xDAI option.Verify your xDAI balance and click Proceed.Specify the amount of xDAI to convert to xBZZ and click Swap Now.Your Node Wallet address will be credited with xBZZ.  "},{"title":"Setup Chequebook‚Äã","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#setup-chequebook","content":"Your node address is now funded with xDAI and xBZZ. However, to upload data on Swarm, you will need to transfer your funds to the Chequebook contract address. Follow these steps: Go to the Account section in the left menu.Select the Chequebook tab in the top menu.Click the Deposit button.Specify the amount of xBZZ to deposit into your Chequebook, which will be used for storage costs. "},{"title":"Publish Website‚Äã","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#publish-website","content":"To publish your website on Swarm, follow these steps: Navigate to the Files tab.Click the Add Website button.Select your website folder. NOTE: The index.html file should be in the root folder.Purchase a Postage Stamp to publish your page. NOTE: Postage stamps cover storage costs for a specified duration.Upload the website.  https://api.gateway.ethswarm.org/bzz/6843d3be17364ea0620011430e4db2a26ff781da478493a02d6eb5aae886b8ae/ "},{"title":"Connecting an ENS Domain to Your Website‚Äã","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#connecting-an-ens-domain-to-your-website","content":"Associating your ENS domain with a Swarm hash generates a memorable, user-friendly identifier for your website, allowing users to easily locate and access your website without having to recall a lengthy, complex Swarm hash. Initially, you‚Äôll need to register your domain name. To register and manage your ENS domain, you can use the ENS Domains Dapp along with the MetaMask browser extension. After registering your name and connecting MetaMask to the relevant Ethereum account, set the resolver to use the public ENS if you haven‚Äôt already. Navigate to My Names and select the name you want to link to your Swarm content.Click on ADD/EDIT RECORD.From the &quot;add record&quot; dropdown menu, select Content.Enter your Swarm Hash, beginning with &quot;bzz://&quot; and click &quot;Save.&quot;  Your website is now available on: https://api.gateway.ethswarm.org/bzz/swarm-devrel.eth/ "},{"title":"Update the Website: Set up and update a feed‚Äã","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#update-the-website-set-up-and-update-a-feed","content":"Swarm feeds allow you to easily create a permanent address for your content stored on Swarm that you can update at any time. If you plan to update your website in the future, it‚Äôs recommended that you set up a ‚ÄúFeed‚Äù before uploading your website to Swarm. This way, the Swarm Hash connected to your ENS domain will stay the same, even as you change the content behind that hash. This will enable you to update your website‚Äôs content without changing the Swarm Hash and incurring Ethereum transaction costs each time you do so. Set up a Feed:‚Äã Navigate to to AccountClick on Feeds in the top menuClick on Create New FeedDefine Identity nameAnd click Create Feed. Upload Website on Swarm and connect it to the Feed:‚Äã Navigate to to AccountClick on Feeds in the top menuChoose the Feed you want to updateClick View Feed PageClick the Add Website button.Select your website folder. NOTE: The index.html file should be in the root folder.Add Postage Stamp to publish your page. NOTE: Postage stamps cover storage costs for a specified duration.Upload the website to your Node.Connect Feed hash to ENS domain as described above.  By following these instructions, you can now leverage the benefits of decentralised storage, maintain a censorship-resistant website, and create a user-friendly experience by connecting your site to an ENS domain. "},{"title":"Upload Content","type":0,"sectionRef":"#","url":"/docs/desktop/upload-content","content":"Upload Content After purchasing a batch of postage stamps you will be able to upload files. First go to the ‚ÄúFiles‚Äù tab. Here you can choose between three options, depending on what you want to upload: a single file, a folder or a website. After choosing your option you‚Äôll need to add a postage stamp. Click ‚ÄúAdd postage stamp‚Äù and choose a postage stamp. Click ‚ÄúProceed with the selected stamp‚Äù and upload your data. Once your file is uploaded a Swarm hash and Swarm Gateway link will be displayed (which you can use to share the file with others) along with other pertinent information.","keywords":""},{"title":"browse-the-swarm","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/browse-the-swarm","content":"browse-the-swarm title: Browse the Swarm id: browse-the-swarm One of Swarm's most compelling features in the ability to host unstoppable websites.","keywords":""},{"title":"Direct upload","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/direct-upload","content":"Direct upload caution We recommend turning encryption ON when using this feature with content that has to be re-uploaded due to uniqueness and uniformity assumptions regarding chunk addresses that have been stamped in a given postage batch. Please note that when uploading the same content multiple times using the same postage stamp without encryption will cause your postage stamp to fill-up faster than usual! Configuration‚Äã By default your bee instance will handle uploads in a deferred manner. If you want to upload directly to the network you have to set the Swarm-Deferred-Upload header value to &quot;false&quot; in your request. curl \\ -X POST \\ -H &quot;Swarm-Deferred-Upload: false&quot; \\ -H &quot;Content-Type: application/x-tar&quot; \\ -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; \\ --data-binary @my_data.tar http://localhost:1633/bzz Mode of Operation‚Äã You have the option to push data directly onto the network while still allowing buffered uploads as it was traditionally allowed. With this new mode of operation, the root hash of an upload will only be received once all chunks have been safely uploaded into the network.","keywords":""},{"title":"Host Your Website on Swarm","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/host-your-website","content":"","keywords":""},{"title":"Enable ENS on Your Node‚Äã","type":1,"pageTitle":"Host Your Website on Swarm","url":"/docs/develop/access-the-swarm/host-your-website#enable-ens-on-your-node","content":"In order to resolve ENS names using your API endpoints, you must specify a valid ENS resolver endpoint when starting your Bee node. We recommend that users run their own Geth node, which can be trusted absolutely, however service providers such as https://cloudflare-eth.com or Infura may suffice. Public gateways such as gateway.ethswarm.org will also usually provide ENS resolution. bee start --resolver-options &quot;https://cloudflare-eth.com&quot;  If specifying using your bee.yaml configuration file, the syntax is as follows: resolver-options: [ &quot;https://cloudflare-eth.com&quot; ]  Once you have restarted your node, you should be able to see the Swarm homepage at: http://localhost:1633/bzz/swarm.eth/ info Use the resolver-options flag to point the Bee resolver to any ENS compatible smart-contract on any EVM compatible chain danger Make sure you trust the gateway you are interacting with! To ensure that you are retrieving the correct content, run your own ENS resolver and Bee node. "},{"title":"Link an ENS domain to a website.‚Äã","type":1,"pageTitle":"Host Your Website on Swarm","url":"/docs/develop/access-the-swarm/host-your-website#link-an-ens-domain-to-a-website","content":"First we will need to upload the website assets to Swarm in order to get its Swarm reference hash, seeuploading a directoryfor more information. This time we will also include the Swarm-Index-Document header set to the index.html. This will cause Bee to serve each directories index.html file as default when browsing to the directory root / url. We will also provide a custom error page, using the Swarm-Error-Document header. In the case that your website is a single page app, where you would like to direct to the JavaScript history API powered router, you may provide the index.html page for both settings. curl \\ -X POST \\ -H &quot;Content-Type: application/x-tar&quot; \\ -H &quot;Swarm-Index-Document: index.html&quot; \\ -H &quot;Swarm-Error-Document: index.html&quot; \\ --data-binary @my_website.tar http://localhost:1633/bzz  { &quot;reference&quot;: &quot;b25c89a401d9f26811680476619a1eb4a4e189e614bc6161cbfd8b343214917b&quot; }  Next, we add a Content record to your ENS domain's resolver contract. We recommend the excellent ENS Domains Dapp used with the MetaMask browser extension for registering and administrating your ENS domain. Once you have registered your name, and have connected MetaMask with the relevant Ethereum account, you must first set the resolver to use the public ENS if you have not already done so. First, navigate to 'My Names', and select the name you would like to link your Swarm content to. Press 'Set' next to your resolver record.  Select 'Use Public Resolver'.  Select '+' to add a record.  Choose the 'Content' record type from the drop down menu.  Add the Swarm reference you created earlier and press 'Save'.  Verify the Content Record has been created!  Done! üëè Now you will be able to see your website hosted using the ENS name instead of the Swarm Reference!  "},{"title":"Buy a Batch of Stamps","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch","content":"","keywords":""},{"title":"Fund your node's wallet.‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#fund-your-nodes-wallet","content":"To start up your node, you will already have provided your node with xDAI for gas and xBZZ which was transferred into your chequebook when your node was initialised. This will be used to interact with other nodes using the SWAP protocol. In order to access more funds to buy batches of stamps, your wallet must be funded with xBZZ. The easiest way to achieve this is to withdraw funds from your chequebook: curl -XPOST &quot;http://localhost:1635/chequebook/withdraw?amount=1000&quot;  "},{"title":"Purchase a batch of stamps‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#purchase-a-batch-of-stamps","content":"Stamps are created in batches so that storer nodes may calculate the validity of a chunk's stamp with only local knowledge. This enables the privacy you enjoy with Swarm. Stamp batches are created in buckets with a bucket depth of 16. The entire Swarm address space is divided into 216=65,5362^{16} = 65,536216=65,536 different buckets. When uploaded, each of your files are split into 4kb chunks and assigned to a specific bucket based on its address. When creating a batch you must specify two values, batch depth and amount. "},{"title":"Amount‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#amount","content":"The amount is the quantity of xBZZ in PLUR (1√ó1016PLUR=1¬†xBZZ)(1 \\times 10^{16}PLUR = 1 \\text{ xBZZ})(1√ó1016PLUR=1¬†xBZZ) that is assigned per chunk in the batch. The total number of xBZZ that will be paid for the batch is calculated from this figure and the batch depth like so: 2batch_depth√óamount2^{batch \\_ depth} \\times {amount}2batch_depth√óamount The paid xBZZ forms the balance of the batch. This balance is then slowly depleted as time ticks on and blocks are mined on Gnosis Chain. For example, with a batch depth of 20 and an amount of 1000000000 PLUR: 220√ó1000000000=1048576000000000¬†PLUR=0.1048576¬†xBZZ2^{20} \\times 1000000000 = 1048576000000000 \\text{ PLUR} = 0.1048576 \\text{ xBZZ}220√ó1000000000=1048576000000000¬†PLUR=0.1048576¬†xBZZ "},{"title":"Batch Depth‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#batch-depth","content":"The batch depth determines how many chunks are allowed to be in each bucket. The number of chunks allowed in each bucket is calculated like so:2batch_depth‚àíbucket_depth2^{batch \\_ depth - bucket \\_ depth}2batch_depth‚àíbucket_depth === 2batch_depth‚àí162^{batch \\_ depth - 16}2batch_depth‚àí16. With a minimum batch depth of 17. "},{"title":"Calculating the depth and amount of your batch of stamps‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#calculating-the-depth-and-amount-of-your-batch-of-stamps","content":"One notable aspect of batch utilization is that the entire batch is considered fully utilized as soon as any one of its buckets are filled. This means that the actual amount of chunks storable by a batch is less than the nominal maximum amount. Right now, the easiest way to start uploading content is to buy a large enough batch so that it is incredibly unlikely you will end up with too many chunks falling into the same bucket. The amount you specify will govern the amount of time your chunks live in Swarm. Because pricing is variable, it is not possible to predict with accuracy exactly when your chunks will run out of balance, however, it can be estimated based on the current price and the remaining batch balance. For now, we suggest you specify batch depth 20 and amount 10000000000 for your batches just to get started. This should be enough to upload several GB of data for a few weeks. danger When you purchase a batch of stamps, you agree to burn xBZZ. Although your 'balance' slowly decrements as time goes on, there is no way to withdraw xBZZ from a batch. This is an outcome of Swarm's decentralised design, to read more about the different components of Swarm fit together, read The Book of Swarm . curl -s -XPOST http://localhost:1635/stamps/10000000000/20  info Once your batch has been purchased, it will take a few minutes for other Bee nodes in the Swarm to catch up and register your batch. Allow some time for your batch to propagate in the network before proceeding to the next step. Look out for more ways to more accurately estimate the correct size of your batch coming soon! To check on your stamps, send a GET request to the stamp endpoint. curl http://localhost:1635/stamps  info When uploading content which has been stamped using an already expired postage stamp, the node will not attempt to sync the content. You are advised to use longer-lived postage stamps and encrypt your content to work around this. It is not possible to reupload unencrypted content which was stamped using an expired postage stamp. We're working on improving on this. "},{"title":"Calculating the remaining TTL (time to live) of your batch‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#calculating-the-remaining-ttl-time-to-live-of-your-batch","content":"info At present, TTL is a primitive calculation based on the current storage price and the assumption that storage price will remain static in the future. As more data is uploaded into Swarm, the price of storage will begin to increase. For data that it is important to keep alive, make sure your batches have plenty of time to live! In order to make sure your batch has sufficient remaining balance to be stored and served by nodes in its area of responsibility, you must regularly check on its time to live and act accordingly. The time to live is the number of seconds before the chunks will be considered for garbage collection by nodes in the network. The remaining time to live in seconds is shown in the returned json object as the value for batchTTL.  curl http://localhost:1635/stamps  { &quot;stamps&quot;: [ { &quot;batchID&quot;: &quot;6d32e6f1b724f8658830e51f8f57aa6029f82ee7a30e4fc0c1bfe23ab5632b27&quot;, &quot;utilization&quot;: 0, &quot;usable&quot;: true, &quot;label&quot;: &quot;&quot;, &quot;depth&quot;: 20, &quot;amount&quot;: &quot;113314620&quot;, &quot;bucketDepth&quot;: 16, &quot;blockNumber&quot;: 19727733, &quot;immutableFlag&quot;: false, &quot;exists&quot;: true, &quot;batchTTL&quot;: 68795140 } ] }  "},{"title":"Top up your batch‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#top-up-your-batch","content":"danger Don't let your batch run out! If it does, you will need to restamp and resync your content. If your batch is starting to run out, or you would like to extend the life of your batch to protect against storage price rises, you can increase the batch TTL by topping up your batch using the stamps endpoint, passing in the relevant batchID into the HTTP PATCH request. curl -X PATCH &quot;http://localhost:1635/stamps/topup/6d32e6f1b724f8658830e51f8f57aa6029f82ee7a30e4fc0c1bfe23ab5632b27/10000000&quot;  "},{"title":"Dilute your batch‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#dilute-your-batch","content":"In order to store more data with a batch of stamps, you must &quot;dilute&quot; the batch. Dilution simply refers to increasing the depth of the batch, thereby allowing it to store a greater number of chunks. As dilution only increases the the depth of a batch and does not automatically top up the batch with more xBZZ, dilution will decrease the TTL of the batch. Therefore if you wish to store more with your batch but don't want to decrease its TTL, you will need to both dilute and top up your batch with more xBZZ. Here we call the /stamps endpoint and find a batch with depth 17 and a batchTTL of 2083223 which we wish to dilute: curl http://localhost:1635/stamps  { &quot;stamps&quot;: [ { &quot;batchID&quot;: &quot;0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5&quot;, &quot;utilization&quot;: 0, &quot;usable&quot;: true, &quot;label&quot;: &quot;&quot;, &quot;depth&quot;: 17, &quot;amount&quot;: &quot;10000000000&quot;, &quot;bucketDepth&quot;: 16, &quot;blockNumber&quot;: 29717348, &quot;immutableFlag&quot;: false, &quot;exists&quot;: true, &quot;batchTTL&quot;: 2083223, &quot;expired&quot;: false } ] }  Next we call the dilute endpoint to increase the depth of the batch using the batchID and our new depth of 19: curl -s -XPATCH http://localhost:1635/stamps/dilute/0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5/19  And a txHash of our successful transaction: { &quot;batchID&quot;: &quot;0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5&quot;, &quot;txHash&quot;: &quot;0x298e80358b3257292752edb2535a1cd84440c074451b61f78fab349aea4962b7&quot; }  And finally we use the /stamps endpoint again to confirm the new depth and decreased batchTTL: curl http://localhost:1635/stamps  We can see the new depth of 19 decreased batchTTL of 519265. { &quot;stamps&quot;: [ { &quot;batchID&quot;: &quot;0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5&quot;, &quot;utilization&quot;: 0, &quot;usable&quot;: true, &quot;label&quot;: &quot;&quot;, &quot;depth&quot;: 19, &quot;amount&quot;: &quot;10000000000&quot;, &quot;bucketDepth&quot;: 16, &quot;blockNumber&quot;: 29717348, &quot;immutableFlag&quot;: false, &quot;exists&quot;: true, &quot;batchTTL&quot;: 519265, &quot;expired&quot;: false } ] }  "},{"title":"Stewardship‚Äã","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#stewardship","content":"The stewardship endpoint in combination with pinning can be used to guarantee that important content is always available. It is used for checking whether the content for a Swarm reference is retrievable and for re-uploading the content if it is not. An HTTP GET request to the stewardship endpoint checks to see whether the content for the specified Swarm reference is retrievable: curl &quot;http://localhost:1633/stewardship/c0c2b70b01db8cdfaf114cde176a1e30972b556c7e72d5403bea32e c0207136f&quot;  { &quot;isRetrievable&quot;: true }  If the content is not retrievable, an HTTP PUT request can be used to re-upload the content: curl -X PUT &quot;http://localhost:1633/stewardship/c0c2b70b01db8cdfaf114cde176a1e30972b556c7e72d5403bea32ec0207136f&quot;  Note that for the re-upload to succeed, the associated content must be available locally, either pinned or cached. Since it isn't easy to predict if the content will be cached, for important content pinning is recommended. "},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/introduction","content":"","keywords":""},{"title":"Decentralise Your Files‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#decentralise-your-files","content":"Bee provides several convenient ways to upload your data into the Swarm. Once your data has been uploaded, it will be distributed, stored and retrievable by a worldwide network of p2p nodes, and made available from Swarm's web gateway. "},{"title":"Upload Whole Directories‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#upload-whole-directories","content":"Find out how to upload whole directories at once using Bee's HTTP API. "},{"title":"Host Your Website on the Decentralised Web‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#host-your-website-on-the-decentralised-web","content":"Swarm is a distributed international network of nodes that provides hosting for your unstoppable websites. See this guide to hosting your website on swarm. "},{"title":"Sync With the Network‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#sync-with-the-network","content":"Watch as your uploaded data is synced with the network of thousands of nodes worldwide! "},{"title":"Keep Your Data Alive‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#keep-your-data-alive","content":"Learn how to assign xBZZ to your data using postage stamps so that it remains live on the Swarm network. "},{"title":"Pinning‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#pinning","content":"Learn how to pin your data so that it always remains available locally on your Bee node. "},{"title":"Light Nodes‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#light-nodes","content":"When accessing the Swarm network for certain use cases a Bee might not want to take part in forwarding and storing data. Find out how to use Bee in Light Node mode. "},{"title":"Ultra Light Nodes‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#ultra-light-nodes","content":"For the cases when we want to get started quickly we can run the Bee instance without a blockchain. Find out how to use Bee in Ultra Light Node mode. "},{"title":"Store with Encryption","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/store-with-encryption","content":"Store with Encryption In Swarm, all data is public by default. To protect sensitive content, it must be encrypted so that only authorised users are able to decrypt and then view the plaintext content. The Bee client provides a facility to encrypt files and directories while uploading which are only able to be read by users with access to the corresponding decryption key. Encrypt and Upload a File To encrypt a file simply include the Swarm-Encrypt: true header with your HTTP request. curl -F file=@bee.jpg -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; -H &quot;Swarm-Encrypt: true&quot; http://localhost:1633/bzz When successful, the Bee client will return a 64 byte reference, instead of the usual 32 bytes. More information on how to buy a postage stamp batch and get its batch id can be found here. { &quot;reference&quot;: &quot;f7b1a45b70ee91d3dbfd98a2a692387f24db7279a9c96c447409e9205cf265baef29bf6aa294264762e33f6a18318562c86383dd8bfea2cec14fae08a8039bf3&quot; } Here we see that, when using the Bee node's encryption function, the reference hash that is returned is 128 hex characters long. The first 64 characters of this is the familiar Swarm address - the reference that allows us to retrieve the data from the swarm. This is the same reference we would get uploading unencrypted files using Bee, so it is safe to share. The second part of the reference is a 64 character decryption key which is required to decrypt the referenced content and view the original data in the clear. This is sensitive key material and must be kept private. It is important that this data not be sent in requests to a public gateway as this would mean that gateway would be able to decrypt your data. However, if you are running a node on your local machine, you may safely use the API bound to localhost. The key material is never exposed to the network so your data remains safe. info Encryption is disabled by default on all Swarm Gateways to keep your data safe. Install Bee on your computer to use the encryption feature. Download and Decrypt a File To retrieve your file, simply supply the full 64 byte string to the files endpoint, and the Bee client will download and decrypt all the relevant chunks and restore them to their original format. curl -OJ http://localhost:1633/bzz/f7b1a45b70ee91d3dbfd98a2a692387f24db7279a9c96c447409e9205cf265baef29bf6aa294264762e33f6a18318562c86383dd8bfea2cec14fae08a8039bf3 danger Never use public gateways when requesting full encrypted references. The hash contains sensitive key information which should be kept private. Run your own node to use Bee's encryption features.","keywords":""},{"title":"Pinning","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/pinning","content":"","keywords":""},{"title":"Pin During Upload‚Äã","type":1,"pageTitle":"Pinning","url":"/docs/develop/access-the-swarm/pinning#pin-during-upload","content":"To store content so that it will persist even when Bee's garbage collection routine is deleting old chunks, we simply pass the Swarm-Pin header set to true when uploading. curl -H &quot;Swarm-Pin: true&quot; -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; --data-binary @bee.mp4 localhost:1633/bzz\\?bee.mp4  { &quot;reference&quot;: &quot;1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30&quot; }  "},{"title":"Administer Pinned Content‚Äã","type":1,"pageTitle":"Pinning","url":"/docs/develop/access-the-swarm/pinning#administer-pinned-content","content":"To check what content is currently pinned on your node, query the pins endpoint of your Bee API: curl localhost:1633/pins  { &quot;references&quot;: [ &quot;1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30&quot; ] }  or, to check for specific references: curl localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30  A 404 response indicates the content is not available. "},{"title":"Unpinning Content‚Äã","type":1,"pageTitle":"Pinning","url":"/docs/develop/access-the-swarm/pinning#unpinning-content","content":"We can unpin content by sending a DELETE request to the pinning endpoint using the same reference: curl -XDELETE http://localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30 `` ```json {&quot;message&quot;:&quot;OK&quot;,&quot;code&quot;:200}  Now, when check again, we will get a 404 error as the content is no longer pinned. curl localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30  { &quot;message&quot;: &quot;Not Found&quot;, &quot;code&quot;: 404 }  info Pinning and unpinning is possible for files (as in the example) and also the chunks, directories, and bytes endpoints. See the API documentation for more details. "},{"title":"Pinning Already Uploaded Content‚Äã","type":1,"pageTitle":"Pinning","url":"/docs/develop/access-the-swarm/pinning#pinning-already-uploaded-content","content":"The previous example showed how we can pin content upon upload. It is also possible to pin content that is already uploaded and present in the Swarm. To do so, we can send a POST request including the swarm reference to the files pinning endpoint. curl -XPOST http://localhost:1633/pins/7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f  { &quot;message&quot;: &quot;OK&quot;, &quot;code&quot;: 200 }  The pins operation will attempt to fetch the content from the network if it is not available on the local node. Now, if we query our files pinning endpoint again, the swarm reference will be returned. curl http://localhost:1633/pins/7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f  { &quot;reference&quot;: &quot;7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f&quot; }  danger While the pin operation will attempt to fetch content from the network if it is not available locally, we advise you to ensure that the content is available locally before calling the pin operation. If the content, for whatever reason, is only fetched partially from the network, the pin operation only partly succeeds and leaves the internal administration of pinning in an inconsistent state. "},{"title":"Track Upload Status","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/syncing","content":"","keywords":""},{"title":"Generate the tag‚Äã","type":1,"pageTitle":"Track Upload Status","url":"/docs/develop/access-the-swarm/syncing#generate-the-tag","content":"While the automatically-generated tag is convenient, with big uploads it might take a while until the Bee API returns the headers. What you want to do in this case is to pre-generate the tag and pass this tag along with the upload command. Generate a tag: curl -X POST http://localhost:1633/tags  The uid value is your tag:  { &quot;split&quot;: 0, &quot;seen&quot;: 0, &quot;stored&quot;: 0, &quot;sent&quot;: 0, &quot;synced&quot;: 0, &quot;uid&quot;: 5, &quot;address&quot;: &quot;&quot;, &quot;startedAt&quot;: &quot;2023-08-31T06:46:41.574003454Z&quot; }  info In order to upload your data to swarm, you must agree to burn some of your xBZZ to signify that the content is important. Before you progress to the next step, you must buy stamps! See this guide on how to purchase an appropriate batch of stamps. Pass the tag along with the upload: curl --data-binary @bee.jpg \\ -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; \\ -H &quot;Swarm-Tag: 5&quot; \\ &quot;http://localhost:1633/bzz?name=bee.jpg&quot;  "},{"title":"Ask for the Current Status‚Äã","type":1,"pageTitle":"Track Upload Status","url":"/docs/develop/access-the-swarm/syncing#ask-for-the-current-status","content":"To get the current status of an upload, send a GET request to the tag/&lt;Swarm-Tag&gt; API endpoint. curl http://localhost:1633/tags/5 | jq  The response contains all the information that you need to follow the status of your file as it is synced with the network. info The numbers that the tags endpoint returns under total, processed and synced are denominated in chunks, i.e. Swarm's 4kb data units. "},{"title":"Ultra Light Nodes","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/ultra-light-nodes","content":"Ultra Light Nodes danger When running without a blockchain connections, bandwidth incentive payments (SWAP) cannot be made so there is a risk of getting blocklisted by other peers for unpaid services. Configuration‚Äã To run Bee as an ultra-light node full-node and swap-enable must both be set to false, and the blockchain-rpc-endpoint value should be set to an empty string &quot;&quot; or commented out in the configuration. Mode of Operation‚Äã The target audience for this mode of operations are users who want to try out running a node but don't want to go through the hassle of blockchain onboarding. Ultra-light nodes will be able to download data as long as the data consumed does not exceed the payment threshold (payment-threshold in configuration) set by peers they connect to. Running Bee without a connected blockchain backend, however, imposes some limitations: Can't do overlay verificationCan't do SWAP settlements Since we can't buy postage stamps: Can't send PSS messagesCan't upload data to the network","keywords":""},{"title":"Upload and Download Files","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/upload-and-download","content":"","keywords":""},{"title":"Overview‚Äã","type":1,"pageTitle":"Upload and Download Files","url":"/docs/develop/access-the-swarm/upload-and-download#overview","content":"To upload data to the swarm, you must perform the following steps: Fund your node's wallet with xBZZ.Purchase a batch of stamps with your xBZZ.Wait for the batch to propogate into the network.Upload your content, specifying the batch id so that Bee can attach stamps to your chunks.Download your content using your content's hash. "},{"title":"Purchasing Your Batch of Stamps‚Äã","type":1,"pageTitle":"Upload and Download Files","url":"/docs/develop/access-the-swarm/upload-and-download#purchasing-your-batch-of-stamps","content":"In order to upload your data to swarm, you must agree to burn (spend) some of your xBZZ to signify to storer and fowarder nodes that this content is valued. Before you proceed to the next step, you must buy stamps! See this guide on how to purchase an appropriate batch of stamps. "},{"title":"Upload‚Äã","type":1,"pageTitle":"Upload and Download Files","url":"/docs/develop/access-the-swarm/upload-and-download#upload","content":"Once your Bee node is running, a HTTP API is enabled for you to interact with. The command line utility curl is a great way to interact with a Bee node's API. First, let's check to see if the API is running as expected... curl http://localhost:1633  Ethereum Swarm Bee  Once running, a file can be uploaded by making an HTTP POST request to the files endpoint of the Bee API. Here, you must specify your Batch ID in the Swarm-Postage-Batch-Id header as follows. curl --data-binary &quot;@bee.jpg&quot; -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; http://localhost:1633/bzz  We may also pass the appropriate mime type in the Content-Type header, and a file name to the name query parameter so that the file will be correctly handled by web browsers and other applications. curl --data-binary &quot;@bee.jpg&quot; -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; -H &quot;Content-Type: image/jpg&quot; http://localhost:1633/bzz  danger Data uploaded to the swarm is always public. In Swarm, sensitive files must be encryptedbefore uploading to ensure their contents always remains private. When succesful, a JSON formatted response will be returned, containing a swarm reference or hash which is the address of the uploaded file, for example: { &quot;reference&quot;: &quot;22cbb9cedca08ca8d50b0319a32016174ceb8fbaa452ca5f0a77b804109baa00&quot; }  Keep this address safe, as we'll use it to retrieve our content later on. In Swarm, every piece of data has a unique address which is a unique and reproducible cryptographic hash digest. If you upload the same file twice, you will always receive the same hash. This makes working with data in Swarm super secure! info If you are uploading a large file it is useful to track the status of your upload as it is processed into the network. To improve the user experience, learn how to follow the status of your upload. Once your file has been completely synced with the network, you will be able to turn off your computer and other nodes will take over to serve the data for you! "},{"title":"Download‚Äã","type":1,"pageTitle":"Upload and Download Files","url":"/docs/develop/access-the-swarm/upload-and-download#download","content":"Once your file is uploaded into the swarm, it can be retrieved with a simple HTTP GET request. Substitute the hash in the last part of the URL with the reference to your own data. curl -OJL http://localhost:1633/bzz/042d4fe94b946e2cb51196a8c136b8cc335156525bf1ad7e86356c2402291dd4/  You may even simply navigate to the URL in your browser: http://localhost:1633/bzz/22cb...aa00 "},{"title":"Public Gateways‚Äã","type":1,"pageTitle":"Upload and Download Files","url":"/docs/develop/access-the-swarm/upload-and-download#public-gateways","content":"To share files with someone who isn't running a Bee node yet, simply change the host in the link to be one of our public gateways. Send the link to your friends, and they will be able to download the file too! https://download.gateway.ethswarm.org/bzz/22cb...aa00/ "},{"title":"Upload a Directory","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/upload-a-directory","content":"","keywords":""},{"title":"Upload the Directory Containing Your Website‚Äã","type":1,"pageTitle":"Upload a Directory","url":"/docs/develop/access-the-swarm/upload-a-directory#upload-the-directory-containing-your-website","content":"First, use the tar command line utility to create an archive containing all the files of your directory. If uploading a website, we must take care to ensure that the index.html file is at the root of the directory tree. tree my_website &gt; my_website ‚îú‚îÄ‚îÄ assets ‚îÇ ‚îî‚îÄ‚îÄ style.css ‚îú‚îÄ‚îÄ index.html ‚îî‚îÄ‚îÄ error.html  Use the following command to ensure that the tar package maintains the correct directory structure: cd my_website tar -cf ../my_website.tar . cd ..  Next, simply POST the tar file as binary data to Bee's dir endpoint, taking care to include the header Content Type: application/x-tar. info In order to upload your data to swarm, you must agree to burn some of your xBZZ to signify to storer and fowarder nodes that the content is important. Before you progress to the next step, you must buy stamps! See this guide on how to purchase an appropriate batch of stamps. curl \\ -X POST \\ -H &quot;Content-Type: application/x-tar&quot; \\ -H &quot;Swarm-Index-Document: index.html&quot; \\ -H &quot;Swarm-Error-Document: error.html&quot; \\ -H &quot;Swarm-Collection: true&quot; \\ -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; \\ --data-binary @my_website.tar http://localhost:1633/bzz  info For instances where a single page app has a JavaScript router that handles url queries itself, simply pass index.html as the error document. Bee will pass over control to the JavaScript served by the index.html file in the circumstance that a path does not yield a file from the manifest. When the upload is successful, Bee will return a JSON document containing the Swarm Reference. { &quot;reference&quot;: &quot;b25c89a401d9f26811680476619a1eb4a4e189e614bc6161cbfd8b343214917b&quot; }  Now, simply navigate your browser to view the reference using the bzz endpoint and your website will be served! http://localhost:1633/bzz/b25c89a...214917b/ Other files are served at their relative paths, e.g: http://localhost:1633/bzz/b25c89a...214917b/assets/style.css Once your data has been fully processed into the network, you will then be able to retrieve it from any Bee node. https://gateway.ethswarm.org/bzz/b25c89a...214917b/index.html If you are not able to download your file from a different Bee node, you may be experiencing connection issues, see troubleshooting connectivity for assistance. "},{"title":"Developer mode","type":0,"sectionRef":"#","url":"/docs/develop/bee-developers/bee-dev-mode","content":"","keywords":""},{"title":"Configuration options‚Äã","type":1,"pageTitle":"Developer mode","url":"/docs/develop/bee-developers/bee-dev-mode#configuration-options","content":"It accepts the same configuration options as a normal bee but it will ignore the ones that are not relevant (accounting, networking, blockchain etc). info If you enable the restricted mode but don't provide an admin password it will use the default value 'hello'. Then you can use it in your header: Authorization: Bearer OmhlbGxv "},{"title":"Useful Developer Info","type":0,"sectionRef":"#","url":"/docs/develop/bee-developers/useful-dev-info","content":"Useful Developer Info Welcome to the Dev area! We love PR's! üêù We would would love you to get involved with our Github repo. All the action can be found on our Discord Server. Sign up and get involved with our buzzing hive of daily dev chat. If you would like to contribute, please read the coding guidelines before you get started. Installation from source is described in the Installation. Testing a connection with PingPong protocol To check if two nodes are connected and to see the round trip time for message exchange between them, get the overlay address from one node, for example local node 2: curl localhost:1835/addresses Make sure that Debug API is enabled and addresses configured as in examples above. And use that address in the Debug API call on another node, for example, local node 1: curl -XPOST localhost:1735/pingpong/d4440baf2d79e481c3c6fd93a2014d2e6fe0386418829439f26d13a8253d04f1 Generating protobuf To process protocol buffer files and generate the Go code from it two tools are needed: protocprotoc-gen-gogofaster Makefile rule protobuf can be used to automate protoc-gen-gogofaster installation and code generation: make protobuf Tracing Developers can gain an additional level of insight into the node by enabling tracing. To make use of Tracing, we advice to make use of jaeger. Set up tracing by: Start jaeger:docker run -p 6831:6831/udp -p 16686:16686 jaegertracing/all-in-one:latest start locally two bee nodes (different data dirs and ports) and connect them (see &quot;Start a test network&quot; in the advanced section) with --tracing flag provided for both nodes Make a call to the PingPong API on one of the two nodes (curl -XPOST localhost:1735/pingpong/&lt;overlay address other node&gt;). Validate tracing in the web interface (http://localhost:16686/).","keywords":""},{"title":"Starting a Test Network","type":0,"sectionRef":"#","url":"/docs/develop/bee-developers/starting-a-test-network","content":"","keywords":""},{"title":"Start a network on your own computer‚Äã","type":1,"pageTitle":"Starting a Test Network","url":"/docs/develop/bee-developers/starting-a-test-network#start-a-network-on-your-own-computer","content":""},{"title":"Configuration‚Äã","type":1,"pageTitle":"Starting a Test Network","url":"/docs/develop/bee-developers/starting-a-test-network#configuration","content":"Starting a network is easiest achieved by making use of configuration files. We need at least two nodes to start a network. Hence, below two configuration files are provided. Save them respectively as config_1.yaml and config_2.yaml. config_1.yaml network-id: 7357 api-addr: :1633 p2p-addr: :1634 debug-api-addr: 127.0.0.1:1635 debug-api-enable: true bootnode: &quot;&quot; data-dir: /tmp/bee/node1 password: some pass phze swap-enable: false  config_2.yaml network-id: 7357 api-addr: :1733 p2p-addr: :1734 debug-api-addr: 127.0.0.1:1735 debug-api-enable: true data-dir: /tmp/bee/node2 bootnode: &quot;&quot; password: some pass phze welcome-message: &quot;Bzz Bzz Bzz&quot; swap-enable: false  Note that for each node, we provide a different api-addr,debug-api-addr. If we had not specified different addresses here, we would get an address already in use error, as no two applications can listen to the same port. We also specify a differentp2p-addr. If we had not, our nodes would not be able to communicate with each other. We also specify a separate data-dir for each node, as each node must have its own separate key and chunk data store. We also provide a network-id, so that our network remains separate from the Swarm mainnet, which has network-id 1. Nodes will not connect to peers which have a different network id. We also set our bootnode to be the empty string &quot;&quot;. A bootnode is responsible for bootstrapping the network so that a new node can find its first few peers before it begins its own journey to find friends in the Swarm. In Swarm any node can be used as a bootnode. Later, we will manually join our nodes together so in this case a bootnode isn't required. Finally, note the welcome-message in the first nodes configuration file. This is a friendly feature allowing you to send a message to peers that connect to you! "},{"title":"Starting Your Nodes‚Äã","type":1,"pageTitle":"Starting a Test Network","url":"/docs/develop/bee-developers/starting-a-test-network#starting-your-nodes","content":"Now we have created our configuration files, let's start our nodes by running bee start --config config_1.yaml, then in another Terminal session, run bee start --config-file config_2.yaml. We can now inspect the state of our network by sending HTTP requests to the Debug API.. curl -s http://localhost:1635/topology | jq .connected  0  curl -s http://localhost:1735/topology | jq .connected  0  No connections yet? Right! Let's remedy that! info Here we are using the jq command line utility to count the amount of objects in the peers array in the JSON response we have received from our Debug API, learn more about how to install and use jq here. "},{"title":"Making a network‚Äã","type":1,"pageTitle":"Starting a Test Network","url":"/docs/develop/bee-developers/starting-a-test-network#making-a-network","content":"In order to create a network from our two isolated nodes, we must first instruct our nodes to connect to each other. This step is not explicitly needed if you connect to the main Swarm network, as the default bootnodes in the Swarm network will automatically suggest peers. First, we will need to find out the network address of the first node. To do this, we send a HTTP request to the addresses endpoint of the Debug API. curl localhost:1635/addresses | jq  { &quot;overlay&quot;: &quot;f57a65207f5766084d3ebb6bea5e2e4a712504e54d86a00961136b514f07cdac&quot;, &quot;underlay&quot;: [ &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot;, &quot;/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot;, &quot;/ip6/::1/tcp/1634/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot;, &quot;/ip4/xx.xx.xx.xx/tcp/40317/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot; ] }  Here, we get firstly the overlay address - this is the permanent address Swarm uses as your anonymous identity in the network and secondly, a list of all the multiaddresses, which are physical network addresses at which you node can be found by peers. Note the addresses starting with an /ip4, followed by 127.0.0.1, which is the localhost internal network in your computer. Now we can use this full address to be the bootnode of our second node so that when it starts up, it goes to this address and both nodes become peers of each other. Let's add this into our config_2.yaml file. config_2.yaml network-id: 7357 api-addr: :1733 p2p-addr: :1734 debug-api-addr: 127.0.0.1:1735 debug-api-enable: true data-dir: /tmp/bee/node2 bootnode: &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot; password: some pass phze welcome-message: &quot;Bzz Bzz Bzz&quot; swap-enable: false  Now, we can shut our second node and reboot with the new configuration. Look at the the output for your first node, you should see our connection message! Let's also verify that we can see both nodes in using each other's Debug API's. curl -s http://localhost:1635/peers | jq  curl -s http://localhost:1635/peers | jq  Congratulations! You have made your own tiny two bee Swarm! üêù üêù "},{"title":"Bee JS","type":0,"sectionRef":"#","url":"/docs/develop/dapps-on-swarm/bee-js","content":"Bee JS Bee-js is Bee's complementary JavaScript library. It is the technology underpinning the swarm-cli and bee-dashboard tools and is a powerful tool for building completely decentralised apps. For more information on how to develop with Bee without blowing all your xBZZ, read this guide See the bee-js documentation for detailed information on using and installing the library.","keywords":""},{"title":"Chunk Types","type":0,"sectionRef":"#","url":"/docs/develop/dapps-on-swarm/chunk-types","content":"","keywords":""},{"title":"Content Addressed Chunks‚Äã","type":1,"pageTitle":"Chunk Types","url":"/docs/develop/dapps-on-swarm/chunk-types#content-addressed-chunks","content":"Content addressed chunks are chunks whose addresses are determined by the BMT hashing algorithm. This means you can be sure that all content addressed chunks content is already verified - no more need to check md5 hashes of your downloaded data! danger To be able trust your data, you must run your own Bee node that automatically verifies data, using gateways puts your trust in the gateway operators. "},{"title":"Trojan Chunks‚Äã","type":1,"pageTitle":"Chunk Types","url":"/docs/develop/dapps-on-swarm/chunk-types#trojan-chunks","content":"Trojan chunks are a special version of content addressed chunks that have been 'mined' so that their natural home is in a particular area of the Swarm. If the destination node is in the right neighbourhood, it will be able to receive and decrypt the message. See PSS for more information, or check out the bee-js bindings. "},{"title":"Single Owner Chunks‚Äã","type":1,"pageTitle":"Chunk Types","url":"/docs/develop/dapps-on-swarm/chunk-types#single-owner-chunks","content":"Single Owner Chunks are distinct from Trojan and Content Addressed Chunks and are the only other type of chunk which is allowed in Swarm. These chunks represent part of Swarm's address space which is reserved just for your personal Ethereum key pair! Here you can write whatever you'd please. Single Owner Chunks are the technology that powers Swarm's feeds, but they are capable of much more! Look out for more chats about this soon, and for more info read The Book of Swarm . "},{"title":"Custom Chunk Types‚Äã","type":1,"pageTitle":"Chunk Types","url":"/docs/develop/dapps-on-swarm/chunk-types#custom-chunk-types","content":"Although all chunks must satisfy the constraints of either being addressed by the BMT hash of their payload, or assigned by the owner of an Ethereum private key pair, so much more is possible. How else can you use the DISC to distribute and store your data? We're excited to see what you come up with! üí° Share your creations in the #develop-on-swarm channel of our Discord Server. "},{"title":"Feeds","type":0,"sectionRef":"#","url":"/docs/develop/dapps-on-swarm/feeds","content":"","keywords":""},{"title":"What are Feeds?‚Äã","type":1,"pageTitle":"Feeds","url":"/docs/develop/dapps-on-swarm/feeds#what-are-feeds","content":"A feed is a collection of Single Owner Chunks with predicatable addresses. This enables creators to upload pointers to data so that consumers of the feed are able to find the data in Swarm using only an Ethereum address and Topic ID. "},{"title":"Creating and Updating a Feed‚Äã","type":1,"pageTitle":"Feeds","url":"/docs/develop/dapps-on-swarm/feeds#creating-and-updating-a-feed","content":"In order to edit a feed, you will need to sign your chunks using an Ethereum keypair. For the intrepid, check out the The Book of Swarm on precise details on how to do this. For the rest of us, both bee-jsand swarm-cli provide facilities to achieve this using JavaScript and a node-js powered command line tool respectively. "},{"title":"No More ENS Transaction Charges‚Äã","type":1,"pageTitle":"Feeds","url":"/docs/develop/dapps-on-swarm/feeds#no-more-ens-transaction-charges","content":"Swarm's feeds provide the ability to update your immutable content in a mutable world. Simply reference your feed's manifest address as the content hash in your ENS domain's resolver, and Bee will automatically provide the latest version of your website. "},{"title":"Use Cases for Feeds‚Äã","type":1,"pageTitle":"Feeds","url":"/docs/develop/dapps-on-swarm/feeds#use-cases-for-feeds","content":"Feeds are a hugely versatile data structure. Key Value Store‚Äã Use bee-js to use feeds to store values as a simple key value store in your JavaScript application. No more need for servers and databases! Store the History of a File‚Äã Use swarm-cli to store a file at the same location, and update whenever you like without changing the address. "},{"title":"Develop on Bee","type":0,"sectionRef":"#","url":"/docs/develop/dapps-on-swarm/develop-on-bee","content":"","keywords":""},{"title":"Setting Up Bee for Developing Dapps‚Äã","type":1,"pageTitle":"Develop on Bee","url":"/docs/develop/dapps-on-swarm/develop-on-bee#setting-up-bee-for-developing-dapps","content":"To develop apps on Bee, you might need to adjust the following settings. Check out the brand new bee-factory for information on how to run a self contained development environment so you can go wild in swarm without spending all your xBZZ on swap and stamps! "},{"title":"Hosting Your Dapps & Storing Their Data‚Äã","type":1,"pageTitle":"Develop on Bee","url":"/docs/develop/dapps-on-swarm/develop-on-bee#hosting-your-dapps--storing-their-data","content":"Swarm is hugely versatile, but at a very basic level you can think of it as storage for your dapps data that is too big for blockchain, but still needs to live in our totally decentralised universe. Swarm is perfect for storing your NFT meta-data and images in a web3 way that won't break the bank and can live forever! "},{"title":"Your Data Structures on Swarm‚Äã","type":1,"pageTitle":"Develop on Bee","url":"/docs/develop/dapps-on-swarm/develop-on-bee#your-data-structures-on-swarm","content":"As well as your simple data needs, Swarm also provides key-value store / pubsub type primitives, and allows users to store authenticated data signed using a regular Ethereum Wallet. Seechunk types andfeeds for more info! Let us know what you come up with in the#develop-on-swarm room in ourDiscord Server. "},{"title":"Bee Proxy Mode‚Äã","type":1,"pageTitle":"Develop on Bee","url":"/docs/develop/dapps-on-swarm/develop-on-bee#bee-proxy-mode","content":"With Bee running as a proxy inLight Node mode on your end-user's computers, your applications can have privileged access to all sorts of useful tools and ways to interact with the swarm. Check out theAPI andDebug API as well asbee-js for details on what you can do with your Bee! "},{"title":"Dapps With Swarm Gateways‚Äã","type":1,"pageTitle":"Develop on Bee","url":"/docs/develop/dapps-on-swarm/develop-on-bee#dapps-with-swarm-gateways","content":"If you want your users to be able to access Swarm without running their own Bee node, for the time being you will need to access Bee in gateway mode. Join us in the#develop-on-swarm room in ourDiscord Server for more information on how to allow your web app users to read and write to the swarm. "},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/develop/dapps-on-swarm/introduction","content":"","keywords":""},{"title":"Developing on Bee‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/dapps-on-swarm/introduction#developing-on-bee","content":"Bee isn't just for mining xBZZ - learn how to use Bee for all your dapp development, production infrastructure and deployment needs! "},{"title":"Bee JS‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/dapps-on-swarm/introduction#bee-js","content":"Our maverick JavaScript team, the Bee-Gees (üï∫), have been working hard in the last few months to build some impressive tools for all you budding dapp developer Bees to get stuck into! Find out how to use the bee-js JavaScript library to start creating your own that live and work on Swarm! "},{"title":"Chunk Types‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/dapps-on-swarm/introduction#chunk-types","content":"Swarm contains 3 types of chunks which enable us to build novel structures of how data can be stored in the swarm - in a completely decentralised way. Learn more aboutchunk typesto change the way you deal with data in your dapps forever! "},{"title":"Feeds‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/dapps-on-swarm/introduction#feeds","content":"Swarm's single owner chunks have been cleverly combined to create user generated feeds in the swarm, see this example of how chunks are combined into a useful data structure you can use to build amazing applications. "},{"title":"PSS‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/develop/dapps-on-swarm/introduction#pss","content":"Hey there! Pss! ü§´ Swarm's trojan chunks are implemented in Bee to deliver Postal Service on Swarm - a pub-sub system that provides a totally leak-proof messaging system over the swarm. "},{"title":"Swarm","type":0,"sectionRef":"#","url":"/docs/develop/introduction","content":"","keywords":""},{"title":"Swarm Whitepaper and The Book of Swarm‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/develop/introduction#swarm-whitepaper-and-the-book-of-swarm","content":"What happens with a Bee node after startup? Want to know more about the Swarm technology behind Bee? Want to make your own client? Head over to the Learn section to start learning about the technology and concepts powering Swarm. For more in-depth documentation, you can get started with the Swarm Whitepaper and for an even deeper dive, The Book of Swarm, our 250 page guide to the tech underpinning the Swarm network. "},{"title":"Bonding Curve‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/develop/introduction#bonding-curve","content":"Find more information on the bonding curve, including its source code in the github repository. Bee "},{"title":"Working With Bee‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/develop/introduction#working-with-bee","content":"Once Bee is installed, find out how to configure the software, interact with the API, monitor what Bee is up to, and make those all important backups in the working with Bee section. "},{"title":"Access the Swarm‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/develop/introduction#access-the-swarm","content":"To learn more about how to get the most out of Bee, find out how to access the swarm section so you can share files with your friends, use Bee to host a website on a public Swarm Gateway, and much more! "},{"title":"API Reference‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/develop/introduction#api-reference","content":"Find detailed information on all the endpoints available in the autogenerated API reference and Debug API reference guides. "},{"title":"Dapps‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/develop/introduction#dapps","content":"Swarm is all about dapps. It strives to provide the most developer friendly environment on which to build decentralised organisations. Built on the principles of functionality, flexibility and accessibility, Bee provides high level constructs for file storage, feeds and key-value stores, while also providing the low level access with libraries that create Single Owner and Trojan chunks clientside, with total e2e privacy. Learn more about how to develop on Swarm. "},{"title":"Development‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/develop/introduction#development","content":"Join our efforts! Are you up to the challenge of helping to create Bee and the other incredible technologies that are being build on top of it? You are invited to contribute code to the Bee client or any of the other projects in Swarm'sEthersphere. "},{"title":"Community‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/develop/introduction#community","content":"There is a vibrant and buzzing community behind Swarm - get involved in one of our group channels: SwarmDiscordTwitter @ethswarmreddit channelBlog "},{"title":"PSS Messaging","type":0,"sectionRef":"#","url":"/docs/develop/dapps-on-swarm/pss","content":"","keywords":""},{"title":"Subscribe and Receive Messages‚Äã","type":1,"pageTitle":"PSS Messaging","url":"/docs/develop/dapps-on-swarm/pss#subscribe-and-receive-messages","content":"Once your Bee node is up and running, you will be able to subscribe to feeds using WebSockets. For testing, it is useful to use the websocat command line utility. Here we subscribe to the topic test-topic websocat ws://localhost:1633/pss/subscribe/test-topic  Our node is now watching for new messages received in its nearest neighbourhood. info Because a message is disguised as a normal chunk in Swarm, you will receive the message upon syncing the chunk, even if your node is not online at the moment when the message was send to you. "},{"title":"Send Messages‚Äã","type":1,"pageTitle":"PSS Messaging","url":"/docs/develop/dapps-on-swarm/pss#send-messages","content":"Messages can be sent simply by sending a POST request to the PSS API endpoint. When sending messages, we must specify a 'target' prefix of the recipient's Swarm address, a partial address representing their neighbourhood. Currently the length of this prefix is recommended to be two bytes, which will work well until the network has grown to a size of ca. 20-50K nodes. We must also provide the public key, so that Bee can encrypt the message in such a way that it may only be read by the intended recipient. For example, if we want to send a PSS message with topic test-topic to a node with address... 7bc50a5d79cb69fa5a0df519c6cc7b420034faaa61c175b88fc4c683f7c79d96 ...and public key... 0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd ...we must include the target 7bc5 and the public key itself as a query argument. curl -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; -XPOST \\ localhost:1833/pss/send/test-topic/7bc5?recipient=0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd \\ --data &quot;Hello Swarm&quot;  More information on how to buy a postage stamp batch and get its batch id can be found here. "},{"title":"Send Messages in a Test Network‚Äã","type":1,"pageTitle":"PSS Messaging","url":"/docs/develop/dapps-on-swarm/pss#send-messages-in-a-test-network","content":"Now, let's see this in action by setting up two Bee nodes on a test network, connecting them, and sending PSS messages from one to the other. First start two Bee nodes. We will start them with distinct ports for the API, Debug API, and p2p port, since they will be running on the same computer. Run the following command to start the first node. Note that we are passing &quot;&quot; to the --bootnode argument so that our nodes will not connect to a network. bee start \\ --api-addr=:1833 \\ --debug-api-enable \\ --debug-api-addr=:1835 \\ --data-dir=/tmp/bee2 \\ --bootnode=&quot;&quot; \\ --p2p-addr=:1834 \\ --blockchain-rpc-endpoint=http://localhost:8545  We must make a note of the Swarm overlay address, underlay address and public key which are created once each node has started. We find this information from the addresses endpoint of the Debug API. curl -s localhost:1835/addresses | jq  { &quot;overlay&quot;: &quot;46275b02b644a81c8776e2459531be2b2f34a94d47947feb03bc1e209678176c&quot;, &quot;underlay&quot;: [ &quot;/ip4/127.0.0.1/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq&quot;, &quot;/ip4/192.168.0.10/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq&quot;, &quot;/ip6/::1/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq&quot; ], &quot;ethereum&quot;: &quot;0x0b546f2817d0d889bd70e244c1227f331f2edf74&quot;, &quot;public_key&quot;: &quot;03660e8dbcf3fda791e8e2e50bce658a96d766e68eb6caa00ce2bb87c1937f02a5&quot; }  Now the same for the second node. bee start \\ --api-addr=:1933 \\ --debug-api-enable \\ --debug-api-addr=:1935 \\ --data-dir=/tmp/bee3 \\ --bootnode=&quot;&quot; \\ --p2p-addr=:1934 \\ --blockchain-rpc-endpoint=http://localhost:8545  curl -s localhost:1935/addresses | jq  { &quot;overlay&quot;: &quot;085b5cf15a08f59b9d64e8ce3722a95b2c150bb6a2cef4ac8b612ee8b7872253&quot;, &quot;underlay&quot;: [ &quot;/ip4/127.0.0.1/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr&quot;, &quot;/ip4/192.168.0.10/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr&quot;, &quot;/ip6/::1/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr&quot; ], &quot;ethereum&quot;: &quot;0x9ec47bd86a82276fba57f3009c2f6b3ace4286bf&quot;, &quot;public_key&quot;: &quot;0289634662d3ed7c9fb1d7d2a3690b69b4075cf138b683380023d2edc2e6847826&quot; }  Because we configured the nodes to start with no bootnodes, neither node should have peers yet. curl -s localhost:1835/peers | jq  curl -s localhost:1935/peers | jq  { &quot;peers&quot;: [] }  Let's connect node 2 to node 1 using the localhost (127.0.0.1) underlay address for node 1 that we have noted earlier. curl -XPOST \\ localhost:1935/connect/ip4/127.0.0.1/tcp/1834/p2p/16Uiu2HAmP9i7VoEcaGtHiyB6v7HieoiB9v7GFVZcL2VkSRnFwCHr  Now, if we check our peers endpoint for node 1, we can see our nodes are now peered together. curl -s localhost:1835/peers | jq  { &quot;peers&quot;: [ { &quot;address&quot;: &quot;a231764383d7c46c60a6571905e72021a90d506ef8db06750f8a708d93fe706e&quot; } ] }  Of course, since we are p2p, node 2 will show node 1 as a peer too. curl -s localhost:1935/peers | jq  { &quot;peers&quot;: [ { &quot;address&quot;: &quot;7bc50a5d79cb69fa5a0df519c6cc7b420034faaa61c175b88fc4c683f7c79d96&quot; } ] }  We will use websocat to listen for the PSS messages' Topic IDtest-topic on our first node. websocat ws://localhost:1833/pss/subscribe/test-topic  Now we can use PSS to send a message from our second node to our first node. Since our first node has a 2 byte address prefix of a231, we will specify this as the targets section in our POST request's URL. We must also include the public key of the recipient as a query parameter so that the message can be encrypted in a way only our recipient can decrypt. curl \\ -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; -XPOST &quot;localhost:1933/pss/send/test-topic/7bc5?recipient=0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd&quot; \\ --data &quot;Hello Swarm&quot;  The PSS API endpoint will now create a PSS message for its recipient in the form of a 'Trojan Chunk' and send this into the network so that it may be pushed to the correct neighbourhood. Once it is received by its recipient it will be decrypted and determined to be a message with the topic we are listening for. Our second node will decrypt the data and we'll see a message pop up in our websocat console! websocat ws://localhost:1833/pss/subscribe/test-topic  Hello Swarm  Congratulations! üéâ You have sent your first encrypted, zero leak message over Swarm! "},{"title":"Swarm","type":0,"sectionRef":"#","url":"/docs/learn/cut-content","content":"","keywords":""},{"title":"Swarm Whitepaper and The Book of Swarm‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#swarm-whitepaper-and-the-book-of-swarm","content":"What happens with a Bee node after startup? Want to know more about the Swarm technology behind Bee? Want to make your own client? Read the Swarm Whitepaper and for a deeper dive, The Book of Swarm, our 250 page guide to the tech underpinning the Swarm network. "},{"title":"Bonding Curve‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#bonding-curve","content":"Find more information on the bonding curve, including its source code in the github repository. Bee "},{"title":"Installation‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#installation","content":"Don't have Bee installed yet? It's easy! Head over to the installation section to get Bee up and running. "},{"title":"Working With Bee‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#working-with-bee","content":"Once Bee is installed, find out how to configure the software, interact with the API, monitor what Bee is up to, and make those all important backups in the working with Bee section. "},{"title":"Access the Swarm‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#access-the-swarm","content":"To learn more about how to get the most out of Bee, find out how to access the swarm section so you can share files with your friends, use Bee to host a website on a public Swarm Gateway, and much more! "},{"title":"API Reference‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#api-reference","content":"Find detailed information on all the endpoints available in the autogenerated API reference and Debug API reference guides. "},{"title":"Dapps‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#dapps","content":"Swarm is all about dapps. It strives to provide the most developer friendly environment on which to build decentralised organisations. Built on the principles of functionality, flexibility and accessibility, Bee provides high level constructs for file storage, feeds and key-value stores, while also providing the low level access with libraries that create Single Owner and Trojan chunks clientside, with total e2e privacy. Learn more about how to develop on Swarm. "},{"title":"Incentives‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#incentives","content":"Need even more incentive to get involved with the wonderful world of Swarm? Find out how you can earn xBZZ tokens for storing and distributing your share of the world's data - sharing is caring! "},{"title":"Find Out More‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#find-out-more","content":"Read The Book of Swarm, our 250 page epic guide to the future tech underpinning the Swarm network. For a lighter read, read the Swarm Whitepaper. "},{"title":"Development‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#development","content":"Join our efforts! Are you up to the challenge of helping to create Bee and the other incredible technologies that are being build on top of it? You are invited to contribute code to the Bee client or any of the other projects in Swarm'sEthersphere. "},{"title":"Community‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#community","content":"There is a vibrant and buzzing community behind Swarm - get involved in one of our group channels: SwarmDiscordTwitter @ethswarmreddit channelBlog "},{"title":"Reporting a bug‚Äã","type":1,"pageTitle":"Swarm","url":"/docs/learn/cut-content#reporting-a-bug","content":"If your Bee isn't working, get in touch with the #node-operators channel on Discord or let us know on GitHub! üêù Thanks for beeing here, Love and Bees from the Swarm Team x "},{"title":"Awesome-Swarm","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/awesome","content":"","keywords":""},{"title":"Services‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#services","content":"Bee - Also referred to as the node or the client, this service allows you to join the Swarm network "},{"title":"Libraries‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#libraries","content":"Bee-JS - A high-level Javascript library to interact with Bee through its REST API Mantaray-JS - A low-level Swarm manifest manipulation library Sepatree - The SepaTree data structure abstracted on Swarm BeeJeez - Javascript implementation of the handshake protocol and others based on libp2p "},{"title":"CI/CD‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#cicd","content":"Beekeeper - Orchestrate and test Bee clusters through Kubernetes Bee Factory - Sets up a Dockerized stack of Bee nodes including Ganache blockchain Bee Factory VPS - Provides an automatized way to set up Bee Factory on a fresh Ubuntu VPS Beeload Action - GitHub Actions workflow for uploading data to the Swarm network "},{"title":"UI‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#ui","content":"Bee Dashboard - React project to troubleshoot and interact with your Bee node Gateway - Gateway to the Swarm project, for uploading, downloading and sharing assets on the network Pastebee - Pastebin, but on Swarm and with unstoppable publishing Chess UI - Play, store and share Chess games on Swarm "},{"title":"Tools‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#tools","content":"Swarm CLI - Do everything on Swarm with the power of the terminal Swarm Extension - Official extension that adds Swarm support and injects Bee library to the browser Pastebee CLI - Upload to Pastebee via the CLI and share the Swarm hash Swarm CID Converter - Convert Swarm hashes or links to CID and vice versa. Bee-AFS - FUSE filesystem for Bee Nextcloud Swarm Plugin - Plugin for bridging Nextcloud and Swarm. "},{"title":"Smart Contracts‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#smart-contracts","content":"Swap, Swear and Swindle - Protocols for peer-to-peer accounting Storage Incentives - Smart contracts providing the basis for Swarm's storage incentivization model "},{"title":"Documentation‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#documentation","content":"The Book of Swarm - Storage and communication infrastructure for self-sovereign digital society back-end stack for the decentralised web Bee Docs - Documentation for the Swarm Bee Client. View at docs.ethswarm.org. Bee-JS Docs - Documentation for the Swarm Bee-js javascript library. View at bee-js.ethswarm.org. "},{"title":"Community / Ecosystem‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#community--ecosystem","content":"Fair data society - Ecosystem initiative for ethical Web3 FairOS - Distributed file system, key-value store and nosql store on Swarm (for developers) Fair Data Protocol roadmap enabling data interoperability - Develop your dapp on Swarm fast and in an interoperable way FDP play - CLI tool to spin up local development FDP environment and Bee cluster with Docker Blossom browser extension - Browser Extension based on Fair Data Protocol that acts as a web3 framework for dApps and a Fair Data Society account manager for end-users Fairdrive - Decentralised and unstoppable &quot;Dropbox&quot; for end-users and developers using Fair Data Protocol Fairdrive code - Code for decentralised and unstoppable &quot;Dropbox&quot; for end-users and developers using Fair Data Protocol Fairdrop - Decentralised file sharing Galileo - Open Street Maps on Swarm Dracula - Hackmd-like markdown editor that works with Swarm SwarmScan - Get network insights Etherna.io - Decentralised Youtube on Swarm Social Archive - Archive your social media Swapchat 2.0 - Decentralised, ephemeral, peer-to-peer, encrypted chat Hacker Manifesto - The Hacker Manifesto on Swarm with a community funded postage stamp SwarmNFT library - JavaScript library for creating NFTs on Ethereum-compatible blockchains and storing content on Swarm videoNFT - NFT live streaming with Swarm (winner of EthBerlin3 2022 Freedom to Transact Track) DeBoot - DeBoot is a project to research and implement approaches to bootloading OS images from a decentralized storage network such as Swarm or IPFS. Swarm DAppNode Package - Swarm DAppNode package for Swarm Mainnet with multi-platform (x86_64 and arm64) support. Testnet DAppNode packages can be found here. "},{"title":"Miscellaneous‚Äã","type":1,"pageTitle":"Awesome-Swarm","url":"/docs/learn/ecosystem/awesome#miscellaneous","content":"Swarm Bot - Discord bot handling commands related to Swarm and its community "},{"title":"Community","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/community","content":"","keywords":""},{"title":"Links‚Äã","type":1,"pageTitle":"Community","url":"/docs/learn/ecosystem/community#links","content":"Twitter Discord server Reddit GitHub Blog Homepage "},{"title":"Grants and Bounties","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/grants-bounties","content":"Grants and Bounties Swarm grants support many interesting projects that are already building their products on top of Swarm. Swarm bounties extend the ecosystem with tooling and infrastructure. If you have an idea for a project which uses Swarm's technology we welcome you to apply for a grant. Learn more about grants for building on Swarm at the EthSwarm homepage.","keywords":""},{"title":"Foundation","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/swarm-foundation","content":"","keywords":""},{"title":"Swarm Foundation‚Äã","type":1,"pageTitle":"Foundation","url":"/docs/learn/ecosystem/swarm-foundation#swarm-foundation","content":"The goal of Swarm Foundation is to support and contribute to the creation of technology whose code will be open to all and will allow the storage and exchange of data in a decentralised manner. To this end, the foundation intends in particular to promote and support a community and a sustainable and independent ecosystem for the development of free open source software (FLOSS), allowing for digital services coordinated by crypto-economic incentives that process, distribute and store data. Its mission is to empower digital freedom by promoting the development and maintenance of the Swarm network, the base layer of the emerging fair data economy, and the community surrounding this network. It does so by supporting many different initiatives, either through financial grants or other types of support, all of which is assessed on a case-by-case basis. "},{"title":"Fair Data Society","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/fair-data-society","content":"","keywords":""},{"title":"Links‚Äã","type":1,"pageTitle":"Fair Data Society","url":"/docs/learn/ecosystem/fair-data-society#links","content":"FDS YouTubeFDS HomepageFDS DiscordFDS TwitterFDS GitHub "},{"title":"Glossary","type":0,"sectionRef":"#","url":"/docs/learn/glossary","content":"","keywords":""},{"title":"Swarm‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#swarm","content":"The Swarm network consists of a collection of Bee nodes which work together to enable decentralised data storage for the next generation of censorship-resistant, unstoppable, serverless dapps. Swarm is also the name of the core organization that oversees the development and success of the Bee Swarm as a whole. They can be found at ethswarm.org. "},{"title":"Gnosis Chain‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#gnosis-chain","content":"Gnosis Chain (previously known as xDai chain) is a PoS, EVM compatible Ethereum sidechain which uses the same addressing scheme as Ethereum. Swarm's smart contracts have been issued on Gnosis Chain. "},{"title":"Smart Contracts‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#smart-contracts","content":"Smart contracts are automatically executable code which can be published on a blockchain to ensure immutability. Swarm uses smart contracts on Gnosis Chain for a variety of key aspects of the network including incentivization, inter-node accounting, and payments for storage. "},{"title":"Bee‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#bee","content":"Swarm nodes are referred to as &quot;Bee&quot; nodes. Bee nodes can run on a wide variety of computer types including desktop computers, hobbyist computers like Raspberry Pi 4, remotely hosted virtual machines, and much more. When running, Bee nodes interact with Swarm smart contracts on Gnosis Chain and connect with other Bee nodes to form the Swarm network. Bee nodes can act as both client and service provider, or solely as client or service provider, depending on the needs of the node operator. Bee nodes pay each other for services on the Swarm network with the xBZZ token. "},{"title":"Overlay‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#overlay","content":"An overlay network is a virtual or logical network built on top of some lower level &quot;underlay&quot; network. Examples include the Internet as an overlay network built on top of the telephone network, and the p2p Bittorent network built on top of the Internet. With Swarm, the overlay network is based on a Kademlia DHT with overlay addresses derived from each node's Gnosis address. Swarm's overlay network addresses are permanent identifiers for each node and do not change over time. "},{"title":"Overlay Address‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#overlay-address","content":"Overlay addresses are a Keccak256 hash of a node‚Äôs Gnosis Chain address and the Swarm network ID (the Swarm network ID is included to prevent address collisions). The overlay address for a node does not change over time and is a permanent identifier for the node. Overlay addresses are used to group nodes into neighborhoods which are responsible for storing the same chunks of data. If not otherwise specified, when referring to a &quot;node address&quot;, it typically is referring to the overlay address, not the underlay address. The overlay address is the address used to determine which nodes connect to each other and which chunks nodes are responsible for. "},{"title":"Neighborhood‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#neighborhood","content":"Neighborhoods are nodes which are grouped together based on their overlay addresses and are responsible for storing the same chunks of data. The chunks which each neighborhood are responsible for storing are defined by the proximity order of the nodes and the chunks. See DISC for more details. "},{"title":"Underlay‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#underlay","content":"An underlay network is the low level network on which an overlay network is built. It allows nodes to find each other, communicate, and transfer data. Swarm's underlay network is a p2p network built with libp2p. Nodes are assigned underlay addresses which in contrast to their overlay addresses are not permanent and may change over time. "},{"title":"Swap‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#swap","content":"Swap is the p2p accounting protocol used for Bee nodes. It allows for the automated accounting and settlement of services between Bee nodes in the Swarm network. In the case that services exchanged between nodes is balanced equally, no settlement is necessary. In the case that one node is unequally indebted to another, settlement is made to clear the node's debts. Two key elements of the Swap protocol are cheques and the chequebook contract. "},{"title":"Cheques & Chequebook‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#cheques--chequebook","content":"Cheques are the off-chain method of accounting used by the Swap protocol where the issuing node signs a cheque specifying a beneficiary, a date, and an amount, and gives it to the recipient node as a token of promise to pay at a later date. The chequebook is the smart contract where the cheque issuer's funds are stored and where the beneficiary can cash the cheque received. The cheque and chequebook system reduces the number of required on-chain transactions by allowing multiple cheques to accumulate and be settled together as a group, and in the case that the balance of cheques between nodes is equal, no settlement transaction is required at all. "},{"title":"Postage Stamps‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#postage-stamps","content":"Postage stamps can be purchased with xBZZ and represent the right to store data on the Swarm network. In order to upload data to Swarm, a user must purchase a batch of stamps which they can then use to upload an equivalent amount of data to the network. "},{"title":"Kademlia‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#kademlia","content":"Kademlia is a distributed hash table (DHT) which is commonly used in distributed peer-to-peer networks. A distributed hash table is a type of hash table which is designed to be stored across a decentralized group of nodes in order to be persistent and fault tolerant. It is designed so that each node is only required to store a subset of the total set of key / value pairs. One of the unique features of the Kademlia DHT design is a distance metric based on the XOR bitwise operation. It is referred to as &quot;Kademlia distance&quot; or just &quot;distance&quot;. Swarm‚Äôs DISC uses a modified version of Kademlia which has been specialized for storage purposes, and understanding the concepts behind Kademlia is necessary for understanding Swarm. "},{"title":"Kademlia distance‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#kademlia-distance","content":"Within Kademlia, nodes have numeric ids with the same length and format taken from the same namespace as the keys of the key/value pairs. Kademlia distance between node ids and keys is calculated through the XOR bitwise operation done over any ids or keys. Note: For a Kademlia DHT, any standardized numerical format can be used for ids. However, within Swarm, ids are derived from a Keccak256 digest and are represented as 256 bit hexadecimal numbers. They are referred to as addresses or hashes. Example: We have a Kademlia DHT consisting of only ten nodes with ids of 1 - 10. We want to find the distance between node 4 and 7. In order to do that, we perform the XOR bitwise operation: 4 | 0100 7 | 0111 ‚Äî‚Äî‚Äî‚ÄîXOR 3 | 0011 And we find that the distance between the two nodes is 3. "},{"title":"Chunk‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#chunk","content":"When data is uploaded to Swarm, it is broken down into 4kb sized pieces which are each assigned an address in the same format as node‚Äôs overlay addresses. Chunk addresses are formed by taking the BMT hash of the chunk content along with an 8 byte measure of the number of the chunk‚Äôs child chunks, the span. The BMT hashing algorithm is based on the Keccac256 hashing algorithm, so it produces an address with the same format as that for the node overlay addresses. "},{"title":"Proximity Order (PO)‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#proximity-order-po","content":"Proximity Order is a concept defined in The Book of Swarm and is closely related to Kademlia distance. Proximity order is defined as the number of shared prefix bits of any two addresses. It is found by performing the XOR bitwise operation on the two addresses and counting how many leading 0 there are before the first 1. In other words, PO is equal to the number of shared binary prefix bits. Taking the previous example used in the Kademlia distance definition: 4 | 0100 7 | 0111 ‚Äî‚Äî‚Äî‚ÄîXOR 3 | 0011 In the result we find that the distance is 3, and that there are two leading zeros. Therefore for the PO of these two nodes is 2. Both Proximity Order and distance are measures of the relatedness of ids, however Kademlia distance is a more exact measurement. Taking the previous example used in the Kademlia distance definition: 5 | 0101 7 | 0111 ‚Äî‚Äî‚Äî‚ÄîXOR 2 | 0010 Here we find that the distance between 5 and 7 is 2, and the PO is also two. Although 5 is closer to 7 than 4 is to 7, they both fall within the same PO, since PO is only concerned with the shared leading bits. PO is a fundamental concept to Swarm‚Äôs design and is used as the basic unit of relatedness when discussing the addresses of chunks and nodes. PO is also closely related to the concept of depth. "},{"title":"Depth types‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#depth-types","content":"There are three fundamental categories of depth: "},{"title":"1. Topology related depth‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#1-topology-related-depth","content":"This depth is defined in relation to the connection topology of a single node as the subject in relation to all the other nodes it is connected to. It is referred to using several different terms which all refer to the same concept (Connectivity depth / Kademlia depth / neighbourhood depth / physical depth) Connectivity depth refers to the saturation level of the node‚Äôs topology - the level to which the topology of a node‚Äôs connections has Kademlia connectivity. Defined as one level deeper than the deepest fully saturated level. A PO is defined as saturated if it has at least the minimum required level of connected nodes, which is set at 8 nodes in the current implementation of Swarm. The output from the Bee debug API's topology endpoint:  Here we can see the depth is 8, meaning that PO bin 7 is the deepest fully saturated PO bin:  Here we can confirm that at least 8 nodes are connected in bin 7. Connectivity depth is defined from the point of view of individual nodes, it is not defined as characteristic of the entire network. However, given a uniform distribution of node ids within the namespace and given enough nodes, all nodes should converge towards the same connectivity depth. While this is sometimes referred to as Kademlia depth, the term ‚ÄúKademlia depth‚Äù is not defined within the original Kademlia paper, rather it refers to the depth at which the network in question (Swarms) has the characteristics which fulfill the requirements described in the Kademlia paper. "},{"title":"2. Area of responsibility related depths‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#2-area-of-responsibility-related-depths","content":"Area of responsibility refers to which chunks a node is responsible for storing. There are two concepts of depth related to a node‚Äôs area of responsibility - storage depth and reserve depth. Both reserve depth and storage depth are measures of PO which define the chunks a node is responsible for storing. "},{"title":"2a. Reserve Depth‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#2a-reserve-depth","content":"The PO which measures the node‚Äôs area of responsibility based on the theoretical 100% utilization of all postage stamp batches (all the chunks which are eligible to be uploaded and stored are uploaded and stored). Has an inverse relationship with area of responsibility - as depth grows, area of responsibility gets smaller. "},{"title":"2b. Storage Depth‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#2b-storage-depth","content":"The PO which measures the node‚Äôs effective area of responsibility. Storage depth will equal reserve depth in the case of 100% utilization - however 100% utilization is uncommon. If after syncing all the chunks within the node‚Äôs area of responsibility at its reserve depth then node still has sufficient space left, then the storage depth will decrease so that the area of responsibility doubles. "},{"title":"3. Postage stamp batch and chunk related depths‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#3-postage-stamp-batch-and-chunk-related-depths","content":""},{"title":"3a. Batch depth‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#3a-batch-depth","content":"Batch depth is the value d which is defined in relation to the size of a postage stamp batch. The size of a batch is defined as the number of chunks which can be stamped by that batch (also referred to as the number of slots per batch, with one chunk per slot). The size is calculated by: 2d2^{d}2dddd is a value selected by the batch issuer which determines how much data can be stamped with the batch "},{"title":"3b. Bucket depth‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#3b-bucket-depth","content":"Bucket depth is the constant value which defines how many buckets the address space for chunks is divided into for postage stamp batches. Bucket depth is set to 16, and the number of buckets is defined as 2bucketdepth2^{bucket depth}2bucketdepth "},{"title":"PLUR‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#plur","content":"PLUR (name inspired by the PLUR principles) is the smallest denomination of BZZ. 1 PLUR is equal to 1e-16 BZZ. "},{"title":"Bridged Tokens‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#bridged-tokens","content":"Bridged tokens are tokens from one blockchain which have been bridged to another chain through a smart contract powered bridge. For example, xDAI and xBZZ on Gnosis Chain are the bridged version of DAI and BZZ on Ethereum. "},{"title":"BZZ Token‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#bzz-token","content":"BZZ is Swarm's ERC-20 token issued on Ethereum. Blockchain\tContract addressEthereum, BZZ\t0x19062190b1925b5b6689d7073fdfc8c2976ef8cb Gnosis Chain, xBZZ\t0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da Goerli (testnet), gBZZ\t0x2ac3c1d3e24b45c6c310534bc2dd84b5ed576335 "},{"title":"xBZZ Token‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#xbzz-token","content":"xBZZ is BZZ bridged to the Gnosis Chain using OmniBridge. It is used as payment for postage stamps and as the unit of accounting between the nodes. It is used to incentivize nodes to provide resources to the Swarm. "},{"title":"DAI Token‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#dai-token","content":"DAI is an ERC-20 stable token issued on the Ethereum blockchain, tracking USD. "},{"title":"xDai Token‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#xdai-token","content":"xDai is DAI bridged to the Gnosis Chain using xDai Bridge. It is also the native token of the Gnosis Chain, i.e. transaction fees are paid in xDai. "},{"title":"Goerli‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#goerli","content":"Goerli is an Ethereum testnet. It is an environment where smart contracts can be developed and tested without spending cryptocurrency with real value, and without putting valuable assets at risk. Tokens on Goerli are often prefixed with a lower-case 'g', example: 'gBZZ' and 'gETH,' and because this is a test network carry no monetary value. It is an environment where Bee smart contracts can be tested and interacted with without any risk of monetary loss. "},{"title":"Faucet‚Äã","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#faucet","content":"A cryptocurrency faucet supplies small amounts of cryptocurrency to requestors (typically for testing purposes). It supplies small amounts of gBZZ and gETH for anyone who submits a request at the Swarm Discord server by using the /faucet command in the #develop-on-swarm channel. "},{"title":"Swarm FAQ","type":0,"sectionRef":"#","url":"/docs/learn/faq","content":"","keywords":""},{"title":"Community‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#community","content":""},{"title":"What are the Swarm Foundation's official channels?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-are-the-swarm-foundations-official-channels","content":"Website: https://ethswarm.org/Blog:https://blog.ethswarm.org/Github: https://github.com/etherspheree-mail: info@ethswarm.orgDiscord: https://discord.ethswarm.org/Twitter: https://twitter.com/ethswarmReddit: https://www.reddit.com/r/ethswarmYoutube: https://www.youtube.com/channel/UCu6ywn9MTqdREuE6xuRkskA "},{"title":"Where can I find technical support and get answers to my other questions?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#where-can-i-find-technical-support-and-get-answers-to-my-other-questions","content":"The Swarm community is centered around our Discord server where you will find many people willing and able to help with your every need! https://discord.ethswarm.org/ "},{"title":"Where can I find support for running Bee node on Dappnode?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#where-can-i-find-support-for-running-bee-node-on-dappnode","content":"You can find support for running Bee on Dappnode on the Dappnode Discord server: https://discord.gg/dRd5CrjF "},{"title":"Who can I contact for other inquiries?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#who-can-i-contact-for-other-inquiries","content":"For any other inquiries, you can contact us at info@ethswarm.org "},{"title":"What's the relationship between Swarm and Ethereum?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#whats-the-relationship-between-swarm-and-ethereum","content":"Swarm started in the first days of Ethereum as part of the original &quot;world computer&quot; vision, consisting of Ethereum (the processor), Whisper (messaging) and Swarm (storage). The project is the result of years of research and work by the Ethereum Foundation, the Swarm Foundation, teams, individuals across the ecosystem and the community. The conceptual idea for Swarm was started in the Ethereum team at the beginning, and the Ethereum Foundation incubated Swarm. After five years of research, Swarm and Ethereum are now two separate entities. "},{"title":"BZZ Token‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#bzz-token","content":""},{"title":"What is BZZ Token?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-bzz-token","content":"Swarm's native token BZZ, was initially issued on Ethereum. It has been bridged over to Gnosis where it is referred to as xBZZ for differentiation, and serves as a means of accessing the platform's data relay and storage services, while also providing compensation for node operators who provide these services. "},{"title":"What is PLUR?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-plur","content":"1 PLUR is the atomic unit of xBZZ, where xBZZ then has 16 decimals (ie. 1 PLUR = 1e-16 xBZZ) "},{"title":"Where can I buy BZZ Token?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#where-can-i-buy-bzz-token","content":"There are many ways to acquire BZZ token, either on custodial centralised exchanges where you can trade traditional currencies and cryptocurrency or through decentralised exchanges and protocols where you can trade between cryptocurrencies. For more information please visit the Get BZZ page on the Ethswarm.org homepage. "},{"title":"What is the BZZ token address?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-bzz-token-address","content":"See this page for a list of relevant token addresses. "},{"title":"What is the BZZ token supply?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-bzz-token-supply","content":"The BZZ token does not have a fixed supply, but instead, it can be minted by depositing collateral (DAI token) and burned which releases this collateral. The amount of collateral that needs to be deposited/withdrawn to mint/burn 1 BZZ token is defined by a token bonding curve in a smart contract. Current token supply: https://tokenservice.ethswarm.org/circulating_supply "},{"title":"BZZ token tokenomics‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#bzz-token-tokenomics","content":"More about BZZ token tokenomics: https://blog.ethswarm.org/hive/2021/bzz-tokenomics/ "},{"title":"What is the bonding curve?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-bonding-curve","content":"A bonding curve is a mathematical function in the form of y=f(x) that determines the price of a single token, depending on the number of tokens currently in existence, or the market supply. The key difference is that with a traditional exchange platform market makers are required to provide liquidity to the market, whereas a bonding curve takes over the role of providing liquidity, negating the need for market makers. "},{"title":"What is the \"Bzzaar\" bonding curve?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-bzzaar-bonding-curve","content":"Bzzaar is a unique exchange platform that enables users to buy and sell BZZ tokens. Unlike traditional exchange platforms, Bzzaar uses a bonding curve to instantly complete transactions without relying on market makers. The platform allows new projects to easily integrate Bzzaar's contracts into their own interfaces, creating a front-end interface for BZZ exchange. The bonding curve is community-owned and fuels all projects created on Swarm. Read more about the &quot;Bzzaar&quot; bonding curve: https://medium.com/ethereum-swarm/swarm-and-its-bzzaar-bonding-curve-ac2fa9889914 Bzzaar contractinformation: https://github.com/ethersphere/bzzaar-contracts Bzzaar's smart contract: https://etherscan.io/address/0x4f32ab778e85c4ad0cead54f8f82f5ee74d46904 Is Bzzaar's bonding curve audited? A full audit of Swarm was performed by QuantStamp and the final audit report has been made publicly available. "},{"title":"Running a Bee Node‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#running-a-bee-node","content":""},{"title":"How can I become part of the Swarm network?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#how-can-i-become-part-of-the-swarm-network","content":"You can become part of the network by running a bee node. Bee is a peer-to-peer client that connects you with other peers all over the world to become part of Swarm network, a global distributed p2p storage network that aims to store and distribute all of the world's data Depending on your needs you can run ultra-light, light or full node. "},{"title":"What are the differences between Bee node types?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-are-the-differences-between-bee-node-types","content":"A bee node can be configured to run in various modes based on specific use cases and requirements. See here for an overview of the differences. What are the requirements for running a Bee node?‚Äã See the install section for more information about running a Bee node. Full node‚Äã 20GB -30GB SSD (ideally nvme).8GB RAMCPU with 2+ coresRCP connection to Gnosis ChaiinMin 0.1 xDAI for Gnosis GAS fees1 xBZZ for initial chequebook deployment10 xBZZ for staking (optional) How much bandwidth is required for each node?‚Äã Typically, each node requires around 10 megabits per second (Mbps) of bandwidth during normal operation. How do I Install Bee on Windows?‚Äã You can install Bee node on Windows but it is not mentioned in the documentation, however, the steps are the same as the manual installation https://docs.ethswarm.org/docs/bee/installation/manual you can download the binary from here https://github.com/ethersphere/bee/releases and download one of the Windows releases. It is also possible to build from the source. How do I get the node's wallet's private key (use-case for Desktop app)?‚Äã See the backup section for more info. How do I import the swarm private key to metamask?‚Äã You can import the swarm.key json file in MetaMask using your password file or the password you have set in your bee config file. Where can I find my password?‚Äã You can find the password in the root of your data directory. See the backup section for more info. "},{"title":"Connectivity‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#connectivity","content":""},{"title":"Which p2p port does Bee use and which should I open in my router?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#which-p2p-port-does-bee-use-and-which-should-i-open-in-my-router","content":"The default p2p port for Bee is 1634, please forward this using your router and allow traffic over your firewall as necessary. Bee also supports UPnP but it is recommended you do not use this protocol as it lacks security. For more detailed information see the connectivity section in the docs. https://docs.ethswarm.org/docs/bee/installation/connectivity "},{"title":"How do I know if I am connected to other peers?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#how-do-i-know-if-i-am-connected-to-other-peers","content":"You may communicate with your Bee using its HTTP api. Type curl http://localhost:1635/peers at your command line to see a list of your peers. "},{"title":"What does \"Failed to connect to local host port 1635: Connection refused\" mean?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-does-failed-to-connect-to-local-host-port-1635-connection-refused-mean","content":"Your node is not listening on port 1635, either the debug-api is not enabled, or it is not listening on localhost. Make sure your bee.yaml file has debug-api-enable: true "},{"title":"Errors‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#errors","content":""},{"title":"What does \"could not connect to peer\" mean?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-does-could-not-connect-to-peer-mean","content":"‚ÄúCould connect to peer can happen for various reasons.‚Äù One of the most common is that you have the identifier of a peer in your address book from a previous session. When trying to connect to this node again, the peer may no longer be online. "},{"title":"What does \"context deadline exceeded\" error mean?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-does-context-deadline-exceeded-error-mean","content":"The &quot;context deadline exceeded&quot; is a non critical warning. It means that a node took unexpectedly long to respond to a request from your node. Your node will automatically try again via another node. "},{"title":"How do I set up a blockchain endpoint?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#how-do-i-set-up-a-blockchain-endpoint","content":"We recommend you run your own Gnosis Node using Nethermind. If you use &quot;bee start&quot; you can set it in your bee configuration under --blockchain-rpc-endpoint or BEE_BLOCKCHAIN_RPC_ENDPOINTopen ~/.bee.yamlset blockchain-rpc-endpoint: http://localhost:8545 If you use bee.service you can set it in your bee configuration under --blockchain-rpc-endpoint or BEE_BLOCKCHAIN_RPC_ENDPOINTopen /etc/bee/bee.yamland then uncomment blockchain-rpc-endpoint configurationand set it to http://localhost:8545after that sudo systemctl restart bee "},{"title":"How can I export my private keys?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#how-can-i-export-my-private-keys","content":"See the section on backups for exporting your keys. "},{"title":"How to import bee node address to MetaMask?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#how-to-import-bee-node-address-to-metamask","content":"See the backup section for info on exporting keys.Go to Metamask and click &quot;Account 1&quot; --&gt; &quot;Import Account&quot;Choose the &quot;Select Type&quot; dropdown menu and choose &quot;JSON file&quot;Paste the password (Make sure to do this first)Upload exported JSON file Click &quot;Import&quot; "},{"title":"What are the restart commands of bee?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-are-the-restart-commands-of-bee","content":"If you use bee.service: Start: sudo systemctl start bee.serviceStop: sudo systemctl stop bee.serviceStatus: sudo systemctl status bee.service If you use &quot;bee start&quot; Start: bee startStop: ctrl + c or cmd + c or close terminal to stop process "},{"title":"Relevant endpoints and explanations‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#relevant-endpoints-and-explanations","content":"See the API Reference pages for details. Most common use cases: curl http://localhost:1635/peers - Shows you the currently connected peerscurl http://localhost:1635/balances - Shows balances (positive=incoming, negative=outgoing) accumulating with peers, some of which may or may not be currently connectedcurl http://localhost:1635/settlements - When the balance with a given peer exceeds a threshold, a settlement will be issued, if the settlement is received, then your node should have a check from that peer.curl http://localhost:1635/chequebook/address your chequebook contract to see the xBZZ. "},{"title":"How can I check how many cashed out cheques do I have?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#how-can-i-check-how-many-cashed-out-cheques-do-i-have","content":"You can look at your chequebook contract at etherscan. Get your chequebook contract address with: curl http://localhost:1635/chequebook/address "},{"title":"I have compared transactions between my ethereum address and my chequebook address, the numbers are different, which is quite weird.‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#i-have-compared-transactions-between-my-ethereum-address-and-my-chequebook-address-the-numbers-are-different-which-is-quite-weird","content":"Your chequebook will show OUT xBZZ transactions when your peers cash cheques issued by you, but you don't pay any gas for those so they won't show up in your Ethereum address transaction list. "},{"title":"Where can I find documents about the cashout commands?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#where-can-i-find-documents-about-the-cashout-commands","content":"https://docs.ethswarm.org/docs/bee/working-with-bee/cashing-out "},{"title":"When I run http://localhost:1635/chequebook/balance I get \"totalBalance\" and \"availableBalance\" what is the difference?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#when-i-run-httplocalhost1635chequebookbalance-i-get-totalbalance-and-availablebalance-what-is-the-difference","content":"totalBalance is the balance on the blockchain, availableBalance is that balance minus the outstanding (non-cashed) cheques that you have issued to your peers. These latter cheques do not show up on the blockchain. It's like what the bank thinks your balance is vs what your chequebook knows is actually available because of the cheques you've written that are still &quot;in the mail&quot; and not yet cashed. "},{"title":"What determines the number of peers and how to influence their number? Why are there sometimes 300+ peers and sometimes 30?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-determines-the-number-of-peers-and-how-to-influence-their-number-why-are-there-sometimes-300-peers-and-sometimes-30","content":"The number of connected peers is determined by your node as it attempts to keep the distributed Kademlia well connected. As nodes come and go in the network your peer count will go up and down. If you watch bee's output logs for &quot;successfully connected&quot;, there should be a mix of (inbound) and (outbound) at the end of those messages. If you only get (outbound) then you may need to get your p2p port opened through your firewall and/or forwarded by your router. Check out the connectivity section in the docs https://docs.ethswarm.org/docs/bee/installation/connectivity. "},{"title":"What is the difference between \"systemctl\" and \"bee start\"?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-difference-between-systemctl-and-bee-start","content":"bee start and systemctl start bee actually run 2 different instances with 2 different bee.yaml files and two different data directories. bee start uses ~/.bee.yaml and the ~/.bee directory for datasystemctl uses /etc/bee/bee.yaml and (IIRC) /var/lib/bee for data "},{"title":"Swarm Protocol‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#swarm-protocol","content":""},{"title":"Can I use one Ethereum Address/Wallet for many nodes?‚Äã","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#can-i-use-one-ethereum-addresswallet-for-many-nodes","content":"No, this violates the requirements of the Swarm Protocol. The Swarm Protocol relies upon the Swarm Address, also known as the peer address. This address is a hash of the node's Ethereum address, therefore it is deterministic. As all nodes must have a unique address, if you were to use the same wallet, it would violate the uniqueness constraint and result in malfunctioning nodes. Therefore, the rule is, each node must have: 1 Ethereum Address1 Chequebook3 unique ports for API / p2p / Debug API. "},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/learn/introduction","content":"","keywords":""},{"title":"Welcome!‚Äã","type":1,"pageTitle":"Introduction","url":"/docs/learn/introduction#welcome","content":"Swarm is a peer-to-peer network of Bee nodes that collectively provide censorship resistant decentralised storage and communication services. Swarm's mission is to shape the future towards a self-sovereign global society and permissionless open markets by providing scalable base-layer data storage infrastructure for the decentralised internet. Its incentive system is enforced through smart contracts on the Gnosis Chain blockchain and powered by the xBZZ token, making it economically self-sustaining. "},{"title":"Chequebook","type":0,"sectionRef":"#","url":"/docs/learn/technology/contracts/chequebook","content":"Chequebook The chequebook contract is a smart contract used in the Swarm Accounting Protocol (SWAP) to manage cheques that are sent between nodes on the network. The contract is responsible for keeping track of the balances of each node and ensuring that cheques are valid and can be cashed out correctly. When a node sends a cheque to another node, it includes a signed message that specifies the amount of xBZZ tokens being transferred and the recipient's address. The chequebook contract receives this message and verifies that it is valid by checking the signature and ensuring that the sender has enough funds to cover the transfer. caution Settlement of cheques is not enforced by smart contract. If the cheque is valid, the contract updates the balances of both nodes accordingly. The recipient can then cash out their xBZZ tokens by sending a transaction to the blockchain that invokes a function in the chequebook contract. This function transfers the specified amount of BZZ tokens from the sender's account to the recipient's account. The chequebook contract also includes some additional features to prevent abuse. For example, it can impose limits on how much debt a node can accumulate before requiring payment. The chequebook contract plays an important role in ensuring that SWAP operates smoothly and fairly by providing a secure and reliable way for nodes to exchange value on the network.","keywords":""},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/learn/technology/contracts/overview","content":"","keywords":""},{"title":"Token Contracts‚Äã","type":1,"pageTitle":"Overview","url":"/docs/learn/technology/contracts/overview#token-contracts","content":"Contract\tBlockchain\tAddressBZZ token\tEthereum\t0x19062190b1925b5b6689d7073fdfc8c2976ef8cb xBZZ token\tGnosis Chain\t0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da gBZZ token\tGoerli (Ethereum testnet)\t0x2ac3c1d3e24b45c6c310534bc2dd84b5ed576335 BZZ Bonding Curve\tEthereum\t0x4F32Ab778e85C4aD0CEad54f8f82F5Ee74d46904 "},{"title":"Storage Incentives Contracts‚Äã","type":1,"pageTitle":"Overview","url":"/docs/learn/technology/contracts/overview#storage-incentives-contracts","content":"Storage Incentives Github Repo. Contract\tBlockchain\tAddressPostage Stamp\tGnosis Chain\t0x30d155478eF27Ab32A1D578BE7b84BC5988aF381 Staking\tGnosis Chain\t0x781c6D1f0eaE6F1Da1F604c6cDCcdB8B76428ba7 Redistribution\tGnosis Chain\t0x1F9a1FDe5c6350E949C5E4aa163B4c97011199B4 Price Oracle\tGnosis Chain\t0x344A2CC7304B32A87EfDC5407cD4bEC7cf98F035 "},{"title":"Price Oracle","type":0,"sectionRef":"#","url":"/docs/learn/technology/contracts/price-oracle","content":"Price Oracle The job of the Oracle contract is to set the price of Postage Stamps. The oracle contract uses data from the Postage Stamp contract in order to set the appropriate price for Postage Stamps. The data in the Postage Stamp contract is used to calculate a ‚Äúutilization signal‚Äù. This signal is an indicator of how much the Swarm network‚Äôs data storage capacity is being utilized. Specifically, the signal is a measure of data redundancy on the network. Redundancy is a measure of how many copies of each piece of data can be stored by the network. The protocol targets a fourfold level of data redundancy as a safe minimum. For example, if there is an increase in postage stamps being purchased while the number of nodes remains constant, the data redundancy level will begin to fall as data storers‚Äô available space begins to become reserved. If too many postage stamps are purchased without an equivalent increase in storage providers, the redundancy level may fall below four. In this case, the oracle will increase the price of postage stamps so that it becomes more expensive to store data on Swarm. The higher cost of storage will then lead to less postage stamps being purchased, and will push the redundancy level back up towards four. Conversely, if the amount of Stamps being purchased decreases while the number of storage provider nodes remains constant, the redundancy level will increase as there are fewer chunks of data to be distributed amongst the same number of nodes. In this case, the oracle will decrease the Postage Stamp price in order to promote more data storers to store their data on Swarm. The lower cost of storage will then lead to more Postage Stamps being purchased and push the redundancy level back down towards four.","keywords":""},{"title":"DISC","type":0,"sectionRef":"#","url":"/docs/learn/technology/disc","content":"","keywords":""},{"title":"Kademlia Topology and Routing‚Äã","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#kademlia-topology-and-routing","content":"Each node within Swarm connects to a certain number of its peers. When a chunk is first inserted into the network, the uploading node will send it to the peer which is closest (as measured by proximity order, which is based on a measure of Kademlia distance) to the destination of the chunk, and then the recipient node will then forward the chunk on to its peer which is closest to the destination of the chunk, and so on until the chunk arrives at its destination. One of the advantages of using Kademlia as a model for network topology is that both the number of forwarding &quot;hops&quot; required to route a chunk to its destination and the number of peer connections required to maintain Kademlia topology are logarithmic to the size of the network (a minimum of two connections is required in order to maintain Kademlia topology in case of network churn - nodes dropping in and out of the network). This makes Swarm a highly scalable system which is efficient even at very large scales. "},{"title":"Neighborhoods‚Äã","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#neighborhoods","content":"Neighborhoods are groups of nodes which are responsible for sharing the same chunks. The chunks which each neighborhood is responsible for storing are defined by the proximity order of the nodes and the chunks. In other words, each node is responsible for storing chunks with which their overlay addresses share a certain number of prefix bits, and together with other nodes which share the same prefix bits, make up neighborhoods which share the responsibility for storing the same chunks. "},{"title":"Chunks‚Äã","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#chunks","content":"In the DISC model, chunks are the canonical unit of data. When a file is uploaded to Swarm, it gets broken down into 4kb pieces with attached metadata. The pieces then get distributed amongst nodes in the Swarm network based on their overlay addresses. There are two fundamental chunk types: content-addressed chunks and single-owner chunks. "},{"title":"Content-Addressed Chunks and Single-Owner Chunks‚Äã","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#content-addressed-chunks-and-single-owner-chunks","content":"Content-addressed chunks are chunks whose address is based on the hash digest of their data. Using a hash as the chunk address makes it possible to verify the integrity of chunk data. Swarm uses the BMT hash function based on a binary Merkle tree over small segments of the chunk data. A content-addressed chunk has an at most 4KB payload, and its address is calculated as the hash of the span (chunk metadata) and the Binary Merkle Tree hash of the payload. For single-owner chunks on the other hand, the address is calculated as the hash of a unique id and the owner's overlay address. The content consists of an arbitrary data payload along with required headers. Unlike a content-addressed chunk, the contents of a single-owner chunk may be updated while the address remains unchanged. Single owner chunks form the basis for feeds, which are data structures that allow for mutable content with a static address. "},{"title":"Push-Sync, Pull-Sync, and Retrieval Protocols‚Äã","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#push-sync-pull-sync-and-retrieval-protocols","content":"When a file is first uploaded to Swarm, it gets broken down by the uploading Bee node chunks which are then distributed amongst other Bee nodes in the Swarm network. Chunks get distributed to the target neighborhood by the push-sync protocol. Once a chunk reaches its destination, it will then be duplicated and synced to other nodes in order to achieve data redundancy through the pull-sync protocol. The pull-sync protocol operates continuously as nodes enter or exit the network ‚Äì ensuring that data redundancy is always maintained. When a client node requests a file for download, its request gets forwarded by the retrieval-protocol to all the nodes storing the relevant chunks, and then those chunks get returned to the requesting node and the file gets reconstructed from its constituent chunks. "},{"title":"Postage Stamp","type":0,"sectionRef":"#","url":"/docs/learn/technology/contracts/postage-stamp","content":"","keywords":""},{"title":"Batch Buckets‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-buckets","content":"Postage stamps are issued in batches with a certain number of storage slots divided amongst 2bucketDepth2^{bucketDepth}2bucketDepth equally sized address space buckets. Each bucket is responsible for storing chunks that fall within a certain range of the address space. When uploaded, files are split into 4kb chunks, each chunk is assigned a unique address, and each chunk is then assigned to the bucket in which its address falls. While the value of bucket depth is not defined in The Book of Swarm, in its current implementation in the Bee client, bucket depth has been set to 16, so there are a total of 65,536 buckets. "},{"title":"Bucket Size‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#bucket-size","content":"Each bucket has a certain number of slots which can be &quot;filled&quot; by chunks (In other words, for each bucket, a certain number of chunks can be stamped). Once all the slots of a bucket are filled, the entire postage batch will be fully utilised and can no longer be used to upload additional data. Given the constant bucket depth of 16, for a batch depth of 20, the number of chunks per bucket is calculated like so: 2(20‚àí16)=16¬†chunks/bucket2^{(20 - 16)} = 16 \\text{ chunks/bucket}2(20‚àí16)=16¬†chunks/bucket Batch depth determines how many chunks are allowed in each bucket. The number of chunks allowed in each bucket is calculated like so: 2(batchDepth‚àíbucketDepth)2^{(batchDepth - bucketDepth)}2(batchDepth‚àíbucketDepth) "},{"title":"Batch Depth and Batch Amount‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-depth-and-batch-amount","content":"Each batch of stamps has two key parameters, batch depth and amount, which are recorded on Gnosis Chain at issuance. Note that these &quot;depths&quot; do not refer to the depth terms used to describe topology which are outlined here in the glossary. "},{"title":"Batch Depth‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-depth","content":"Batch depth determines how much data can be stored by a batch. The number of chunks which can be stored (stamped) by a batch is equal to 2depth2^{depth}2depth. For a batch with a batch depth of 23, a maximum of 223=33,554,4322^{23} = 33,554,432223=33,554,432 chunks can be stamped. Since we know that one chunk can store 4 kb of data, we can calculate the theoretical maximum amount of data which can be stored by a batch from the batch depth. Theoretical¬†maximum¬†batch¬†volume=2batchDepth√ó4¬†kb\\text{Theoretical maximum batch volume} = 2^{batchDepth} \\times \\text{4 kb}Theoretical¬†maximum¬†batch¬†volume=2batchDepth√ó4¬†kb However, due to the way postage stamp batches are utilised, batches will become fully utilised before stamping the theoretical maximum number of chunks. Therefore when deciding which batch depth to use, it is important to consider the effective amount of data that can be stored by a batch, and not the theoretical maximum. The effective rate of utilisation increases along with the batch depth. See section on stamp batch utilisation below for more information. "},{"title":"Batch Amount (& Batch Cost)‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-amount--batch-cost","content":"The amount parameter is the quantity of xBZZ in PLUR (1√ó1016PLUR=1¬†xBZZ)(1 \\times 10^{16}PLUR = 1 \\text{ xBZZ})(1√ó1016PLUR=1¬†xBZZ) that is assigned per chunk in the batch. The total number of xBZZ that will be paid for the batch is calculated from this figure and the batch depth like so: 2batchDepth√óamount2^{batchDepth} \\times {amount}2batchDepth√óamount The paid xBZZ forms the balance of the batch. This balance is then slowly depleted as time ticks on and blocks are mined on Gnosis Chain. For example, with a batch depth of 20 and an amount of 1000000000 PLUR: 220√ó1000000000=1048576000000000¬†PLUR=0.1048576¬†xBZZ2^{20} \\times 1000000000 = 1048576000000000 \\text{ PLUR} = 0.1048576 \\text{ xBZZ}220√ó1000000000=1048576000000000¬†PLUR=0.1048576¬†xBZZ Batch cost calculator:‚Äã Calculate Batch cost: "},{"title":"Batch Utilisation‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-utilisation","content":""},{"title":"Immutable Batches‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#immutable-batches","content":"Utilisation of an immutable batch is computed using a hash map of size 2bucketDepth2^{bucketDepth}2bucketDepth which is 2162^{16}216 for all batches, so 65536 total entries. For the keys of the key-value pairs of the hash map, the keys are 16 digit binary numbers from 0 to 65535, and the value is a counter.  As chunks are uploaded to Swarm, each chunk is assigned to a bucket based the first 16 binary digits of the chunk's hash. The chunk will be assigned to whichever bucket's key matches the first 16 bits of its hash, and that bucket's counter will be incremented by 1. The batch is deemed &quot;full&quot; when ANY of these counters reach a certain max value. The max value is computed from the batch depth as such: 2(batchDepth‚àíbucketDepth)2^{(batchDepth-bucketDepth)}2(batchDepth‚àíbucketDepth). For example if the batch depth is 21, then the max value is 2(21‚àí16)2^{(21-16)}2(21‚àí16) or 32. A bucket can be thought of as have a number of &quot;slots&quot; equal to this maximum value, and every time the bucket's counter is incremented, one of its slots gets filled. In the diagram below, the batch depth is 18, so there are 2(18‚àí16)2^{(18-16)}2(18‚àí16) or 4 slots for each bucket. The utilisation of a batch is simply the highest number of filled slots out of all 65536 entries or &quot;buckets&quot;. In this batch, none of the slots in any of the buckets have yet been filled with 4 chunks, so the batch is not yet fully utilised. The most filled slots out of all buckets is 2, so the stamp batch's utilisation is 2 out of 4.  As more chunks get uploaded and stamped, the bucket slots will begin to fill. As soon as the slots for any SINGLE bucket get filled, the entire batch is considered 100% utilised and can no longer be used to upload additional chunks.  "},{"title":"Mutable Batches‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#mutable-batches","content":"Mutable batches use the same hash map structure as immutable batches, however its utilisation works very differently. In contrast with immutable batches, mutable batches are never considered fully utilised. Rather, at the point where an immutable batch would be considered fully utilised, a mutable batch can continue to stamp chunks. However, if any chunk's address lands in a bucket whose slots are already filled, rather than the batch becoming fully utilised, that bucket's counter gets reset, and the new chunk will replace the oldest chunk in that bucket.  Therefore rather than speaking of the number of slots as determining the utilisation of a batch as with immutable batches, we can think of the slots as defining a limit to the amount of data which can be uploaded before old data starts to get overwritten. "},{"title":"Which Type of Batch to Use‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#which-type-of-batch-to-use","content":"Immutable batches are suitable for long term storage of data or for data which otherwise does not need to be changed and should never be overwritten, such as records archival, legal documents, family photos, etc. Mutable batches are great for data which needs to be frequently updated and does not require a guarantee of immutability. For example, a blog, personal or company websites, ephemeral messaging app, etc. The default batch type when unspecified is immutable. This can be modified through the Bee api by setting the immutable header with the \\stamps POST endpoint to false. "},{"title":"Implications for Swarm Users‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#implications-for-swarm-users","content":"Due to the nature of batch utilisation described above, batches are often fully utilised before reaching their theoretical maximum storage amount. However as the batch depth increases, the chance of a postage batch becoming fully utilised early decreases. At batch depth 23, there is a 0.1% chance that a batch will be fully utilised/start replacing old chunks before reaching 50% of its theoretical maximum. Let's look at an example to make it clearer. Using the method of calculating the theoretical maximum storage amount outlined above, we can see that for a batch depth of 23, the theoretical maximum amount which can be stored is 33,554,432 kb: 223√ó4kb=33,554,432¬†kb2^{23} \\times \\text{4kb} = \\text{33,554,432 kb}223√ó4kb=33,554,432¬†kb Therefore we should use 50% the effective rate of usage for the stamp batch: 33,554,432¬†kb√ó0.5=16,777,216¬†kb=16.77¬†gb\\text{33,554,432 kb} \\times{0.5} = \\text{16,777,216 kb} = \\text{16.77 gb}33,554,432¬†kb√ó0.5=16,777,216¬†kb=16.77¬†gb info The details of how the effective rates of utilisation are calculated will be published soon. "},{"title":"Effective Utilisation Table‚Äã","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#effective-utilisation-table","content":"info This table is based on preliminary calculations and may be subject to change. The provided table shows the effective volume for each batch depth from 20 to 41. The &quot;utilisation rate&quot; is the rate of utilisation a stamp batch can reach with a 0.1% failure rate (that is, there is a 1/1000 chance the batch will become fully utilised before reaching that utilisation rate). The &quot;effective volume&quot; figure shows the actual amount of data which can be stored at the effective rate. The effective volume figure is the one which should be used as the de-facto maximum amount of data that a batch can store before becoming either fully utilised (for immutable batches), or start overwriting older chunks (mutable batches). Batch Depth\tUtilisation Rate\tTheoretical Max Volume\tEffective Volume20\t0.00%\t4.29 GB\t0.00 B 21\t0.00%\t8.59 GB\t0.00 B 22\t28.67%\t17.18 GB\t4.93 GB 23\t49.56%\t34.36 GB\t17.03 GB 24\t64.33%\t68.72 GB\t44.21 GB 25\t74.78%\t137.44 GB\t102.78 GB 26\t82.17%\t274.88 GB\t225.86 GB 27\t87.39%\t549.76 GB\t480.43 GB 28\t91.08%\t1.10 TB\t1.00 TB 29\t93.69%\t2.20 TB\t2.06 TB 30\t95.54%\t4.40 TB\t4.20 TB 31\t96.85%\t8.80 TB\t8.52 TB 32\t97.77%\t17.59 TB\t17.20 TB 33\t98.42%\t35.18 TB\t34.63 TB 34\t98.89%\t70.37 TB\t69.58 TB 35\t99.21%\t140.74 TB\t139.63 TB 36\t99.44%\t281.47 TB\t279.91 TB 37\t99.61%\t562.95 TB\t560.73 TB 38\t99.72%\t1.13 PB\t1.12 PB 39\t99.80%\t2.25 PB\t2.25 PB 40\t99.86%\t4.50 PB\t4.50 PB 41\t99.90%\t9.01 PB\t9.00 PB "},{"title":"Incentives","type":0,"sectionRef":"#","url":"/docs/learn/technology/incentives","content":"","keywords":""},{"title":"Storage Incentives‚Äã","type":1,"pageTitle":"Incentives","url":"/docs/learn/technology/incentives#storage-incentives","content":"Swarm's storage incentives protocol is defined in depth in the Future Proof Storage paper published by the Swarm team. Swarm's storage incentives are based on postage stamps, which serve as verifiable proof of payment associated with chunks witnessed by their owner's signature. Postage stamps signal chunks' relative importance by ascribing them with xBZZ quantity which storer nodes can use when selecting which chunks to retain and which to evict from their reserve when their reserve capacity is exceeded. The amount of xBZZ required for a postage stamp depends on the amount of data being stored and the duration for which it will be stored. The longer a chunk is stored, the more xBZZ is required for the postage stamp. This ensures that users are incentivised to store data for longer periods, which helps ensure that data remains available in the network. Storer nodes can use the xBZZ associated with postage stamps when selecting which chunks to retain and serve or garbage collect during capacity shortage. This means that popular content will be widely distributed across the network, reducing retrieval latency. "},{"title":"Storage Incentives Details‚Äã","type":1,"pageTitle":"Incentives","url":"/docs/learn/technology/incentives#storage-incentives-details","content":"When someone wants to upload data to Swarm, they do so by buying postage stamp batches with xBZZ. The xBZZ is collected and later redistributed to storage provider nodes to pay for their services. Which node earns the reward is determined by playing a &quot;game&quot;. Every 152 Gnosis Chain blocks the game is played, and one node will win the accumulated xBZZ. The game has 3 phases, commit, reveal, and claim. In the reveal phase of a previous game, an &quot;anchor&quot; overlay address is randomly generated and used to determine the neighborhood for the current round. Only nodes within that neighborhood (meaning they have a certain number of shared leading bits with the neighborhood address) may participate and have a chance to win. In the commit phase, nodes issue an on-chain transaction including an encrypted hash of the data they are storing (the unencrypted hash is known as the &quot;reserve commitment&quot;) along with the depth for which they are reporting. This serves as an attestation of the data they are storing without revealing any other information. In the reveal phase, each node reveals the decryption key for their encrypted hashes thereby publishing the hash. One of the nodes is chosen as the honest node, and from among the honest nodes, one node is chosen as the winner. The winner is chosen at random among the honest nodes, but it is weighted in proportion to each node's stake density. Stake density is calculated as so: stake¬†density=stake(xBZZ)√ó2storage¬†depth\\text{stake density} = \\text{stake(xBZZ)} \\times {2}^\\text{storage depth}stake¬†density=stake(xBZZ)√ó2storage¬†depth "},{"title":"Penalties‚Äã","type":1,"pageTitle":"Incentives","url":"/docs/learn/technology/incentives#penalties","content":"During the reveal phase if a nodes' revealed hash does not match the honest nodes' hash, that node will be temporarily frozen and will not be able to participate in a number of upcoming rounds. "},{"title":"Bandwidth Incentives (SWAP)‚Äã","type":1,"pageTitle":"Incentives","url":"/docs/learn/technology/incentives#bandwidth-incentives-swap","content":"The Swarm Accounting Protocol (SWAP) is a protocol used in the Swarm network to manage the exchange of resources between nodes. SWAP ensures that node operators collaborate in routing messages and data while protecting the network against frivolous use of bandwidth. SWAP works by tracking the relative consumption of bandwidth between nodes. As nodes relay requests and responses, they keep track of their bandwidth usage with each of their peers. Within bounds, peers engage in a service-for-service exchange, where they provide resources to each other based on their relative usage. However, once a limit is reached, the party in debt can either wait until their liabilities are amortized over time or can pay by sending cheques that cash out in xBZZ on the blockchain. Chequebook contracts are used to manage these cheques and ensure that they are valid and can be cashed out correctly. SWAP uses built-in incentives to optimize the allocation of bandwidth and storage resources and render Swarm economically self-sustaining. Swarm nodes track their relative bandwidth contribution on each peer connection, and excess debt due to unequal consumption can be settled in xBZZ. Publishers in Swarm must spend xBZZ to purchase the right to write data to Swarm and prepay some rent for long-term storage. The SWAP protocol also includes some additional features to prevent abuse or fraud. For example, it can impose limits on how much debt a node can accumulate before requiring payment or require nodes to provide collateral before sending cheques. "},{"title":"What is Swarm?","type":0,"sectionRef":"#","url":"/docs/learn/technology/what-is-swarm","content":"","keywords":""},{"title":"1. Underlay Network‚Äã","type":1,"pageTitle":"What is Swarm?","url":"/docs/learn/technology/what-is-swarm#1-underlay-network","content":"The first part of Swarm is a peer-to-peer network protocol that serves as the underlay transport. The underlay transport layer is responsible for establishing connections between nodes in the network and routing data between them. It provides a low-level communication channel that enables nodes to communicate with each other directly, without relying on any centralised infrastructure. Swarm is designed to be agnostic of the particular underlay transport used, as long as it satisfies certain requirements. Addressing ‚Äì Nodes are identified by their underlay address.Dialling ‚Äì Nodes can initiate a direct connection to a peer by dialing them on their underlay address.Listening ‚Äì Nodes can listen to other peers dialing them and can accept incoming connections. Nodes that do not accept incoming connections are called light nodes.Live connection ‚Äì A node connection establishes a channel of communication which is kept alive until explicit disconnection, so that the existence of a connection means the remote peer is online and accepting messages.Channel security ‚Äì The channel provides identity verification and implements encrypted and authenticated transport resisting man in the middle attacks.Protocol multiplexing ‚Äì The underlay network service can accommodate several protocols running on the same connection. Delivery guarantees ‚Äì Protocol messages have guaranteed delivery, i.e. delivery failures due to network problems result in direct error response. Order of delivery of messages within each protocol is guaranteed. Serialisation ‚Äì The protocol message construction supports arbitrary data structure serialisation conventions. As the libp2p library meets all these requirements it has been used to build the Swarm underlay network. "},{"title":"2. Overlay Network‚Äã","type":1,"pageTitle":"What is Swarm?","url":"/docs/learn/technology/what-is-swarm#2-overlay-network","content":"The second part of Swarm is an overlay network with protocols powering the Distributed Immutable Store of Chunks (DISC). This layer is responsible for storing and retrieving data in a decentralised and secure manner. Swarm's overlay network is built on top of the underlay transport layer and uses Kademlia overlay routing to enable efficient and scalable communication between nodes. Kademlia is a distributed hash table (DHT) algorithm that allows nodes to locate each other in the network based on their unique identifier or hash. Swarm's DISC is an implementation a Kademlia DHT optimized for storage. While the use of DHTs in distributed data storage protocols is common, for many implementations DHTs are used only for indexing of specific file locations. Swarm's DISC distinguishes itself from other implementations by instead breaking files into chunks and storing the chunks themselves directly within a Kademlia DHT. Each chunk has a fixed size of 4kb and is distributed across the network using the DISC model. Each chunk has a unique address taken from the same namespace as the network node addresses that allows it to be located and retrieved by other nodes in the network. Swarm's distributed immutable storage provides several benefits, including data redundancy, tamper-proofing, and fault tolerance. Because data is stored across multiple nodes in the network, it can be retrieved even if some nodes fail or go offline. "},{"title":"3. Data Access Layer‚Äã","type":1,"pageTitle":"What is Swarm?","url":"/docs/learn/technology/what-is-swarm#3-data-access-layer","content":"The third part of Swarm is a component that provides high-level data access and defines APIs for base-layer features. This layer is responsible for providing an easy-to-use interface for developers to interact with Swarm's underlying storage and communication infrastructure. Swarm's high-level data access component provides APIs that allow developers to perform various operations on the network, including uploading and downloading data and searching for content. These APIs are designed to be simple and intuitive, making it easy for developers to build decentralised applications on top of Swarm. "},{"title":"4. Application Layer‚Äã","type":1,"pageTitle":"What is Swarm?","url":"/docs/learn/technology/what-is-swarm#4-application-layer","content":"The fourth part of Swarm is an application layer that defines standards and outlines best practices for more elaborate use cases. This layer is responsible for providing guidance to developers on how to build complex applications on top of Swarm's underlying infrastructure. "},{"title":"PSS","type":0,"sectionRef":"#","url":"/docs/learn/technology/pss","content":"PSS PSS, or Postal Service over Swarm, is a messaging protocol that enables users to send and receive messages over Swarm. It is an essential component of Swarm's infrastructure, providing secure, private, and efficient communication between nodes. PSS is designed to be secure by encrypting messages for the intended recipient and wrapping them with a topic in a content-addressed chunk. The chunk is crafted in such a way that its content address falls into the recipient's neighborhood, ensuring that delivery is naturally taken care of by the push-sync protocol. This ensures that messages are delivered only to the intended recipient's neighborhood and cannot be intercepted or read by unauthorized parties. While the chunk will be delivered to all members of the recipient's neighborhood, only the recipient will be able to decrypt the message using their private key. PSS also provides privacy by allowing users to receive messages from previously unknown identities. This makes it an ideal communication primitive for sending anonymous messages to public identities such as registrations or initial contact to start a thread by setting up secure communication. Efficiency is another key feature of PSS. It uses direct node-to-node messaging in Swarm, which means that messages are delivered directly from one node to another without the need for intermediaries. This reduces latency and ensures that messages are delivered quickly and reliably. PSS also supports mailboxing, which allows users to deposit messages for download if the recipient is not online. This ensures that messages are not lost if the recipient is offline when they are sent.","keywords":""},{"title":"Tokens","type":0,"sectionRef":"#","url":"/docs/learn/tokens","content":"","keywords":""},{"title":"Swarm Ecosystem Tokens‚Äã","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#swarm-ecosystem-tokens","content":""},{"title":"BZZ‚Äã","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#bzz","content":"BZZ is the original token issued from the Ethswarm Bonding Curve contract on the Ethereum blockchain. BZZ Ethereum address: 0x19062190b1925b5b6689d7073fdfc8c2976ef8cb PLUR is the smallest denomination of BZZ. 1 PLUR is equal to 1e-16 BZZ. "},{"title":"xBZZ‚Äã","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#xbzz","content":"&quot;xBZZ&quot; is the term used to indicate BZZ on Gnosis Chain. It is the bridged version of the original Ethereum BZZ token issued on Gnosis Chain. xBZZ is the token used for staking and to pay for storage fees on Swarm. xBZZ Gnosis Chain address: 0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da info Note that the ticker symbol is the same BZZ for both Gnosis Chain and Ethereum versions of the token. xBZZ is term of convenience used to differentiate the tokens within the Swarm community. As with BZZ, PLUR is the smallest denomination of xBZZ. 1 PLUR is equal to 1e-16 xBZZ. "},{"title":"gBZZ‚Äã","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#gbzz","content":"gBZZ is the testnet version of BZZ on the Goerli Ethereum testnet. Goerli testnet address: 0x2ac3c1d3e24b45c6c310534bc2dd84b5ed576335 "},{"title":"DAI‚Äã","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#dai","content":"DAI is the popular decentralized stablecoin from MakerDAO. "},{"title":"xDAI‚Äã","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#xdai","content":"xDAI is the bridged version of DAI on Gnosis Chain and also serves as the native gas token for Gnosis Chain and is used to pay transaction fees on Gnosis Chain in the same way ETH is used to pay for transactions on Ethereum. It is required by Bee nodes to pay for transaction fees when interacting with Swarm smart contracts on Gnosis Chain. "},{"title":"Getting BZZ / xBZZ‚Äã","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#getting-bzz--xbzz","content":"The Swarm official website has a page with a list of resources for getting BZZ tokens. Be careful to check whether it is BZZ on Ethereum or Gnosis Chain. "},{"title":"Bridging BZZ or DAI‚Äã","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#bridging-bzz-or-dai","content":"If you already have DAI or BZZ on Ethereum then you can use one of the Gnosis Chain bridges to swap for the bridged version of each token on Gnosis Chain (or vice versa). You can use either xDAI Bridge or OmniBridge for swapping between DAI and xDAI or BZZ and xBZZ. "}]