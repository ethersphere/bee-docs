{"searchDocs":[{"title":"Build from Source","type":0,"sectionRef":"#","url":"/docs/bee/installation/build-from-source","content":"","keywords":"","version":"Next"},{"title":"Build from Source​","type":1,"pageTitle":"Build from Source","url":"/docs/bee/installation/build-from-source#build-from-source","content":" Clone the repository: git clone https://github.com/ethersphere/bee cd bee Use git to find the latest release: git describe --tags Checkout the required version: git checkout v2.1.0 Build the binary: make binary Check you are able to run the bee command. Success can be verified by running: dist/bee version 2.1.0 (optional) Additionally, you may also like to move the Bee binary to somewhere in your $PATH sudo cp dist/bee /usr/local/bin/bee  ","version":"Next","tagName":"h3"},{"title":"Bee FAQ","type":0,"sectionRef":"#","url":"/docs/bee/bee-faq","content":"","keywords":"","version":"Next"},{"title":"Running a Bee Node​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#running-a-bee-node","content":" ","version":"Next","tagName":"h2"},{"title":"How can I become part of the Swarm network?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#how-can-i-become-part-of-the-swarm-network","content":" You can become part of the network by running a bee node. Bee is a peer-to-peer client that connects you with other peers all over the world to become part of Swarm network, a global distributed p2p storage network that aims to store and distribute all of the world's data  Depending on your needs you can run ultra-light, light or full node.  ","version":"Next","tagName":"h3"},{"title":"What are the differences between Bee node types?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#what-are-the-differences-between-bee-node-types","content":" A bee node can be configured to run in various modes based on specific use cases and requirements. See here for an overview of the differences.  What are the requirements for running a Bee node?​  See the install section for more information about running a Bee node.  Full node​  20GB -30GB SSD (ideally nvme).8GB RAMCPU with 2+ coresRCP connection to Gnosis ChaiinMin 0.1 xDAI for Gnosis GAS fees1 xBZZ for initial chequebook deployment10 xBZZ for staking (optional)  How much bandwidth is required for each node?​  Typically, each node requires around 10 megabits per second (Mbps) of bandwidth during normal operation.  How do I Install Bee on Windows?​  You can install Bee node on Windows but it is not mentioned in the documentation, however, the steps are the same as the manual installation https://docs.ethswarm.org/docs/bee/installation/manual you can download the binary from here  https://github.com/ethersphere/bee/releases and download one of the Windows releases.  It is also possible to build from the source.  How do I get the node's wallet's private key (use-case for Desktop app)?​  See the backup section for more info.  How do I import the swarm private key to metamask?​  You can import the swarm.key json file in MetaMask using your password file or the password you have set in your bee config file.  Where can I find my password?​  You can find the password in the root of your data directory. See the backup section for more info.  ","version":"Next","tagName":"h3"},{"title":"Connectivity​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#connectivity","content":" ","version":"Next","tagName":"h2"},{"title":"Which p2p port does Bee use and which should I open in my router?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#which-p2p-port-does-bee-use-and-which-should-i-open-in-my-router","content":" The default p2p port for Bee is 1634, please forward this using your router and allow traffic over your firewall as necessary. Bee also supports UPnP but it is recommended you do not use this protocol as it lacks security. For more detailed information see the connectivity section in the docs. https://docs.ethswarm.org/docs/bee/installation/connectivity  ","version":"Next","tagName":"h3"},{"title":"How do I know if I am connected to other peers?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#how-do-i-know-if-i-am-connected-to-other-peers","content":" You may communicate with your Bee using its HTTP api. Type curl http://localhost:1633/peers at your command line to see a list of your peers.  ","version":"Next","tagName":"h3"},{"title":"Errors​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#errors","content":" ","version":"Next","tagName":"h2"},{"title":"What does \"could not connect to peer\" mean?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#what-does-could-not-connect-to-peer-mean","content":" “Could connect to peer can happen for various reasons.” One of the most common is that you have the identifier of a peer in your address book from a previous session. When trying to connect to this node again, the peer may no longer be online.  ","version":"Next","tagName":"h3"},{"title":"What does \"context deadline exceeded\" error mean?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#what-does-context-deadline-exceeded-error-mean","content":" The &quot;context deadline exceeded&quot; is a non critical warning. It means that a node took unexpectedly long to respond to a request from your node. Your node will automatically try again via another node.  ","version":"Next","tagName":"h3"},{"title":"How do I set up a blockchain endpoint?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#how-do-i-set-up-a-blockchain-endpoint","content":" We recommend you run your own Gnosis Node using Nethermind.  If you use &quot;bee start&quot; you can set it in your bee configuration under --blockchain-rpc-endpoint or BEE_BLOCKCHAIN_RPC_ENDPOINTopen ~/.bee.yamlset blockchain-rpc-endpoint: http://localhost:8545 If you use bee.service you can set it in your bee configuration under --blockchain-rpc-endpoint or BEE_BLOCKCHAIN_RPC_ENDPOINTopen /etc/bee/bee.yamland then uncomment blockchain-rpc-endpoint configurationand set it to http://localhost:8545after that sudo systemctl restart bee  ","version":"Next","tagName":"h3"},{"title":"How can I export my private keys?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#how-can-i-export-my-private-keys","content":" See the section on backups for exporting your keys.  ","version":"Next","tagName":"h3"},{"title":"How to import bee node address to MetaMask?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#how-to-import-bee-node-address-to-metamask","content":" See the backup section for info on exporting keys.Go to Metamask and click &quot;Account 1&quot; --&gt; &quot;Import Account&quot;Choose the &quot;Select Type&quot; dropdown menu and choose &quot;JSON file&quot;Paste the password (Make sure to do this first)Upload exported JSON fileClick &quot;Import&quot;  ","version":"Next","tagName":"h3"},{"title":"What are the restart commands of bee?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#what-are-the-restart-commands-of-bee","content":" If you use bee.service:  Start: sudo systemctl start bee.serviceStop: sudo systemctl stop bee.serviceStatus: sudo systemctl status bee.service  If you use &quot;bee start&quot;  Start: bee startStop: ctrl + c or cmd + c or close terminal to stop process  ","version":"Next","tagName":"h3"},{"title":"Relevant endpoints and explanations​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#relevant-endpoints-and-explanations","content":" See the API Reference pages for details.  Most common use cases:  curl http://localhost:1633/peers - Shows you the currently connected peerscurl http://localhost:1633/balances - Shows balances (positive=incoming, negative=outgoing) accumulating with peers, some of which may or may not be currently connectedcurl http://localhost:1633/settlements - When the balance with a given peer exceeds a threshold, a settlement will be issued, if the settlement is received, then your node should have a check from that peer.curl http://localhost:1633/chequebook/address your chequebook contract to see the xBZZ.  ","version":"Next","tagName":"h3"},{"title":"How can I check how many cashed out cheques do I have?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#how-can-i-check-how-many-cashed-out-cheques-do-i-have","content":" You can look at your chequebook contract at etherscan. Get your chequebook contract address with: curl http://localhost:1633/chequebook/address  ","version":"Next","tagName":"h3"},{"title":"I have compared transactions between my ethereum address and my chequebook address, the numbers are different, which is quite weird.​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#i-have-compared-transactions-between-my-ethereum-address-and-my-chequebook-address-the-numbers-are-different-which-is-quite-weird","content":" Your chequebook will show OUT xBZZ transactions when your peers cash cheques issued by you, but you don't pay any gas for those so they won't show up in your Ethereum address transaction list.  ","version":"Next","tagName":"h3"},{"title":"Where can I find documents about the cashout commands?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#where-can-i-find-documents-about-the-cashout-commands","content":" https://docs.ethswarm.org/docs/bee/working-with-bee/cashing-out  ","version":"Next","tagName":"h3"},{"title":"When I run http://localhost:1633/chequebook/balance I get \"totalBalance\" and \"availableBalance\" what is the difference?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#when-i-run-httplocalhost1633chequebookbalance-i-get-totalbalance-and-availablebalance-what-is-the-difference","content":" totalBalance is the balance on the blockchain, availableBalance is that balance minus the outstanding (non-cashed) cheques that you have issued to your peers. These latter cheques do not show up on the blockchain.  It's like what the bank thinks your balance is vs what your chequebook knows is actually available because of the cheques you've written that are still &quot;in the mail&quot; and not yet cashed.  ","version":"Next","tagName":"h3"},{"title":"What determines the number of peers and how to influence their number? Why are there sometimes 300+ peers and sometimes 30?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#what-determines-the-number-of-peers-and-how-to-influence-their-number-why-are-there-sometimes-300-peers-and-sometimes-30","content":" The number of connected peers is determined by your node as it attempts to keep the distributed Kademlia well connected. As nodes come and go in the network your peer count will go up and down. If you watch bee's output logs for &quot;successfully connected&quot;, there should be a mix of (inbound) and (outbound) at the end of those messages. If you only get (outbound) then you may need to get your p2p port opened through your firewall and/or forwarded by your router. Check out the connectivity section in the docs https://docs.ethswarm.org/docs/bee/installation/connectivity.  ","version":"Next","tagName":"h3"},{"title":"What is the difference between \"systemctl\" and \"bee start\"?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#what-is-the-difference-between-systemctl-and-bee-start","content":" bee start and systemctl start bee actually run 2 different instances with 2 different bee.yaml files and two different data directories.  bee start uses ~/.bee.yaml and the ~/.bee directory for datasystemctl uses /etc/bee/bee.yaml and (IIRC) /var/lib/bee for data  ","version":"Next","tagName":"h3"},{"title":"Swarm Protocol​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#swarm-protocol","content":" ","version":"Next","tagName":"h2"},{"title":"Can I use one Ethereum Address/Wallet for many nodes?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#can-i-use-one-ethereum-addresswallet-for-many-nodes","content":" No, this violates the requirements of the Swarm Protocol and will break critical node functions such as staking, purchasing stamp batches, and uploading data.  Therefore, the rule is, each node must have:  1 Ethereum address (this address, the Swarm network id, and a random nonce are used to determine the node's overlay address)1 Chequebook2 Unique ports for Bee API / p2p API  ","version":"Next","tagName":"h3"},{"title":"Miscellaneous​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#miscellaneous","content":" ","version":"Next","tagName":"h2"},{"title":"How can I add Gnosis / Sepolia to Metamask?​","type":1,"pageTitle":"Bee FAQ","url":"/docs/bee/bee-faq#how-can-i-add-gnosis--sepolia-to-metamask","content":" You can easily add Sepolia or Gnosis to metamask using the official guide from Metamask.  If you are using a different wallet which does not have an easy option for adding networks like Metamask does, then you may need to add the networks manually. You need to fill in four pieces of information to do so:  Gnosis Chain​  Network name: Gnosis RPC URL: https://rpc.gnosischain.comChain ID: 100 Currency symbol: XDAI  Sepolia​  Network name: Sepolia test network RPC URL: https://sepolia.infura.io/v3/Chain ID: 11155111 Currency symbol: SepoliaETH ","version":"Next","tagName":"h3"},{"title":"Connectivity","type":0,"sectionRef":"#","url":"/docs/bee/installation/connectivity","content":"","keywords":"","version":"Next"},{"title":"Networking Basics​","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#networking-basics","content":" In a network, each computer is assigned an IP address. Each IP address is then subdivided into thousands of sockets or ports, each of which has an incoming and outgoing component.  In a completely trusted network of computers, any connections to or from any of these ports are allowed. However, to protect ourselves from nefarious actors when we join the wider Internet, it is sometimes important to filter this traffic so that some of these ports are off limits to the public.  In order to allow messages to our p2p port from other Bee nodes that we have previously not connected, we must ensure that our network is set up to receive incoming connections (on port 1634 by default).  danger There are also some ports which you should never expose to the outside Internet. Make sure that your api-addr (default 1633) is never exposed to the internet. It is good practice to employ one or more firewalls that block traffic on every port except for those you are expecting to be open. If you do not use a firewall, make sure to change the default api-addr from 1633 to 127.0.0.1:1633 so that it is not publicly exposed.  ","version":"Next","tagName":"h3"},{"title":"Your IP Address​","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#your-ip-address","content":" When you connect to the Internet, you are assigned a unique number called an IP Address. IP stands for Internet Protocol. The most prevalent IP version used is still the archaicIPv4 which was invented way back in 1981. IPv6 is available but not well used. Due to the mitigation of the deficiencies inherent in the IPv4 standard, we may encounter some complications.  ","version":"Next","tagName":"h2"},{"title":"Datacenters and Computers Connected Directly to the Internet​","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#datacenters-and-computers-connected-directly-to-the-internet","content":" If you are renting space in a datacenter, the chances are that your computer will be connected directly to the real Internet. This means that the IP of your networking interface will be directly set to be the same as your public IP.  You can investigate this by running:  ifconfig   or  ip address   Your output should contain something like:  eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 178.128.196.191 netmask 255.255.240.0 broadcast 178.128.207.255   Here we can see our computer's public IP address178.128.196.191. This is the address that is used by other computers we connect to over the Internet. We can verify this using a third party service such as icanhazip or ifconfig.  curl icanhazip.com --ipv4   or  curl ifconfig.co --ipv4   The response something contain something like:  178.128.196.191   With Bee running, try to connect to your Bee's p2p port using the public IP adddress from another computer:  nc -zv 178.128.196.191 1634   If you have success, congratulations!  If this still doesn't work for you, see the last part of Manual: Configure Your Router and Bee section below, as you may need to configure your nat-addr.  ","version":"Next","tagName":"h3"},{"title":"Home, Commercial and Business Networks and Other Networks Behind NAT​","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#home-commercial-and-business-networks-and-other-networks-behind-nat","content":" To address thescarcity of IP numbers, Network Address Translation (NAT) was implemented. This approach creates a smaller, private network which many devices connect to in order to share a public IP address. Traffic destined for the Internet at large is then mediated by another specialised computer. In the cases of the a home network, this computer is the familiar home router, normally also used to provide a WiFi network.  If we run the above commands to find the computer's IP in this scenario, we will see a different output.  ip address   en0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ... inet 192.168.0.10 netmask 0xffffff00 broadcast 192.168.0.255 ...   Here we can see that, instead of the public IP address, we can see that our computer's IP address is 192.168.0.10. This is part of the IP address space that the Internet Engineering Task Force has designated forprivate networks.  As this IP won't work on the global Internet, our router remembers that our computer has been assigned this IP. It then uses Network Address Translation (NAT) to modify all requests from our computer to another computer somewhere in the Internet. As the requests pass through the router it changes our local IP to the public IP of the router, and vice versa when the responses are sent back, from the public IP to the local one.  Navigating Through the NAT​  The presence of NAT presents two problems for p2p networking.  The first is that it can be difficult for programs running on our computer to know our real public IP as it is not explicitly known by our computer's networking interface, which is configured with a private network IP. This is a relatively easy problem to solve as we can simply discover our public IP and then specify it in Bee's configuration, or indeed determine it using other means.  The second issue is that our router has only 65535 ports to expose to the public network, however, each device on your private network is capable of exposing 65535 each. To the global Internet, it appears that there is only one set of ports to connect to, whereas, in actual fact, there is a full set of ports for each of the devices which are connected to the private network. To solve this second problem, routers commonly employ an approach known as port forwarding.  Bee's solution to these problems come in two flavours, automatic and manual.  Automatic: Universal Plug and Play (UPnP)​  UPnP is a protocol designed to simplify the administration of NAT and port forwarding for the end user by providing an API from which software running within the network can use to ask the router for the public IP and to request for ports to be forwarded to the private IP of the computer running the software.  UPnP is a security risk! UPnP is a security risk as it allows any host or process inside (sometimes also outside) your network to open arbitrary ports which may be used to transfer malicious traffic, for example aRAT. UPnP can also be used to determine your IP, and in the case of using ToR or a VPN, your real public IP. We urge you to disable UPnP on your router and use manual port forwarding as described below.  Bee will use UPnP to determine your public IP, which is required for various internal processes.  In addition to this, a request will be sent to your router to ask it to forward a random one of its ports, which are exposed directly to the Internet, to the Bee p2p port (default 1634) which your computer is exposing only to the private network. Doing this creates a tunnel through which other Bee's may connect to your computer safely.  If you start your Bee node in a private network with UPnP available, the output of the addresses endpoint of your API will look something like this:  [ &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP&quot;, &quot;/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP&quot;, &quot;/ip6/::1/tcp/1634/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP&quot;, &quot;/ip4/86.98.94.9/tcp/20529/p2p/16Uiu2HAm5zcoBFWmqjDTwGy9RXepBFF8idy6Pr312obMwwxdJSUP&quot; ]   Note that the port in the externalmultiaddress is the router's randomly selected 20529 which is forwarded by the router to192.168.0.10:1634. These addresses in this multiaddress are also known as the underlay addresses.  Manual: Configure Your Router and Bee​  Inspecting the underlay addresses in the output of the addresses endpoint of our API, we can see addresses only for localhost127.0.0.1 and our private network IP 192.168.0.10. Bee must be having trouble navigating our NAT.  [ &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip6/::1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot; ]   To help fix the first problem, let's determine our public IP address.  curl icanhazip.com   86.98.94.9   Now we can simply supply this IP in our Bee configuration on startup.  Solving our second problem is a little more difficult as we will need to interact with our router's firmware, which is a little cranky.  Each router is different, but the concept is usually the same. Log in to your router by navigating your browser to your router's configuration user interface, usually at http://192.168.0.1. You will need to log in with a password. Sadly, passwords are often left to be the defaults, which can be found readily on the Internet.  Once logged in, find the interface to set up port forwarding. The Port Forward website provides some good information, or you may refer to your router manual or provider.  Here, we will then set up a rule that forwards port 1634 of our private IP address 192.168.0.10 to the same port 1634 of our public IP.  Now, when requests arrive at our public address 86.98.94.9:1634 they are modified by our router and forwarded to our private IP and port192.168.0.10:1634.  Sometimes this can be a little tricky, so let's verify we are able to make a TCP connection using netcat.  First, with Bee not running, let's set up a simple TCP listener using Netcat on the same machine we would like to run Bee on.  nc -l 0.0.0.0 1634   nc -zv 86.98.94.9 1634   Connection to 86.98.94.9 port 1834 [tcp/*] succeeded!   Success! ✨  If this didn't work for you, check out our Debugging Connectivity guide below.  If it did, let's start our Bee node with the --nat-addr configured.  bee start --nat-addr 86.98.94.9:1634   Checking our addresses endpoint again, we can now see that Bee has been able to successfully assign a public address! Congratulations, your Bee is now connected to the outside world!  [ &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip6/::1/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot;, &quot;/ip4/86.98.94.9/tcp/1634/p2p/16Uiu2HAm8Hs91MzWuXfUyKrYaj3h8K8gzvRqzSK5gP9TNCwypkJB&quot; ]   info If you are regularly connecting and disconnecting to a network, you may also want to use your router's firmware to configure the router to reserve and only assign the same local network IP from its DHCP pool to your computer's MAC address. This will ensure that your Bee seamlessly connects when you rejoin the network!  ","version":"Next","tagName":"h3"},{"title":"Debugging Connectivity​","type":1,"pageTitle":"Connectivity","url":"/docs/bee/installation/connectivity#debugging-connectivity","content":" The above guide navigates your NAT, but there are still a few hurdles to overcome. To make sure there is a clear path from your computer to the outside world, let's follow our Bee's journey from the inside out.  Let's set up a netcat listener on all interfaces on the computer we'd like to run Bee on as we have above.  nc -l 0.0.0.0 1634   Now, let's verify we're able to connect to netcat by checking the connection from our local machine.  nc -zv 127.0.0.1 1634   Connection to 127.0.0.1 port 1634 [tcp/*] succeeded!   This should be a no brainer, the connection between localhost in not normally mediated.  If there is a problem here, the problem is with some other software running on your operating system or your operating system itself. Try a different port, such as 1734 and turning off any unneccesary software. If this doesn't work, you may need to try a different operating system environment. Please get in touch and we'll try to help!  If we were successful, let's move on to the next stage.  info If you are not able to get access to some firewall settings, or otherwise debug incoming connectivity, don't worry! All is not lost. Bee can function just fine with just outgoing connections. However, if you can, it is worth the effort to allow incoming connections, as the whole swarm will benefit from the increased connectivity.  Let's find out what our IP looks like to the Internet.  curl icanhazip.com   86.98.94.9   Now try to connect to your port using the global IP.  nc -zv 86.98.94.9 1634   If this is successful, our Bee node's path is clear!  If not, we can try a few things to make sure there are no barriers stopping us from getting through.  Check your computer's firewall.  Sometimes your computer is configured to prevent connections. If you are on a private network mediated by NAT, you can check if this is the problem by trying to connect from another device on your network using the local IP nc -zv 192.168.0.10 1634.  Ubuntu uses UFW, MacOS can be configured using the Firewall tab in the Security &amp; Privacysection of System Preferences. Windows usesDefender Firewall.  For each of these firewalls, set a special rule to allow UDP and TCP traffic to pass through on port 1634. You may want to limit this traffic to the Bee application only.  Check your ingress' firewall.  For a datacenter hired server, this configuration will often take place in somewhere in the web user interface. Refer to your server hosting provider's documentation to work out how to open ports to the open Internet. Ensure that both TCP and UDP traffic are allowed.  Similarly, if you are connecting from within a private network, you may find that the port is blocked by the router. Each router is different, so consult your router's firware documentation to make sure there are no firewalls in place blocking traffic on your Bee's designated p2p port.  You may check this using netcat by trying to connect using your computer's public IP, as above nc -zv 86.98.94.9 1634.  Docker  Docker adds another level of complexity.  To debug docker connectivity issues, we may use netcat as above to check port connections are working as expected. Double check that you are exposing the right ports to your local network, either by using the command line flags or in your docker-compose.yaml. You should be able to successfully check the connection locally using eg. nc -zv localhost 1634 then follow instructions above to make sure your local network has the correct ports exposed to the Internet.  Something else entirely?  Networking is a complex topic, but it keeps us all together. If you still can't connect to your Bee, get in touch via The Beehive and we'll do our best to get you connected. In the swarm, no Bee is left behind. ","version":"Next","tagName":"h3"},{"title":"Fund Your Node","type":0,"sectionRef":"#","url":"/docs/bee/installation/fund-your-node","content":"","keywords":"","version":"Next"},{"title":"A node's wallet​","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#a-nodes-wallet","content":" When your Bee node is installed, an Ethereum wallet is also created. This wallet is used by Bee to interact with the blockchain (e.g. for sending and receiving cheques, or for making purchases of postage stamps, etc.).  ","version":"Next","tagName":"h3"},{"title":"Chequebook​","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#chequebook","content":" When your node has downloaded enough content to exceed the free tier threshold, then cheques are sent to peers to provide payment in return for their services.  In order to send these cheques, a chequebook must be deployed on the blockchain for your node, and for full speed operation it can be funded with BZZ. This deployment happens when a node initialises for the first time. Your Bee node will warn you in its log if there aren't enough funds in its wallet for deploying the chequebook.  You can configure the amount of xBZZ to be sent from the node's wallet. It is 1 xBZZ by default, but it can be set to zero.  ","version":"Next","tagName":"h3"},{"title":"Joining the swarm (mainnet)​","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#joining-the-swarm-mainnet","content":" ","version":"Next","tagName":"h2"},{"title":"Basic deployment​","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#basic-deployment","content":" If you want to get your Bee node up and running as easily as possible, then you can set its--swap-initial-depositvalue to zero. This means that your node's chequebook will not get funded with xBZZ, meaning that other nodes will only serve it within the free tier bandwidth threshold.  Since gas fees on the Gnosis Chain are very low, you won't need much xDAI either to get started. You may acquire a small amount for free by using the official Gnosis Chain xDAI faucet xDAI Faucet. The required amount is a function of the current transaction fee on chain, but 0.01 xDAI should be more than enough to start up your node.  You can use the Blockscout block explorer to inspect what's going on with your wallet by searching for its Ethereum address.  ","version":"Next","tagName":"h3"},{"title":"Full performance node​","type":1,"pageTitle":"Fund Your Node","url":"/docs/bee/installation/fund-your-node#full-performance-node","content":" If you want to run a full node, or upload a lot of content, then you may need more xDAI for gas. To acquire this, you may convert DAI on the main Ethereum network to xDAI using theGnosis Chain bridge, or buy xDAIdirectly using fiat.  You will also need to fund your node with more xBZZ for full speed access, or to purchase postage stamps to upload content. To bridge BZZ from the Ethereum mainet to the Gnosis Chain, you may use the Gnosis Chain Bridge.  To find out what your node's Ethereum address is, please consult your relevant installation guide or check your logs!  Configure Your Wallet App  To interact with the BZZ ecosystem, you will need to make a couple of small configuration additions to your wallet software. In the case of e.g. MetaMask, you'll need toadd the Gnosis Chain network, and thenadd a custom token.  The canonical addresses for the BZZ token on the various blockchains are as follows:  Blockchain\tContract addressEthereum, BZZ\t0x19062190b1925b5b6689d7073fdfc8c2976ef8cb Gnosis Chain, xBZZ\t0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da Sepolia (testnet), sBZZ\t0x543dDb01Ba47acB11de34891cD86B675F04840db  Accessing Your Node's Wallet  If you wish to interact with the node's wallet directly then you can import it into a wallet app like MetaMask. To do that you will need the wallet file and its password. A Bee node's wallet key is stored within the keys/ folder in its datadir, in JSON format, and its password should be in a file nearby it.  For example on Debian or Ubuntu:  sudo cat /var/lib/bee/keys/swarm.key sudo cat /var/lib/bee/password   Testnet  A Bee node needs Sepolia ETH and sBZZ in its wallet to be able to properly interact with the test network. One way to acquire these funds is to sign into our Discord and request Sepolia ETH and sBZZ test tokens from thefaucet bot to your node's Ethereum address.  To find out what your node's Ethereum address is, please consult the installation guide or check the logs!  Once you have the address:  join our Discord servernavigate to the #faucet channelverify your usernamerequest test tokens from the faucet bot  To request the tokens you must type (not copy paste) the following, replacing the address with your own:  /faucet sprinkle 0xabeeecdef123452a40f6ea9f598596ca8556bd57   If you have problems, please let us know by making a post in the #faucet channel, we will do our best to provide tokens to everyone.  Note that you should use a Chromium-based client (e.g., Chrome, native Discord client) to type the faucet command, as support for other browsers is spotty. It's reported to not work on Firefox, for example.  Transactions may take a while to complete, please be patient. We're also keen for you to join us in the swarm, and indeed you soon will! 🐝 &amp;nbsp 🐝 &amp;nbsp 🐝 ","version":"Next","tagName":"h3"},{"title":"Hive","type":0,"sectionRef":"#","url":"/docs/bee/installation/hive","content":"","keywords":"","version":"Next"},{"title":"Docker​","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#docker","content":" Up to date Docker images for Bee are provided.  ","version":"Next","tagName":"h3"},{"title":"Docker-Compose​","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#docker-compose","content":" It becomes easier to run multiple Bee nodes withdocker-compose. Check out the Docker compose section of theDocker README.  ","version":"Next","tagName":"h3"},{"title":"Helm​","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#helm","content":" If you really want to run a lot of Bee nodes and you have experience using Kubernetes with Helm, you can have a look at how we manage our cluster under Ethersphere/helm.  ","version":"Next","tagName":"h3"},{"title":"Manually​","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#manually","content":" If you just want to run a handful of bee nodes, you can run multiple bee nodes by creating separate configuration files.  Create your first configuration file by running  bee printconfig &amp;&gt; bee-config-1.yaml   Make as many copies of bee-config-1.yaml as you want to run bee nodes. Increment the number in the name (bee-config-1 to bee-config-2) for each new configuration file.  Configure your nodes as desired, but ensure that the values api-addr, data-dir and p2p-addr are unique for each configuration.  ","version":"Next","tagName":"h3"},{"title":"Monitoring​","type":1,"pageTitle":"Hive","url":"/docs/bee/installation/hive#monitoring","content":" See the monitoring section on how to access Bee's internal metrics! Share your community creations (like swarmMonitor - thanks doristeo!) in the #node-operators channel of our Discord server so we can add you to our list of all things that are awesome and Swarm. 🧡 ","version":"Next","tagName":"h3"},{"title":"Docker","type":0,"sectionRef":"#","url":"/docs/bee/installation/docker","content":"","keywords":"","version":"Next"},{"title":"Install Docker and Docker Compose​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#install-docker-and-docker-compose","content":" info The steps for setting up Docker and Docker Compose may vary slightly from system to system, so take note system specific commands and make sure to modify them for your own system as needed.  DebianRPM For Debian-based Systems (e.g., Ubuntu, Debian)​ Step 1: Install Docker​ Update the package list: sudo apt-get update Install necessary packages: sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release Add Docker’s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Add Docker’s official repository to APT sources: echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) latest&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null Update the package list again: sudo apt-get update Install Docker packages: sudo apt-get install -y docker-ce docker-ce-cli containerd.io Step 2: Install Docker Compose Plugin​ info Skip this section if you are running Bee with Docker only. Update the package list: sudo apt-get update Install the Docker Compose plugin: sudo apt-get install docker-compose-plugin Verify the installation: docker compose version   ","version":"Next","tagName":"h2"},{"title":"Bee with Docker​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#bee-with-docker","content":" This section will guide you through setting up and running a single Bee node using Docker only without Docker Compose.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Create directories​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-1-create-directories","content":" Create home directory:  mkdir bee-node cd bee-node   Create data directory and change permissions  mkdir .bee sudo chown -R 999:999 .bee   ","version":"Next","tagName":"h3"},{"title":"Step 2: Bee Node Configuration​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-2-bee-node-configuration","content":" Based on your preferred node type, copy one of the three sample configurations below:  Full NodeLight NodeUltra Light Node Full node sample configuration​ # GENERAL BEE CONFIGURATION api-addr: :1633 p2p-addr: :1634 debug-api-addr: :1635 password: aaa4eabb0813df71afa45d data-dir: /home/bee/.bee cors-allowed-origins: [&quot;*&quot;] # DEBUG CONFIGURATION debug-api-enable: true verbosity: 5 # BEE MAINNET CONFIGURATION bootnode: /dnsaddr/mainnet.ethswarm.org # BEE MODE: FULL NODE CONFIGURATION full-node: true swap-enable: true blockchain-rpc-endpoint: https://xdai.fairdatasociety.org   Save the configuration into a YAML configuration file:  sudo vi ./bee.yml   Print out the configuration to make sure it was properly saved:  cat ./bee.yml   ","version":"Next","tagName":"h3"},{"title":"Step 3: Run Bee Node with Docker​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-3-run-bee-node-with-docker","content":" Use the following command to start up your node:  docker run -d --name bee-node \\ -v &quot;$(pwd)/.bee:/home/bee/.bee&quot; \\ -v &quot;$(pwd)/bee.yml:/home/bee/bee.yml&quot; \\ -p 127.0.0.1:1633:1633 \\ -p 1634:1634 \\ -p 127.0.0.1:1635:1635 \\ ethersphere/bee:2.1.0 start --config /home/bee/bee.yml   info Command breakdown: docker run: This is the command to start a new Docker container. -d: This flag runs the container in detached mode, meaning it runs in the background. --name bee-node: This sets the name of the container to bee-node. Naming containers can help manage and identify them easily. -v &quot;$(pwd)/.bee:/home/bee/.bee&quot;: This mounts a volume. It maps the .bee directory in your current working directory ($(pwd)) to the /home/bee/.bee directory inside the container. This allows the container to store and access persistent data on your host machine. -v &quot;$(pwd)/bee.yml:/home/bee/bee.yml&quot;: This mounts another volume. It maps the bee.yml file in your current working directory to the /home/bee/bee.yml file inside the container. This allows the container to use the configuration file from your host machine. -p 127.0.0.1:1633:1633: This maps port 1633 on 127.0.0.1 (localhost) of your host machine to port 1633 inside the container. This is used for the Bee API. -p 1634:1634: This maps port 1634 on all network interfaces of your host machine to port 1634 inside the container. This is used for P2P communication. -p 127.0.0.1:1635:1635: This maps port 1635 on 127.0.0.1 (localhost) of your host machine to port 1635 inside the container. This is used for the debug API. ethersphere/bee:2.1.0: This specifies the Docker image to use for the container. In this case, it is the ethersphere/bee image with the tag 2.1.0. start --config /home/bee/bee.yml: This specifies the command to run inside the container. It starts the Bee node using the configuration file located at /home/bee/bee.yml.  Note that we have mapped the Bee API and Debug API to 127.0.0.1 (localhost), this is to ensure that these APIs are not available publicly, as that would allow anyone to control our node.  Check that the node is running:  docker ps   If everything is set up correctly, you should see your Bee node listed:  CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e53aaa4e76ec ethersphere/bee:2.1.0 &quot;bee start --config …&quot; 17 seconds ago Up 16 seconds 127.0.0.1:1633-&gt;1633/tcp, 0.0.0.0:1634-&gt;1634/tcp, :::1634-&gt;1634/tcp, 127.0.0.1:1635-&gt;1635/tcp bee-node   And check the logs:  docker logs -f bee-node   The output should contain a line which prints the address of your node. Copy this address and save it for use in the next section.  &quot;time&quot;=&quot;2024-07-15 12:23:57.906429&quot; &quot;level&quot;=&quot;warning&quot; &quot;logger&quot;=&quot;node/chequebook&quot; &quot;msg&quot;=&quot;cannot continue until there is at least min xDAI (for Gas) available on address&quot; &quot;min_amount&quot;=&quot;0.0005750003895&quot; &quot;address&quot;=&quot;0xf50Bae90a99cfD15Db5809720AC1390d09a25d60&quot;   ","version":"Next","tagName":"h3"},{"title":"Step 4: Funding (Full and Light Nodes Only)​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-4-funding-full-and-light-nodes-only","content":" To obtain xDAI and fund your node, you can follow the instructions from the main install section.  ","version":"Next","tagName":"h3"},{"title":"Step 5: Add Stake​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-5-add-stake","content":" To add stake, make a POST request to the /stake endpoint and input the amount you wish to stake in PLUR as a parameter after /stake. For example, to stake an amount equal to 10 xBZZ:  curl -XPOST localhost:1633/stake/100000000000000000   Note that since we have mapped our host and container to the same port, we can use the default 1633 port to make our request. If you are running multiple nodes, make sure to update this command for other nodes which will be mapped to different ports on the host machine.  ","version":"Next","tagName":"h3"},{"title":"Bee with Docker Compose​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#bee-with-docker-compose","content":" By adding Docker Compose to our setup, we can simplify the management of our configuration by saving it in a docker-compose.yml file rather than specifying it all in the startup command. It also lays the foundation for running multiple nodes at once. First we will review how to run a single node with Docker Compose.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Create directory for node(s)​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-1-create-directory-for-nodes","content":" mkdir bee-nodes cd bee-nodes   ","version":"Next","tagName":"h3"},{"title":"Step 2: Create home directory for first node​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-2-create-home-directory-for-first-node","content":" mkdir node_01   ","version":"Next","tagName":"h3"},{"title":"Step 3: Create data directory and change permissions​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-3-create-data-directory-and-change-permissions","content":" mkdir node_01/.bee sudo chown -R 999:999 node_01/.bee   Here we change ownership to match the UID and GID of the user specified in the Bee Dockerfile.  ","version":"Next","tagName":"h3"},{"title":"Step 4: Bee node configuration​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-4-bee-node-configuration","content":" Below are sample configurations for different node types.  info The blockchain-rpc-endpoint entry is set to use the free and public https://xdai.fairdatasociety.org RPC endpoint, which is fine for testing things out but may not be stable enough for extended use. If you are running your own Gnosis Node or using a RPC provider service, make sure to update this value with your own endpoint.  Full NodeLight NodeUltra Light Node Full node sample configuration​ # GENERAL BEE CONFIGURATION api-addr: :1633 p2p-addr: :1634 debug-api-addr: :1635 password: aaa4eabb0813df71afa45d data-dir: /home/bee/.bee cors-allowed-origins: [&quot;*&quot;] # DEBUG CONFIGURATION debug-api-enable: true verbosity: 5 # BEE MAINNET CONFIGURATION bootnode: /dnsaddr/mainnet.ethswarm.org # BEE MODE: FULL NODE CONFIGURATION full-node: true swap-enable: true blockchain-rpc-endpoint: https://xdai.fairdatasociety.org   Copy the Docker configuration for the node type you choose and save it into a YAML configuration file:  sudo vi ./node_01/bee.yml   And print out the configuration to make sure it was properly saved:  cat ./node_01/bee.yml   ","version":"Next","tagName":"h3"},{"title":"Step 5: Docker Compose configuration​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-5-docker-compose-configuration","content":" You can use the same Docker Compose configuration for all the node types.  info Note that we have specified the exact version number of the image using the 2.1.0 tag. It's recommended to always specify the exact version number you need using the version tag. You can find all available tags for Bee on Docker Hub.  services: bee_01: container_name: bee-node_01 image: ethersphere/bee:2.1.0 command: start --config /home/bee/bee.yml volumes: - ./node_01/.bee:/home/bee/.bee - ./node_01/bee.yml:/home/bee/bee.yml ports: - 127.0.0.1:1633:1633 # bee api port - 1634:1634 # p2p port - 127.0.0.1:1635:1635 # debug port   warning Note that we are mapping to 127.0.0.1 (localhost), since we do not want to expose our Bee API endpoint to the public internet, as that would allow anyone to control our node. Make sure you do the same, or use a firewall to protect access to your node(s).  Copy the configuration and save it in a YAML file like we did in the previous step. Make sure that you are saving it to the root directory.  sudo vi ./docker-compose.yml   And print out the contents of the file to make sure it was saved properly:  cat ./docker-compose.yml   Now check that you have everything set up properly:  tree -a .   Your folder structure should look like this:  . ├── docker-compose.yml └── node_01 ├── .bee └── bee.yml   ","version":"Next","tagName":"h3"},{"title":"Step 6: Run bee node with docker compose:​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-6-run-bee-node-with-docker-compose","content":" docker compose up -d   The node is started in detached mode by using the -d flag so that it will run in the background.  Check that node is running:  docker ps   If we did everything properly we should see our node listed here:  CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e53aaa4e76ec ethersphere/bee:2.1.0 &quot;bee start --config …&quot; 17 seconds ago Up 16 seconds 127.0.0.1:1636-&gt;1633/tcp, 0.0.0.0:1637-&gt;1634/tcp, :::1637-&gt;1634/tcp, 127.0.0.1:1638-&gt;1635/tcp bee-node_01   Now let's check our logs:  docker logs -f bee-node_01   If everything went smoothly, we should see the logs from our Bee node. Unless you are running a node in ultra light mode, you should see a warning message in your logs which looks like this at the bottom of the logs:  &quot;time&quot;=&quot;2024-07-15 12:23:57.906429&quot; &quot;level&quot;=&quot;warning&quot; &quot;logger&quot;=&quot;node/chequebook&quot; &quot;msg&quot;=&quot;cannot continue until there is at least min xDAI (for Gas) available on address&quot; &quot;min_amount&quot;=&quot;0.0005750003895&quot; &quot;address&quot;=&quot;0xf50Bae90a99cfD15Db5809720AC1390d09a25d60&quot;   This is because in order for a light or full node to operate, your node is required to set up a chequebook contract on Gnosis Chain, which requires xDAI in order to pay for transaction fees. Find the address value and copy it for the next step:  ","version":"Next","tagName":"h3"},{"title":"Step 7: xDAI funding (full and light nodes only)​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-7-xdai-funding-full-and-light-nodes-only","content":" You can fund your node by transferring xDAI and xBZZ to the address you copied from the logs in the previous step.  To obtain xDAI and fund your node, you can follow the instructions from the main install section.  You can also try the node-funder tool, which is especially helpful when you are running multiple nodes, as is described in the next section.  ","version":"Next","tagName":"h3"},{"title":"Step 8: Add stake​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-8-add-stake","content":" To add stake, make a POST request to the /stake endpoint and input the amount you wish to stake in PLUR as a parameter after /stake. In the example below we have input a PLUR value equal to 10 xBZZ.  info The Bee API will not be available while your node is warming up, so wait until your node is fully initialized before staking.  curl -XPOST localhost:1633/stake/100000000000000000   Note that since we have mapped our host and container to the same port, we can use the default 1633 port to make our request. If you are running multiple Bees, make sure to update this command for other nodes which will be mapped to different ports on the host machine.  ","version":"Next","tagName":"h3"},{"title":"Running a Hive​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#running-a-hive","content":" In order to run multiple Bee nodes as a &quot;hive&quot;, all we need to do is repeat the process for running one node and then extend our Docker Compose configuration.  To start with, shut down your node from the first part of this guide if it is still running:  docker compose down   ","version":"Next","tagName":"h2"},{"title":"Step 1: Create new directories for additional node(s)​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-1-create-new-directories-for-additional-nodes","content":" Now create a new directory for your second node:  mkdir node_02   We also create a new data directory and set ownership to match the user in the official Bee Dockerfile.  mkdir node_02/.bee sudo chown -R 999:999 node_02/.bee   Repeat this process for however many new nodes you want to add.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Create new configuration file(s)​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-2-create-new-configuration-files","content":" And add a bee.yml configuration file. You can use the same configuration as for your first node. Here we will use the configuration for a full node:  # GENERAL BEE CONFIGURATION api-addr: :1633 p2p-addr: :1634 debug-api-addr: :1635 password: aaa4eabb0813df71afa45d data-dir: /home/bee/.bee cors-allowed-origins: [&quot;*&quot;] # DEBUG CONFIGURATION debug-api-enable: true verbosity: 5 # BEE MAINNET CONFIGURATION bootnode: /dnsaddr/mainnet.ethswarm.org # BEE MODE: FULL NODE CONFIGURATION full-node: true swap-enable: true blockchain-rpc-endpoint: https://xdai.fairdatasociety.org   sudo vi ./node_02/bee.yml   After saving the configuration, print out the configuration to make sure it was properly saved:  cat ./node_02/bee.yml   Repeat this step for any other additional node directories you created in the previous step.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Modify Docker Compose configuration​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-3-modify-docker-compose-configuration","content":" Here is the Docker compose configuration for running a hive of two Bee nodes:  services: bee_01: container_name: bee-node_01 image: ethersphere/bee:2.1.0 command: start --config /home/bee/bee.yml volumes: - ./node_01/.bee:/home/bee/.bee - ./node_01/bee.yml:/home/bee/bee.yml ports: - 127.0.0.1:1633:1633 # bee api port - 1634:1634 # p2p port - 127.0.0.1:1635:1635 # debug port bee_02: container_name: bee-node_02 image: ethersphere/bee:2.1.0 command: start --config /home/bee/bee.yml volumes: - ./node_02/.bee:/home/bee/.bee - ./node_02/bee.yml:/home/bee/bee.yml ports: - 127.0.0.1:1636:1633 # bee api port - 1637:1634 # p2p port - 127.0.0.1:1638:1635 # debug port   Here is a list of the changes we made to extend our setup:  Created an additional named service with a new unique name (bee_02).Created a unique name for each container_name value (bee-node_01 --&gt; bee-node_02).Made sure that volumes has the correct directory for each node (./node_01/ --&gt; ./node_02/).Updated the ports we map to so that each node has its own set of ports (ie, for node_02, we map 127.0.0.1:1636 to 1633 because node_01 is already using 127.0.0.1:1633, and do the same with the rest of the ports).  ","version":"Next","tagName":"h3"},{"title":"Step 4: Start up the hive​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-4-start-up-the-hive","content":" Start up the hive:  docker compose up -d   After starting up the hive, check that both nodes are running:  docker ps   CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a62ec5143d30 ethersphere/bee:2.1.0 &quot;bee start --config …&quot; 2 seconds ago Up 1 second 127.0.0.1:1636-&gt;1633/tcp, 0.0.0.0:1637-&gt;1634/tcp, :::1637-&gt;1634/tcp, 127.0.0.1:1638-&gt;1635/tcp bee-node_02 a3496b9bb2c8 ethersphere/bee:2.1.0 &quot;bee start --config …&quot; 2 seconds ago Up 1 second 127.0.0.1:1633-&gt;1633/tcp, 127.0.0.1:1635-&gt;1635/tcp, 0.0.0.0:1634-&gt;1634/tcp, :::1634-&gt;1634/tcp bee-node_01   And we can also check the logs for each node:  docker logs -f bee-node_01   Copy the address from the logs:  &quot;time&quot;=&quot;2024-07-23 11:54:08.657999&quot; &quot;level&quot;=&quot;warning&quot; &quot;logger&quot;=&quot;node/chequebook&quot; &quot;msg&quot;=&quot;cannot continue until there is at least min xDAI (for Gas) available on address&quot; &quot;min_xdai_amount&quot;=&quot;0.000500000002&quot; &quot;address&quot;=&quot;0x0E386401AFA8A9e23c6FFD81C7078505a36dB435&quot;   docker logs -f bee-node_02   And copy the second address:  &quot;time&quot;=&quot;2024-07-23 11:54:08.532812&quot; &quot;level&quot;=&quot;warning&quot; &quot;logger&quot;=&quot;node/chequebook&quot; &quot;msg&quot;=&quot;cannot continue until there is at least min xDAI (for Gas) available on address&quot; &quot;min_xdai_amount&quot;=&quot;0.000500000002&quot; &quot;address&quot;=&quot;0xa4DBEa11CE6D089455d1397c0eC3D705f830De69&quot;   ","version":"Next","tagName":"h3"},{"title":"Step 5: Fund nodes​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-5-fund-nodes","content":" You can fund your nodes by sending xDAI and xBZZ the addresses you collected from the previous step.  To obtain xDAI and fund your node, you can follow the instructions from the main install section.  Since you're running a hive, the node-funder tool is recommended, as it will allow you to rapidly fund and stake multiple nodes.  If you plan on staking, you will also want to get some xBZZ to stake. You will need 10 xBZZ for each node.  ","version":"Next","tagName":"h3"},{"title":"Step 6: Add stake​","type":1,"pageTitle":"Docker","url":"/docs/bee/installation/docker#step-6-add-stake","content":" info The Bee API will not be available while your nodes are warming up, so wait until your nodes are fully initialized before staking.  In order to stake you simply need to call the /stake endpoint with an amount of stake in PLUR as a parameter for each node.  For bee-node_01:  curl -XPOST localhost:1633/stake/100000000000000000   And for bee-node_02, note that we updated the port to match the one for the Bee API address we mapped to in the Docker Compose file:  curl -XPOST localhost:1636/stake/100000000000000000   You may also wish to make use of the node-funder tool, which in addition to allowing you to fund multiple addresses at once, also allows you to stake multiple addresses at once. ","version":"Next","tagName":"h3"},{"title":"Quick Start","type":0,"sectionRef":"#","url":"/docs/bee/installation/quick-start","content":"","keywords":"","version":"Next"},{"title":"Comparison of Node Types​","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#comparison-of-node-types","content":" Feature\tFull Node\tLight Node\tUltra-light NodeDownloading\t✅\t✅\t✅ Uploading\t✅\t✅\t❌ Can exceed free download limits by paying xBZZ\t✅\t✅\t❌ Storing &amp; sharing data\t✅\t❌\t❌ Storage incentives\t✅\t❌\t❌ SWAP incentives\t✅\t✅\t❌ PSS messaging\t✅\t✅\t✅ Gnosis Chain Connection\t✅\t✅\t❌  info The Swarm network includes two incentives protocols which each give Bee nodes incentives to participate in maintaining the network in a healthy way. Storage incentives: By participating in the storage incentives protocol, full nodes which store and share data chunks with the network have a chance to earn xBZZ. Staked xBZZ is required to earn storage incentives. Learn more in the staking section. SWAP incentives: The SWAP incentives protocol encourages full or light (but not ultra-light) nodes to share bandwidth with other nodes in exchange for payments from other nodes either in-kind or as a cheque to be settled at a future date. SWAP requires a chequebook contract to be set up on Gnosis Chain for each participating node.  ","version":"Next","tagName":"h3"},{"title":"Which type of node is the right choice?​","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#which-type-of-node-is-the-right-choice","content":" Different node types best suit different use cases:  ","version":"Next","tagName":"h2"},{"title":"Interact with the Swarm network​","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#interact-with-the-swarm-network","content":" If you want to interact with the Bee ecosystem in a decentralised way, but not earn xBZZ by storing or forwarding chunks, simply run a Beelight node in the background on your laptop or desktop computer. This will enable direct access to the Swarm network from your web browser and other applications.  If you only need to download a small amount of data from the Swarm network an ultra light node could be the right choice for you. This will allow you to download a limited amount of data but does not support uploading data.  To run a light or ultra-light node install Bee with the recommended configuration settings for your chosen node type.  info The Swarm Desktop app offers an easy way to automatically set up a light or ultra-light node and interact with it through a graphical user interface.  ","version":"Next","tagName":"h3"},{"title":"Support the Network and Earn xBZZ by Running a Full Node​","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#support-the-network-and-earn-xbzz-by-running-a-full-node","content":" Earn xBZZ and help keep the swarm strong by running your own full node. It's easy to set up your own Bee on a small computer like a Raspberry Pi 4, cloud host, or any home computer that's connected to the internet.  To run a full node install Bee with the recommended configuration settings for a full node.  info Staking is not required to run a full node, but is necessary to earn storage incentives. An altruistic person may want to run a full node without putting up any stake, and in fact, could possibly earn enough xBZZ from bandwidth (swap/cheque) compensation to be able to stake at some point in the future. Learn more in the staking section  ","version":"Next","tagName":"h3"},{"title":"Run Your Own Hive of Nodes​","type":1,"pageTitle":"Quick Start","url":"/docs/bee/installation/quick-start#run-your-own-hive-of-nodes","content":" Take it to the next level by keeping a whole hive of Bees! We provide tooling and monitoring to help you manage large deployments of multiple Bee nodes: Bee Hives. ","version":"Next","tagName":"h3"},{"title":"verify","type":0,"sectionRef":"#","url":"/docs/bee/installation/verify","content":"verify verify.md https://github.com/ethersphere/bee/pull/1581","keywords":"","version":"Next"},{"title":"Backups","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/backups","content":"","keywords":"","version":"Next"},{"title":"Files​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#files","content":" A full Bee node backup includes the kademlia-metrics, keys, localstore, password, stamperstore, statestore, and password files. The node should be stopped before taking a backup and not restarted until restoring the node from the backup to prevent the node from getting out of sync with the network.  A node's data including keys and stamp data are found in the data directory specified in its configuration.  Key data in backup files allows access to Bee node's Gnosis account. If lost or stolen it could lead to the loss of all assets in that account. Furthermore the stamperstore contains postage stamp data, and postage stamps will not be recoverable if it is lost.  info Don't forget - it's not a backup until you're sure the backup files work! Make sure to test restoring from backup files to prevent loss of assets due to data loss or corruption.  ","version":"Next","tagName":"h2"},{"title":"Ubuntu / Debian / Raspbian / CentOS package managers​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#ubuntu--debian--raspbian--centos-package-managers","content":" For Linux installations from package managers yum or apt, the data directory is located at:  /var/lib/bee   It may also be useful to include the bee.yaml config file in a backup so that configuration can be easily restored. The default location of the config file is:  /etc/bee   This guide uses the default package manager location for the data folder, make sure to change the commands to match your data folder's location if it in a different directory.  ","version":"Next","tagName":"h3"},{"title":"Binary package install​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#binary-package-install","content":" If you installed Bee using the automated shell script or by building Bee from source, your data directory will typically be located at:  /home/&lt;user&gt;/.bee   ","version":"Next","tagName":"h3"},{"title":"Docker Compose​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#docker-compose","content":" When using Docker Compose configuration files to run a node, Docker will create a volume for Bee.  Use docker cp to retrieve the contents of these folders:  docker cp bee_bee_1:/home/bee/.bee/ bee   ","version":"Next","tagName":"h3"},{"title":"Data types​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#data-types","content":" The data directory contains three directories. Its default location depends on the node install method used.  For shell script install the location is /home/&lt;user&gt;/.bee and for package manager installs it is /var/lib/bee. The directory structure is as follows:  ├── kademlia-metrics │ └── ... ├── keys │ ├── libp2p.key │ ├── libp2p_v2.key │ ├── pss.key │ └── swarm.key ├── localstore │ ├── indexstore │ └── sharky ├── password ├── stamperstore │ └── ... └── statestore │ └── ...   ","version":"Next","tagName":"h3"},{"title":"Keys​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#keys","content":" The keys directory contains three key files: libp2p.key, libp2p_v2.key, pss.key, swarm.key,. These keys are generated during the Bee node's initialisation and are required for maintaining access to your node.  danger The swarm.key file allows access to Bee node's Gnosis Chain account. If the key is lost or stolen it could lead to the loss of all assets secured by that key.  info To use swarm.key to manage the Gnosis account for a node through Metamask or other wallets, exportSwarmKeys can be used to convert swarm.key to a compatible format.  ","version":"Next","tagName":"h3"},{"title":"Statestore and Localstore.​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#statestore-and-localstore","content":" The statestore retains data related to its operation, and the localstore contains chunks locally which are frequently requested, pinned in the node, or are in the node's neighbourhood of responsibility.  info As the data in statestore and localstore continually changes during normal operation of a node, when taking a backup the node should first be stopped and not re-connected to the Swarm network until restoring from the backup (otherwise the statestore and localstore files will get out of sync with the network). It is possible to restore using out of sync statestore and localstore files, however it may lead to data loss or unexpected behavior related to chunk uploads, postage stamps, and more.  ","version":"Next","tagName":"h3"},{"title":"Back-up your node data​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#back-up-your-node-data","content":" Copy entire bee data folder to fully backup your node. This will do a full backup of kademlia-metrics, keys, localstore, stamperstore, password, and statestore, files into a newly created /backup directory. Make sure to save the backup directory to a safe location.  mkdir backup sudo cp -r /var/lib/bee/ backup   ","version":"Next","tagName":"h2"},{"title":"Back-up your password​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#back-up-your-password","content":" Depending on your configuration, your password may not be located in the /bee data directory which was copied in the previous step. If it has been specified in an environment variable or in your bee.yaml configuration file, make sure to copy it and save it together with the rest of your backup files or write it down in a safe place.  ","version":"Next","tagName":"h3"},{"title":"Back-up blockchain keys only​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#back-up-blockchain-keys-only","content":" If you only need to export your node's blockchain keys, you need to export the swarm.key UTC / JSON keystore file and the password file used to encrypt it. First create a directory for your keys and then copy your keys to that directory.  mkdir keystore sudo cp -r /var/lib/bee/keys/swarm.key /var/lib/bee/password keystore   ","version":"Next","tagName":"h3"},{"title":"Metamask Import​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#metamask-import","content":" If you wish to import your blockchain account to a wallet such as Metamask, you can simply print out your keystore file and password and use those data to import into the wallet:  ","version":"Next","tagName":"h2"},{"title":"View key and password for wallet import​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#view-key-and-password-for-wallet-import","content":" sudo cat /var/lib/bee/keys/swarm.key sudo cat /var/lib/bee/password   info Note that swarm.key is in UTC / JSON keystores format and is encrypted by default by your password file inside the /bee directory. Make sure to export both the swarm.key file and the password file in order to secure your wallet. If you need your private key exported from the keystore file, you may use one of a variety of Ethereum wallets which support exporting private keys from UTC files (such as Metamask, however we offer no guarantees for any software, make sure you trust it completely before using it).  ","version":"Next","tagName":"h3"},{"title":"Get private key from keystore and password​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#get-private-key-from-keystore-and-password","content":" To import to Metamask:  View and copy your swarm.key and password as shown aboveGo to Metamask and click &quot;Account 1&quot; --&gt; &quot;Import Account&quot;Choose the &quot;Select Type&quot; dropdown menu and choose &quot;JSON file&quot;Paste the password (Make sure to do this first)Upload exported JSON fileClick &quot;Import&quot;  To export your private key:  Go to Metamask and click &quot;Account 1&quot; to view the dropdown menu of all accountsClick the three dots next to the account you want to exportClick &quot;Account details&quot;Click &quot;Show private key&quot;Enter your Metamask password (not your keystore password)Copy your private key to a safe location  ","version":"Next","tagName":"h3"},{"title":"Restore from backup​","type":1,"pageTitle":"Backups","url":"/docs/bee/working-with-bee/backups#restore-from-backup","content":" danger Before restoring, make sure to check for any old node data at /var/lib/bee from a previous node which has not yet been backed up, and back it up if needed.  Install Bee. See install page for more info. Delete /bee folder which was generated during install sudo rm -r /var/lib/bee Navigate to backup directory and copy files to data folder. sudo cp -r /&lt;path-to-backup&gt;/. /var/lib/bee Revert ownership of the data folder. sudo chown -R bee:bee /var/lib/bee Start bee service and check logs to see if Bee node is running properly. sudo systemctl restart bee sudo journalctl --lines=100 --follow --unit bee  ","version":"Next","tagName":"h2"},{"title":"Bcrypt hashing utility","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/bcrypt","content":"Bcrypt hashing utility In order to generate a valid admin password hash you can use any available bcrypt compatible tools, both online and offline (htpasswd). For convenience Bee also provides a method to generate and validate password hashes: $ bee bcrypt super$ecret $2a$10$eZP5YuhJq2k8DFmj9UJGWOIjDtXu6NcAQMrz7Zj1bgIVBcHA3bU5u $ bee bcrypt --check super$ecret '$2a$10$eZP5YuhJq2k8DFmj9UJGWOIjDtXu6NcAQMrz7Zj1bgIVBcHA3bU5u' OK: password hash matches provided plain text info When validating a hash don't forget about quotes - the ($) hash prefix might interfere with your terminal.","keywords":"","version":"Next"},{"title":"Install Bee","type":0,"sectionRef":"#","url":"/docs/bee/installation/install","content":"","keywords":"","version":"Next"},{"title":"Recommended Hardware Specifications​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#recommended-hardware-specifications","content":" ","version":"Next","tagName":"h2"},{"title":"Full Nodes​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#full-nodes","content":" Minimum recommended specifications for each full node:  Dual core, recent generation, 2ghz processor8gb RAM30gb SSDStable internet connection  HDD drives are discouraged for full nodes due to their low speeds.  Note that there are additional hardware requirements if you choose to run your own Gnosis Chain node in order to provide your Bee node(s) with the required RPC endpoint. See configuration step for more details.  ","version":"Next","tagName":"h3"},{"title":"Light and UltraLight Nodes​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#light-and-ultralight-nodes","content":" The minimum required hardware specifications for light and ultralight nodes are very low, and can be run on practically any commercially available computer or microcomputer such as a Raspberry Pi.  ","version":"Next","tagName":"h3"},{"title":"Note on Startup Methods​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#note-on-startup-methods","content":" caution When a node is started using the bee start command the node process will be bound to the terminal session and will exit if the terminal is closed. If Bee was installed using one of the supported package managers it is set up to run as a service in the background with tools such as systemctl or brew services (which also use the bee start commandunder the hood). Depending on which of these startup methods was used, the default Bee directories will be different. See the configuration page for more information about default data and config directories.  ","version":"Next","tagName":"h2"},{"title":"Installation Steps​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#installation-steps","content":" Install BeeConfigure BeeFind Bee AddressFund node (Not required for ultra-light nodes)Wait for InitialisationCheck Bee StatusBack Up KeysDeposit Stake (Full node only, optional)  ","version":"Next","tagName":"h2"},{"title":"1. Install Bee​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#1-install-bee","content":" ","version":"Next","tagName":"h2"},{"title":"Package manager install​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#package-manager-install","content":" Bee is available for Linux in .rpm and .deb package format for a variety of system architectures, and is available for MacOS through Homebrew. See the releases page of the Bee repo for all available packages. One of the advantages of this method is that it automatically sets up Bee to run as a service as a part of the install process.  DebianRPMMacOS Get GPG key: curl -fsSL https://repo.ethswarm.org/apt/gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/ethersphere-apt-keyring.gpg Set up repo inside apt-get sources: echo \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ethersphere-apt-keyring.gpg] https://repo.ethswarm.org/apt \\ * *&quot; | sudo tee /etc/apt/sources.list.d/ethersphere.list &gt; /dev/null Install package: sudo apt-get update sudo apt-get install bee   You should see the following output to your terminal after a successful install (your default 'Config' location will vary depending on your operating system):  Reading package lists... Done Building dependency tree... Done Reading state information... Done The following NEW packages will be installed: bee 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 0 B/27.2 MB of archives. After this operation, 50.8 MB of additional disk space will be used. Selecting previously unselected package bee. (Reading database ... 82381 files and directories currently installed.) Preparing to unpack .../archives/bee_2.1.0_amd64.deb ... Unpacking bee (2.1.0) ... Setting up bee (2.1.0) ... Logs: journalctl -f -u bee.service Config: /etc/bee/bee.yaml Bee requires a Gnosis Chain RPC endpoint to function. By default this is expected to be found at ws://localhost:8546. Please see https://docs.ethswarm.org/docs/installation/install for more details on how to configure your node. After you finish configuration run 'sudo bee-get-addr' and fund your node with XDAI, and also XBZZ if so desired. Created symlink /etc/systemd/system/multi-user.target.wants/bee.service → /lib/systemd/system/bee.service.   ","version":"Next","tagName":"h3"},{"title":"Shell script install​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#shell-script-install","content":" The Bee install shell script for Linux automatically detects its execution environment and installs the latest stable version of Bee.  info Note that this install method copies precompiled binaries directly to the /usr/local/bin directory, so Bee installed through this method cannot be managed or uninstalled with package manager command line tools like dpkg, rpm, and brew. Also note that unlike the package install method, this install method will not set up Bee to run as a service (such as with systemctl or brew services).  Use either of the following commands to run the script and install Bee:  wget​  wget -q -O - https://raw.githubusercontent.com/ethersphere/bee/master/install.sh | TAG=v2.1.0 bash   curl​  curl -s https://raw.githubusercontent.com/ethersphere/bee/master/install.sh | TAG=v2.1.0 bash   ","version":"Next","tagName":"h3"},{"title":"Build from source​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#build-from-source","content":" If neither of the above methods works for your system, you can see our guide for building directly from source.  ","version":"Next","tagName":"h3"},{"title":"2. Configure Bee​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#2-configure-bee","content":" Before starting Bee for the first time you will need to make sure it is properly configured.  See the configuration section for more details.  ","version":"Next","tagName":"h2"},{"title":"Config for the Bee Service​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#config-for-the-bee-service","content":" When installing Bee with a package manager the configuration file for the Bee service will be automatically generated.  Check that the file was successfully generated and contains the default configuration for your system:  LinuxMacOS arm64 (Apple Silicon)MacOS amd64 (Intel) test -f /etc/bee/bee.yaml &amp;&amp; echo &quot;$FILE exists.&quot; cat /etc/bee/bee.yaml   The configuration printed to the terminal should match the default configuration for your operating system. See the the packaging section of the Bee repo for the default configurations for a variety of systems. In particular, pay attention to the config and data-dir values, as these differ depending on your system.  If your config file is missing you will need to create it yourself.  info You may be aware of the bee printconfig command which prints out a complete default Bee configuration. However, note that it outputs the default data and config directories for running Bee with bee start, and will need to be updated to use the default locations for your system if you plan on running Bee as a service with systemctl or brew services.  LinuxMacOS arm64 (Apple Silicon)MacOS amd64 (Intel) Create the bee.yaml config file and save it with the the default configuration. sudo touch /etc/bee/bee.yaml sudo vi /etc/bee/bee.yaml   ","version":"Next","tagName":"h3"},{"title":"Config for bee start​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#config-for-bee-start","content":" When running your node using bee start you can set options using either command line flags, environment variables, or a YAML configuration file. See the configuration section for more information on setting options for running a node with bee start.  No default YAML configuration file is generated to be used with the bee start command, so it must be generated and placed in the default config directory if you wish to use it to set your node's options. You can view the default configuration including the default config directory for your system with the bee printconfig command.  root@user-bee:~# bee printconfig   Check the configuration printed to your terminal. Note that the values for config and data-dir will vary slightly depending on your operating system.  # bcrypt hash of the admin password to get the security token admin-password: &quot;&quot; # allow to advertise private CIDRs to the public network allow-private-cidrs: false # HTTP API listen address api-addr: 127.0.0.1:1633 # chain block time block-time: &quot;15&quot; # rpc blockchain endpoint blockchain-rpc-endpoint: &quot;&quot; # initial nodes to connect to bootnode: [] # cause the node to always accept incoming connections bootnode-mode: false # cache capacity in chunks, multiply by 4096 to get approximate capacity in bytes cache-capacity: &quot;1000000&quot; # enable forwarded content caching cache-retrieval: true # enable chequebook chequebook-enable: true # enable clef signer clef-signer-enable: false # clef signer endpoint clef-signer-endpoint: &quot;&quot; # blockchain address to use from clef signer clef-signer-ethereum-address: &quot;&quot; # config file (default is $HOME/.bee.yaml) config: /root/.bee.yaml # origins with CORS headers enabled cors-allowed-origins: [] # data directory data-dir: /root/.bee # size of block cache of the database in bytes db-block-cache-capacity: &quot;33554432&quot; # disables db compactions triggered by seeks db-disable-seeks-compaction: true # number of open files allowed by database db-open-files-limit: &quot;200&quot; # size of the database write buffer in bytes db-write-buffer-size: &quot;33554432&quot; # debug HTTP API listen address debug-api-addr: 127.0.0.1:1635 # enable debug HTTP API debug-api-enable: false # cause the node to start in full mode full-node: false # help for printconfig help: false # triggers connect to main net bootnodes. mainnet: true # NAT exposed address nat-addr: &quot;&quot; # suggester for target neighborhood neighborhood-suggester: https://api.swarmscan.io/v1/network/neighborhoods/suggestion # ID of the Swarm network network-id: &quot;1&quot; # P2P listen address p2p-addr: :1634 # enable P2P WebSocket transport p2p-ws-enable: false # password for decrypting keys password: &quot;&quot; # path to a file that contains password for decrypting keys password-file: &quot;&quot; # percentage below the peers payment threshold when we initiate settlement payment-early-percent: 50 # threshold in BZZ where you expect to get paid from your peers payment-threshold: &quot;13500000&quot; # excess debt above payment threshold in percentages where you disconnect from your peer payment-tolerance-percent: 25 # postage stamp contract address postage-stamp-address: &quot;&quot; # postage stamp contract start block number postage-stamp-start-block: &quot;0&quot; # enable pprof mutex profile pprof-mutex: false # enable pprof block profile pprof-profile: false # price oracle contract address price-oracle-address: &quot;&quot; # redistribution contract address redistribution-address: &quot;&quot; # ENS compatible API endpoint for a TLD and with contract address, can be repeated, format [tld:][contract-addr@]url resolver-options: [] # enable permission check on the http APIs restricted: false # forces the node to resync postage contract data resync: false # staking contract address staking-address: &quot;&quot; # lru memory caching capacity in number of statestore entries statestore-cache-capacity: &quot;100000&quot; # protect nodes from getting kicked out on bootnode static-nodes: [] # enable storage incentives feature storage-incentives-enable: true # gas price in wei to use for deployment and funding swap-deployment-gas-price: &quot;&quot; # enable swap swap-enable: false # swap blockchain endpoint swap-endpoint: &quot;&quot; # swap factory addresses swap-factory-address: &quot;&quot; # initial deposit if deploying a new chequebook swap-initial-deposit: &quot;0&quot; # neighborhood to target in binary format (ex: 111111001) for mining the initial overlay target-neighborhood: &quot;&quot; # admin username to get the security token token-encryption-key: &quot;&quot; # enable tracing tracing-enable: false # endpoint to send tracing data tracing-endpoint: 127.0.0.1:6831 # host to send tracing data tracing-host: &quot;&quot; # port to send tracing data tracing-port: &quot;&quot; # service name identifier for tracing tracing-service-name: bee # bootstrap node using postage snapshot from the network use-postage-snapshot: false # log verbosity level 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace verbosity: info # time to warmup the node before some major protocols can be kicked off warmup-time: 5m0s # send a welcome message string during handshakes welcome-message: &quot;&quot; # withdrawal target addresses withdrawal-addresses-whitelist: []   If you do wish to use a YAML file to manage your configuration, simply generate a new file in the same directory as shown for config from the bee printconfig output. For us, that is /root/.bee.yaml (make sure to change this directory to match the value for the config directory which is output from bee printconfig on your system).  touch /root/.bee.yaml vi /root/.bee.yaml   You can then populate your .bee.yaml file with the default config output from bee printconfig to get started and save the file.  ","version":"Next","tagName":"h3"},{"title":"Set Bee API Address​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#set-bee-api-address","content":" danger Make sure that your api-addr (default 1633) is never exposed to the internet. It is good practice to employ one or more firewalls that block traffic on every port except for those you are expecting to be open.  If you are not using a firewall or other method to protect your node, it's recommended that you change your Bee API address from the default 1633 to 127.0.0.1:1633 to ensure that it is not publicly exposed to the internet.  ## HTTP API listen address (default &quot;:1633&quot;) api-addr: 127.0.0.1:1633   ","version":"Next","tagName":"h3"},{"title":"Set node type​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#set-node-type","content":" Full Node, Light Node, Ultra-light Node​  See the quick start guide if you're not sure which type of node to run.  To run Bee as a full node both full-node and swap-enable must be set to true, and a valid and stable Gnosis Chain RPC endpoint must be specified with blockchain-rpc-endpoint.  ## bee.yaml full-node: true   To run Bee as a light node full-node must be set to false and swap-enable must both be set to true, and a valid and stable Gnosis Chain RPC endpoint must be specified with blockchain-rpc-endpoint.  ## bee.yaml full-node: false   To run Bee as an ultra-light node full-node and swap-enable must both be set to false. No Gnosis Chain endpoint is required, and blockchain-rpc-endpoint can be left to its default value of an empty string.  ## bee.yaml full-node: false swap-enable: false   ","version":"Next","tagName":"h3"},{"title":"Set blockchain RPC endpoint​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#set-blockchain-rpc-endpoint","content":" Full and light Bee nodes require a Gnosis Chain RPC endpoint so they can interact with and deploy their chequebook contract, see the latest view of the current postage stamp batches, and interact with and top-up postage stamp batches. A blockchain RPC endpoint is not required for nodes running in ultra-light mode.  We strongly recommend you run your own Gnosis Chain node if you are planning to run a full node, and especially if you plan to run a hive of nodes.  If you do not wish to run your own Gnosis Chain node and are willing to trust a third party, you may also consider using an RPC endpoint provider such as GetBlock.  For running a light node or for testing out a single full node you may also consider using one of the free public RPC endpoints listed in the Gnosis Chain documentation. However the providers of these endpoints make no SLA or availability guarantees, and is therefore not recommended for full node operators.  To set your RPC endpoint provider, specify it with the blockchain-rpc-endpoint value, which is set to an empty string by default.  ## bee.yaml blockchain-rpc-endpoint: https://rpc.gnosis.gateway.fm   info The gateway.fm RPC endpoint in the example is great for learning how to set up Bee, but for the sake of security and reliability it's recommended that you run your run your own Gnosis Chain node rather than relying on a third party provider.  ","version":"Next","tagName":"h3"},{"title":"Configure Swap Initial Deposit (Optional)​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#configure-swap-initial-deposit-optional","content":" When running your Bee node with SWAP enabled for the first time, your node will deploy a 'chequebook' contract using the canonical factory contract which is deployed by Swarm. Once the chequebook is deployed, Bee will (optionally) deposit a certain amount of xBZZ in the chequebook contract so that it can pay other nodes in return for their services. The amount of xBZZ transferred to the chequebook is set by the swap-initial-deposit configuration setting (it may be left at the default value of zero or commented out).  ","version":"Next","tagName":"h3"},{"title":"NAT address​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#nat-address","content":" Swarm is all about sharing and storing chunks of data. To enable other Bees (also known as peers) to connect to your Bee, you must broadcast your public IP address in order to ensure that Bee is reachable on the correct p2p port (default 1634). We recommend that you manually configure your external IP and check connectivity to ensure your Bee is able to receive connections from other peers.  First determine your public IP address:  curl icanhazip.com   123.123.123.123   Then configure your node, including your p2p port (default 1634).  ## bee.yaml nat-addr: &quot;123.123.123.123:1634&quot;   ","version":"Next","tagName":"h3"},{"title":"ENS Resolution (Optional)​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#ens-resolution-optional","content":" The ENS domain resolution system is used to host websites on Bee, and in order to use this your Bee must be connected to a mainnet Ethereum blockchain node. We recommend you run your own ethereum node. An option for resource restricted devices is geth+nimbus and a guide can be found here. Other options include dappnode, nicenode, stereum and avado.  If you do not wish to run your own Ethereum node you may use a blockchain API service provider such as Infura. After signing up for Infura's API service, simply set your --resolver-options to https://mainnet.infura.io/v3/your-api-key.  ## bee.yaml resolver-options: [&quot;https://mainnet.infura.io/v3/&lt;&lt;your-api-key&gt;&gt;&quot;]   ","version":"Next","tagName":"h3"},{"title":"Set Target Neighborhood (Optional)​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#set-target-neighborhood-optional","content":" In older versions of Bee, neighborhood assignment was random by default. However, we can maximize a node's chances of winning xBZZ and also strengthen the resiliency of the network by strategically assigning neighborhoods to new nodes (see the staking section for more details).  Therefore the default Bee configuration now includes the neighborhood-suggester option which is set by default to to use the Swarmscan neighborhood suggester (https://api.swarmscan.io/v1/network/neighborhoods/suggestion). An alternative suggester URL could be used as long as it returns a JSON file in the same format {&quot;neighborhood&quot;:&quot;101000110101&quot;}, however only the Swarmscan suggester is officially recommended.  Setting Neighborhood Manually​  It's recommended to use the default neighborhood-suggester configuration for choosing your node's neighborhood, however you may also set your node's neighborhood manually using the target-neighborhood option.  To use this option, it's first necessary to identify potential target neighborhoods. A convenient tool for finding underpopulated neighborhoods is available at the Swarmscan website. This tool provides the leading binary bits of target neighborhoods in order of least populated to most. Simply copy the leading bits from one of the least populated neighborhoods (for example, 0010100001) and use it to set target-neighborhood. After doing so, an overlay address within that neighborhood will be generated when starting Bee for the first time.  ## bee.yaml target-neighborhood: &quot;0010100001&quot;   There is also a Swarmscan API endpoint which you can use to get a suggested neighborhood programmatically:  curl https://api.swarmscan.io/v1/network/neighborhoods/suggestion   A suggested neighborhood will be returned:  {&quot;neighborhood&quot;:&quot;1111110101&quot;}   ","version":"Next","tagName":"h3"},{"title":"3. Find Bee address​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#3-find-bee-address","content":" danger In the following section we print our swarm.key file contents to the terminal. Do not share the contents of your swarm.key or any other keys with anyone as it controls access to your Gnosis Chain account and can be used to withdraw assets.  As part of the process of starting a Bee full or light node the node must issue a Gnosis Chain transaction to set up its chequebook contract. We need to find our node's Gnosis Chain address in order to deposit xDAI which will be used to pay for this initial Gnosis Chain transaction. We can find our node's address by reading it directly from our key file. The location for your key file will differ depending on your system and startup method:  ","version":"Next","tagName":"h2"},{"title":"Bee Service​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#bee-service","content":" The default keys directory for a Bee node set up with a package manager to run as a service will differ depending on your system:  LinuxMacOS arm64 (Apple Silicon)MacOS amd64 (Intel) sudo cat /var/lib/bee/keys/swarm.key {&quot;address&quot;:&quot;215693a6e6cf0a27441075fd98c31d48e3a3a100&quot;,&quot;crypto&quot;:{&quot;cipher&quot;:&quot;aes-128-ctr&quot;,&quot;ciphertext&quot;:&quot;9e2706f1ce135dde449af5c529e80d560fb73007f1edb1636efcf4572eed1265&quot;,&quot;cipherparams&quot;:{&quot;iv&quot;:&quot;64b6482b8e04881446d88f4f9003ec78&quot;},&quot;kdf&quot;:&quot;scrypt&quot;,&quot;kdfparams&quot;:{&quot;n&quot;:32768,&quot;r&quot;:8,&quot;p&quot;:1,&quot;dklen&quot;:32,&quot;salt&quot;:&quot;3da537f2644274e3a90b1f6e1fbb722c32cbd06be56b8f55c2ff8fa7a522fb22&quot;},&quot;mac&quot;:&quot;11b109b7267d28f332039768c4117b760deed626c16c9c1388103898158e583b&quot;},&quot;version&quot;:3,&quot;id&quot;:&quot;d4f7ee3e-21af-43de-880e-85b6f5fa7727&quot;} The address field contains the Gnosis Chain address of the node, simply add the 0x prefix and save it for the next step (0x215693a6e6cf0a27441075fd98c31d48e3a3a100).  ","version":"Next","tagName":"h3"},{"title":"For bee start​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#for-bee-start","content":" The default keys directory when running Bee with the bee start command will depend on your operating system. Run the bee printconfig command to see the default config directory for your operating system, and look for the data-dir value.  data-dir: /root/.bee   Your keys folder is found in the root of the data-dir directory. We can print our key data to the terminal to find our node's address:  sudo cat /root/.bee/keys/swarm.key   {&quot;address&quot;:&quot;215693a6e6cf0a27441075fd98c31d48e3a3a100&quot;,&quot;crypto&quot;:{&quot;cipher&quot;:&quot;aes-128-ctr&quot;,&quot;ciphertext&quot;:&quot;9e2706f1ce135dde449af5c529e80d560fb73007f1edb1636efcf4572eed1265&quot;,&quot;cipherparams&quot;:{&quot;iv&quot;:&quot;64b6482b8e04881446d88f4f9003ec78&quot;},&quot;kdf&quot;:&quot;scrypt&quot;,&quot;kdfparams&quot;:{&quot;n&quot;:32768,&quot;r&quot;:8,&quot;p&quot;:1,&quot;dklen&quot;:32,&quot;salt&quot;:&quot;3da537f2644274e3a90b1f6e1fbb722c32cbd06be56b8f55c2ff8fa7a522fb22&quot;},&quot;mac&quot;:&quot;11b109b7267d28f332039768c4117b760deed626c16c9c1388103898158e583b&quot;},&quot;version&quot;:3,&quot;id&quot;:&quot;d4f7ee3e-21af-43de-880e-85b6f5fa7727&quot;}   The address field contains the Gnosis Chain address of the node, simply add the 0x prefix and save it for the next step (0x215693a6e6cf0a27441075fd98c31d48e3a3a100).  ","version":"Next","tagName":"h3"},{"title":"4. Fund Node​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#4-fund-node","content":" info We recommend not holding a high value of xBZZ or xDAI in your nodes' wallet. Please consider regularly removing accumulated funds.  To fund your node with xDAI you can use a Gnosis Chain compatible wallet such as Metamask, or a centralized exchange which supports xDAI withdrawals to Gnosis Chain. If you already have some DAI on Ethereum, you can use the Gnosis Chain Bridge to mint xDAI on Gnosis Chain.  After acquiring some xDAI, you can fund your node by sending some xDAI to the address you saved from the previous step (1 xDAI is more sufficient). You can optionally also send some xBZZ to your node which you can use to pay for storage on Swarm.  While depositing xBZZ is optional, node operators who intend to download or upload large amounts of data on Swarm may wish to deposit some xBZZ in order to pay for SWAP settlements.  For nodes which stake xBZZ and participate in the storage incentives system, small amounts of xDAI are used regularly to pay for staking related transactions on Gnosis Chain, so xDAI must be periodically topped up. See the staking section for more information.  After sending xDAI and optionally xBZZ to the Gnosis Chain address collected in the previous step, restart the node:  ","version":"Next","tagName":"h2"},{"title":"Bee Service​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#bee-service-1","content":" LinuxMacOS sudo systemctl restart bee   ","version":"Next","tagName":"h3"},{"title":"For bee start​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#for-bee-start-1","content":" Restart your terminal and run bee start:  bee start   ","version":"Next","tagName":"h3"},{"title":"5. Wait for Initialisation​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#5-wait-for-initialisation","content":" When first started in full or light mode, Bee must deploy a chequebook to the Gnosis Chain blockchain, and sync the postage stamp batch store so that it can check chunks for validity when storing or forwarding them. This can take a while, so please be patient! Once this is complete, you will see Bee starting to add peers and connect to the network.  You can keep an eye on progress by watching the logs while this is taking place.  LinuxMacOS arm64 (Apple Silicon)MacOS amd64 (Intel) sudo journalctl --lines=100 --follow --unit bee   If you've started your node with bee start, simply observe the logs printed to your terminal.  If all goes well, you will see your node automatically begin to connect to other Bee nodes all over the world.  INFO[2020-08-29T11:55:16Z] greeting &lt;Hi I am a very buzzy bee bzzzz bzzz bzz. 🐝&gt; from peer: b6ae5b22d4dc93ce5ee46a9799ef5975d436eb63a4b085bfc104fcdcbda3b82c   Now your node will begin to request chunks of data that fall within your radius of responsibilty - data that you will then serve to other p2p clients running in the swarm. Your node will then begin to respond to requests for these chunks from other peers.  Incentivisation In Swarm, storing, serving and forwarding chunks of data to other nodes can earn you rewards! Follow this guide to learn how to regularly cash out cheques other nodes send you in return for your services so that you can get your xBZZ!  Your Bee client has now generated an elliptic curve key pair similar to an Ethereum wallet. These are stored in your data directory, in the keys folder.  Keep Your Keys and Password Safe! Your keys and password are very important, back up these files and store them in a secure place that only you have access to. With great privacy comes great responsibility - while no-one will ever be able to guess your key - you will not be able to recover them if you lose them either, so be sure to look after them well and keep secure backups.  ","version":"Next","tagName":"h2"},{"title":"6. Check if Bee is Working​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#6-check-if-bee-is-working","content":" First check that the correct version of Bee is installed:  bee version   2.1.0   Once the Bee node has been funded, the chequebook deployed, and postage stamp batch store synced, its HTTP APIwill start listening at localhost:1633.  To check everything is working as expected, send a GET request to localhost port 1633.  curl localhost:1633   Ethereum Swarm Bee   Great! Our API is listening!  Next, let's see if we have connected with any peers by querying the API which listens at port 1633 by default (localhost:1633).  info Here we are using the jq utility to parse our javascript. Use your package manager to install jq, or simply remove everything after and including the first | to view the raw json without it.  curl -s localhost:1633/peers | jq &quot;.peers | length&quot;   87   Perfect! We are accumulating peers, this means you are connected to the network, and ready to start using Bee to upload and download content or host and browse websites hosted on the Swarm network.  Welcome to the swarm! 🐝 🐝 🐝 🐝 🐝  ","version":"Next","tagName":"h2"},{"title":"7. Back Up Keys​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#7-back-up-keys","content":" Once your node is up and running, make sure to back up your keys.  ","version":"Next","tagName":"h2"},{"title":"8. Deposit Stake (Optional)​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#8-deposit-stake-optional","content":" While depositing stake is not required to run a Bee node, it is required in order for a node to receive rewards for sharing storage with the network. You will need to deposit xBZZ to the staking contract for your node. To do this, send a minimum of 10 xBZZ to your nodes' wallet and run:  curl -XPOST localhost:1633/stake/100000000000000000   This will initiate a transaction on-chain which deposits the specified amount of xBZZ into the staking contract.  Storage incentive rewards are only available for full nodes which are providing storage capacity to the network.  Note that SWAP rewards are available to all full and light nodes, regardless of whether or not they stake xBZZ in order to participate in the storage incentives system.  ","version":"Next","tagName":"h2"},{"title":"Getting help​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#getting-help","content":" The CLI has documentation built-in. Running bee gives you an entry point to the documentation. Running bee start -h or bee start --help will tell you how you can configure your Bee node via the command line arguments.  You may also check out the configuration guide, or simply run your Bee terminal command with the --help flag, eg. bee start --help or bee --help.  ","version":"Next","tagName":"h2"},{"title":"Next Steps to Consider​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#next-steps-to-consider","content":" ","version":"Next","tagName":"h2"},{"title":"Access the Swarm​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#access-the-swarm","content":" If you'd like to start uploading or downloading files to Swarm, start here.  ","version":"Next","tagName":"h3"},{"title":"Explore the API​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#explore-the-api","content":" The Bee API is the primary method for interacting with Bee and getting information about Bee. After installing Bee and getting it up and running, it's a good idea to start getting familiar with the API.  ","version":"Next","tagName":"h3"},{"title":"Run a hive!​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#run-a-hive","content":" If you would like to run a hive of many Bees, check out the hive operators section for information on how to operate and monitor many Bees at once.  ","version":"Next","tagName":"h3"},{"title":"Start building DAPPs on Swarm​","type":1,"pageTitle":"Install Bee","url":"/docs/bee/installation/install#start-building-dapps-on-swarm","content":" If you would like to start building decentralised applications on Swarm, check out our section for developing with Bee. ","version":"Next","tagName":"h3"},{"title":"Bee API","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/bee-api","content":"","keywords":"","version":"Next"},{"title":"Interacting With the API​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#interacting-with-the-api","content":" You can use HTTP requests to interact directly with Bee API to access all of your Bee node's various functions such as purchasing stamp batches, uploading and downloading, staking, and more.  ","version":"Next","tagName":"h2"},{"title":"Alternatives for Working with the API​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#alternatives-for-working-with-the-api","content":" For developers, the Bee JS library offers a more convenient way to interact with the API in a NodeJS environment.  For many other common use cases, you may prefer to make use of the Swarm CLI tool, as it offers a convenient command line based interface for interacting with your node's API.  ","version":"Next","tagName":"h3"},{"title":"Exploring Node Status​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#exploring-node-status","content":" After installing and starting up your node, we can begin to understand the node's status by interacting with the API.  First, let's check how many nodes we are currently connected to.  curl -s http://localhost:1633/peers | jq '.peers | length'   23   Great! We can see that we are currently connected and sharing data with 23 other nodes!  info Here we are using the jq command line utility to count the amount of objects in the peers array in the JSON response we have received from our API, learn more about how to install and use jq here.  Let's review a handful of endpoints which will provide you with important information relevant to detecting and diagnosing problems with your nodes.  ","version":"Next","tagName":"h2"},{"title":"/status​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#status","content":" The /status endpoint returns a quick summary of some important metrics for your node.   curl -s http://localhost:1633/status | jq   { &quot;peer&quot;: &quot;da7e5cc3ed9a46b6e7491d3bf738535d98112641380cbed2e9ddfe4cf4fc01c4&quot;, &quot;proximity&quot;: 0, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3747532, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 183, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }   &quot;peer&quot; - Your node's overlay address.&quot;proximity&quot; - The proximity order (number of shared leading bits) with this node and your node.&quot;beeMode&quot; - The mode of your node, can be &quot;full&quot;, &quot;light&quot;, or &quot;ultraLight&quot;.&quot;reserveSize&quot; - The number of chunks your node is currently storing in its reserve. This value should be roughly similar across nodes in the network. It should be identical for nodes within the same neighborhood.&quot;pullsyncRate&quot; - The rate at which your node is currently syncing chunks from other nodes in the network.&quot;storageRadius&quot; - The radius of responsibility - the proximity order of chunks for which your node is responsible for storing. It should generally match the radius shown on Swarmscan.&quot;connectedPeers&quot; - The number of peers your node is connected to.&quot;neighborhoodSize&quot; - The number of total neighbors in your neighborhood, not including your own node. The more nodes in your neighborhood, the lower your chance of winning rewards as a staking node.&quot;batchCommitment&quot; - The total number of chunks which would be stored on the Swarm network if 100% of all postage batches were fully utilised.&quot;isReachable&quot; - Whether or not your node is reachable on the p2p API by other nodes on the Swarm network (port 1634 by default).  ","version":"Next","tagName":"h3"},{"title":"/status/peers​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#statuspeers","content":" The /status/peers endpoint returns information about all the peers of the node making the request. The type of the object returned is the same as that returned from the /status endpoint. This endpoint is useful for diagnosing syncing / availability issues with your node.  The nodes are ordered by distance (Kademlia distance, not spatial / geographic distance) to your node, with the most distant nodes with PO (proximity order) of zero at the top of the list and the closest nodes with higher POs at the bottom of the list. The nodes at the bottom of the list with a PO equal or greater than the storage depth make up the nodes in your own node's neighborhood. It's possible that not all nodes in your neighborhood will appear in this list each time you call the endpoint if the connection between your nodes and the rest of the nodes in the neighborhood is not stable.  Here are the last 12 entries:   curl -s http://localhost:1633/status/peers | jq    ... { &quot;peer&quot;: &quot;da33f7a504a74094242d3e542475b49847d1d0f375e0c86bac1c9d7f0937acc0&quot;, &quot;proximity&quot;: 9, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3782924, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 188, &quot;neighborhoodSize&quot;: 11, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4b529cc1aedc62e31849cf7f8ab8c1866d9d86038b857d6cf2f590604387fe&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3719593, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 176, &quot;neighborhoodSize&quot;: 11, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da5d39a5508fadf66c8665d5e51617f0e9e5fd501e429c38471b861f104c1504&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3777241, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 198, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4cb0d125bba638def55c0061b00d7c01ed4033fa193d6e53a67183c5488d73&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3849125, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 181, &quot;neighborhoodSize&quot;: 13, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4b1cd5d15e061fdd474003b5602ab1cff939b4b9e30d60f8ff693141ede810&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3778452, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 183, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133827002368, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da49e6c6174e3410edad2e0f05d704bbc33e9996bc0ead310d55372677316593&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3779560, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 185, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4cdab480f323d5791d3ab8d22d99147f110841e44a8991a169f0ab1f47d8e5&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3778518, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 189, &quot;neighborhoodSize&quot;: 11, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4ccec79bc34b502c802415b0008c4cee161faf3cee0f572bb019b117c89b2f&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3779003, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 179, &quot;neighborhoodSize&quot;: 10, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da69d412b79358f84b7928d2f6b7ccdaf165a21313608e16edd317a5355ba250&quot;, &quot;proximity&quot;: 11, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3712586, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 189, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133827002368, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da61967b1bd614a69e5e83f73cc98a63a70ebe20454ca9aafea6b57493e00a34&quot;, &quot;proximity&quot;: 11, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3780190, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 182, &quot;neighborhoodSize&quot;: 13, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da7b6a268637cfd6799a9923129347fc3d564496ea79aea119e89c09c5d9efed&quot;, &quot;proximity&quot;: 13, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3721494, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 188, &quot;neighborhoodSize&quot;: 14, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da7a974149543df1b459831286b42b302f22393a20e9b3dd9a7bb5a7aa5af263&quot;, &quot;proximity&quot;: 13, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3852986, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 186, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true } ] }   The first entry has a proximity of 9, meaning that it is below the storageRadius (depth) of 10 and so does not fall into our node's neighborhood. All the other nodes further down the list have a proximity of 10 or greater with our node, meaning they fall into our node's neighborhood.  You may notice that there is some variation in neighborhoodSize, however they are all close to the same value. This could be due to temporary connection problems between nodes in the neighborhood, but is not considered a problem unless the neighborhood size is much lower than its peers or at zero. As long as each node is connected to most of the other nodes in its neighborhood, each node in the neighborhood will be able to sync the required chunks to its reserve.  And we can compare these entries to our own node's /status results for diagnostic purposes:   curl -s http://localhost:1633/status | jq   { &quot;peer&quot;: &quot;da7e5cc3ed9a46b6e7491d3bf738535d98112641380cbed2e9ddfe4cf4fc01c4&quot;, &quot;proximity&quot;: 0, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3747532, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 183, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }   From the results we can see that we have a healthy neighborhood size when compared with the other nodes in our neighborhood and also has the same batchCommitment value as it should.  ","version":"Next","tagName":"h3"},{"title":"/redistributionstate​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#redistributionstate","content":" This endpoint provides an overview of values related to storage fee redistribution game (in other words, staking rewards). You can use this endpoint to check whether or not your node is participating properly in the redistribution game.  curl -s http://localhost:1633/redistributionstate | jq { &quot;minimumGasFunds&quot;: &quot;11080889201250000&quot;, &quot;hasSufficientFunds&quot;: true, &quot;isFrozen&quot;: false, &quot;isFullySynced&quot;: true, &quot;phase&quot;: &quot;claim&quot;, &quot;round&quot;: 212859, &quot;lastWonRound&quot;: 207391, &quot;lastPlayedRound&quot;: 210941, &quot;lastFrozenRound&quot;: 210942, &quot;lastSelectedRound&quot;: 212553, &quot;lastSampleDuration&quot;: 491687776653, &quot;block&quot;: 32354719, &quot;reward&quot;: &quot;1804537795127017472&quot;, &quot;fees&quot;: &quot;592679945236926714&quot;, &quot;isHealthy&quot;: true }   &quot;minimumGasFunds&quot; - The minimum required xDAI denominated in wei (1 xDAI = 10^18 wei) required for a node to participate in the redistribution game.&quot;hasSufficientFunds&quot; - Whether your node has at least the &quot;minimumGasFunds&quot; amount of xDAI.&quot;isFrozen&quot; - Whether your node is currently frozen. See docs for more information on freezing.&quot;isFullySynced&quot; - Whether your node has fully synced all the chunks in its &quot;storageRadius&quot; (the value returned from the /reservestate endpoint.)&quot;phase&quot; - The current phase of the redistribution game (this does not indicate whether or not your node is participating in the current phase).&quot;round&quot; - The current number of the round of the redistribution game.&quot;lastWonRound&quot; - The last round number in which your node won the redistribtuion game.&quot;lastPlayedRound&quot; - The last round number in which your node participating in the redistribution game. If this number matches the number of the current round shown in &quot;round&quot;, then your node is participating in the current round.&quot;lastFrozenRound&quot; - The last round in which your node was frozen.&quot;lastSelectedRound&quot; - The last round in which your node's neighborhood was selected. Note that it is possible for your node's neighborhood to be selected without your node playing in the redistribution game. This may potentially indicate your node's hardware is not sufficient to calculate the commitment hash fast enough. See section on the /rchash endpoint for more information.&quot;lastSampleDuration&quot; - The time it took for your node to calculate the sample commitment hash in nanoseconds.&quot;block&quot; - current Gnosis block number&quot;reward&quot; - The total all-time reward in PLUR earned by your node.&quot;fees&quot; - The total amount in fees paid by your node denominated in xDAI wei.&quot;isHealthy&quot; - a check of whether your node’s storage radius is the same as the most common radius from among its peer nodes  ","version":"Next","tagName":"h3"},{"title":"/reservestate​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#reservestate","content":" This endpoint shows key information about the reserve state of your node. You can use it to identify problems with your node related to its reserve (whether it is syncing chunks properly into its reserve for example).   curl -s http://localhost:1633/reservestate | jq { &quot;radius&quot;: 15, &quot;&quot;: 10, &quot;commitment&quot;: 134121783296 }   Let's take a look at each of these values:  &quot;radius&quot; - is what the storage radius would be if every available batch was 100% utilised, it is essentially the radius needed for the network to handle all of the batches at 100% utilisation. Radius is measured as a proximity order (PO).&quot;storageRadius&quot; - The radius of responsibility - the proximity order of chunks for which your node is responsible for storing. It should generally match the radius shown on [Swarmscan](https://swarmscan.io/neighborhoods.&quot;commitment&quot; - The total number of chunks which would be stored on the Swarm network if 100% of all postage batches were fully utilised.  ","version":"Next","tagName":"h3"},{"title":"/chainstate​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#chainstate","content":" This endpoint relates to your node's interactions with the Swarm Smart contracts on the Gnosis Chain.   curl -s http://localhost:1633/chainstate | jq { &quot;chainTip&quot;: 32354482, &quot;block&quot;: 32354475, &quot;totalAmount&quot;: &quot;25422512270&quot;, &quot;currentPrice&quot;: &quot;24000&quot; }   &quot;chainTip&quot; - The latest Gnosis Chain block number. Should be as high as or almost as high as the block number shown at GnosisScan.&quot;block&quot; - The block to which your node has synced data from Gnosis Chain. This may be far behind the &quot;chainTip&quot; when you first start up your node as it takes some time to sync all the data from the blockchain (especially if you are not using the snapshot option). Should be very close to &quot;chainTip&quot; if your node has already been operating for a while.&quot;totalAmount&quot; - Cumulative value of all prices per chunk in PLUR for each block.&quot;currentPrice&quot; - The price in PLUR to store a single chunk for each Gnosis Chain block.  ","version":"Next","tagName":"h3"},{"title":"/topology​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#topology","content":" This endpoint allows you to explore the topology of your node within the Kademlia network. The results are split into 32 bins from bin_0 to bin_32. Each bin represents the nodes in the same neighborhood as your node at each proximity order from PO 0 to PO 32.  As the output of this file can be very large, we save it to the topology.json file for easier inspection:   curl -s http://localhost:1633/topology | jq '.' &gt; topology.json   We open the file in vim for inspection:  vim topology.json   The /topology results begin with several values describing the entire topology and are followed by the details for each of the 32 bins. Lets first look at the values describing the total topology:   &quot;baseAddr&quot;: &quot;da7e5cc3ed9a46b6e7491d3bf738535d98112641380cbed2e9ddfe4cf4fc01c4&quot;, &quot;population&quot;: 20514, &quot;connected&quot;: 176, &quot;timestamp&quot;: &quot;2024-02-08T20:57:03.815537925Z&quot;, &quot;nnLowWatermark&quot;: 3, &quot;depth&quot;: 10, &quot;reachability&quot;: &quot;Public&quot;, &quot;networkAvailability&quot;: &quot;Available&quot;, ...   &quot;baseAddr&quot; - Your node's overlay address. &quot;population&quot; - The total number of nodes your node has collected information about. This number should be around ####. If it is far higher or lower it likely indicates a problem. &quot;connected&quot; - The total number of nodes your node is currently connected to. &quot;timestamp&quot; - The time at which this topology snapshot was taken. &quot;nnLowWatermark&quot; - ??? &quot;depth&quot; - &quot;reachability&quot; &quot;networkAvailability&quot; After the first section are 32 sections, one for each bin. At the front of each of these sections is a summary of information about the respective bin followed two list, one of disconnected peers and the other of connected peers. Let's take a look at bin_10 as an example:  ... &quot;bin_10&quot;: { &quot;population&quot;: 3, // The total number of peers in this bin including both connected and disconnected peers. &quot;connected&quot;: 2, // Number of connected peers &quot;disconnectedPeers&quot;: [ //List of all disconnected peers { &quot;address&quot;: &quot;3e06e4667260c761f1b6a8539a99621c1af1f945e97667376c13b5f84984bcbc&quot;, &quot;metrics&quot;: { &quot;lastSeenTimestamp&quot;: 1707426772, &quot;sessionConnectionRetry&quot;: 2, &quot;connectionTotalDuration&quot;: 104619, &quot;sessionConnectionDuration&quot;: 72, &quot;sessionConnectionDirection&quot;: &quot;outbound&quot;, &quot;latencyEWMA&quot;: 849, &quot;reachability&quot;: &quot;Public&quot;, &quot;healthy&quot;: true } } ], &quot;connectedPeers&quot;: [ // List of all connected peers { &quot;address&quot;: &quot;3e09deca28d24a4c6dab9350dd0fb27a2333f03120b9f92f0ac0fd245707c9e3&quot;, &quot;metrics&quot;: { &quot;lastSeenTimestamp&quot;: 1707426766, &quot;sessionConnectionRetry&quot;: 2, &quot;connectionTotalDuration&quot;: 105059, &quot;sessionConnectionDuration&quot;: 33, &quot;sessionConnectionDirection&quot;: &quot;outbound&quot;, &quot;latencyEWMA&quot;: 899, &quot;reachability&quot;: &quot;Public&quot;, &quot;healthy&quot;: true } }, { &quot;address&quot;: &quot;3e1cdf7b1072fcde264c75f70635b9c1e9c1623eab2de55a0380f17b07751955&quot;, &quot;metrics&quot;: { &quot;lastSeenTimestamp&quot;: 1707426741, &quot;sessionConnectionRetry&quot;: 1, &quot;connectionTotalDuration&quot;: 109216, &quot;sessionConnectionDuration&quot;: 59, &quot;sessionConnectionDirection&quot;: &quot;outbound&quot;, &quot;latencyEWMA&quot;: 948, &quot;reachability&quot;: &quot;Public&quot;, &quot;healthy&quot;: true } } ] },   ","version":"Next","tagName":"h3"},{"title":"/node​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#node","content":" This endpoint returns info about options related to your node type and also displays your current node type.  curl -s http://localhost:1633/node | jq { &quot;beeMode&quot;: &quot;full&quot;, &quot;chequebookEnabled&quot;: true, &quot;swapEnabled&quot;: true }   &quot;beeMode&quot; - The mode of your node, can be &quot;full&quot;, &quot;light&quot;, or &quot;ultraLight&quot;.&quot;chequebookEnabled&quot; - Whether or not your node's chequebook-enable option is set to true.&quot;swapEnabled&quot; - Whether or not your node's swap-enable option is set to true.  If your node is not operating in the correct mode, this can help you to diagnose whether you have set your options correctly.  ","version":"Next","tagName":"h3"},{"title":"/rchash​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#rchash","content":" Calling the /rchash endpoint will make your node generate a reserve commitment hash (the hash used in the redistribution game), and will report the amount of time it took to generate the hash. This is useful for getting a performance benchmark to ensure that your node's hardware is sufficient.  sudo curl -sX GET http://localhost:1633/rchash/10/aaaa/aaaa | jq   It should not take much longer than 6 minutes at most for results to be returned:  { &quot;Sample&quot;: { &quot;Items&quot;: [ &quot;000003dac2b2f75842e410474dfa4c1e6e0b9970d81b57b33564c5620667ba96&quot;, &quot;00000baace30916f7445dbcc44d9b55cb699925acfbe157e4498c63bde834f40&quot;, &quot;0000126f48fb1e99e471efc683565e4b245703c922b9956f89cbe09e1238e983&quot;, &quot;000012db04a281b7cc0e6436a49bdc5b06ff85396fcb327330ca307e409d2a04&quot;, &quot;000014f365b1a381dda85bbeabdd3040fb1395ca9e222e72a597f4cc76ecf6c2&quot;, &quot;00001869a9216b3da6814a877fdbc31f156fc2e983b52bc68ffc6d3f3cc79af0&quot;, &quot;0000198c0456230b555d5261091cf9206e75b4ad738495a60640b425ecdf408f&quot;, &quot;00001a523bd1b688472c6ea5a3c87c697db64d54744829372ac808de8ec1d427&quot; ], &quot;Hash&quot;: &quot;7f7d93c6235855fedc34e32c6b67253e27910ca4e3b8f2d942efcd758a6d8829&quot; }, &quot;Time&quot;: &quot;2m54.087909745s&quot; }   If the Time value is much longer than 6 minutes then it likely means that the node's hardware performance is not sufficient. Consider upgrading to use faster memory or processor.  ","version":"Next","tagName":"h3"},{"title":"/health​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#health","content":" The /health endpoint is primarily used by infra tools such as Docker / Kubernetes to check whether the server is live.   curl -s http://localhost:1633/health | jq { &quot;status&quot;: &quot;ok&quot;, &quot;version&quot;: &quot;2.0.0-759f56f7&quot;, &quot;apiVersion&quot;: &quot;5.1.1&quot;, }   &quot;status&quot; - &quot;ok&quot; if the server is responsive.&quot;version&quot; - The version of your Bee node. You can find latest version by checking the Bee github repo.&quot;apiVersion&quot;  ","version":"Next","tagName":"h3"},{"title":"Debug API Removal Notice​","type":1,"pageTitle":"Bee API","url":"/docs/bee/working-with-bee/bee-api#debug-api-removal-notice","content":" info The Debug API endpoints have been merged into the Bee API in the Bee version 2.1.0 release, and will be fully removed in the 2.2.0 release. The Debug API reference docs are still available until the 2.2.0 release for your reference. ","version":"Next","tagName":"h2"},{"title":"Bee Dashboard","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/bee-dashboard","content":"","keywords":"","version":"Next"},{"title":"Bee Dashboard​","type":1,"pageTitle":"Bee Dashboard","url":"/docs/bee/working-with-bee/bee-dashboard#bee-dashboard","content":" Our wonderful community (shout out to matmertz25!) has teamed up with our inimitable javascript team, the Bee Gees 🕺 , to create Bee Dashboard a graphical user interface for your Bee.  Use this tool to make sure your Bee is functioning correctly, keep an eye on cheques as they accumulate, and cash them out, withdraw your earned xBZZ, and much more!  Head over to the Github repo for more information on how to install and use Bee Dashboard. ","version":"Next","tagName":"h3"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/introduction","content":"","keywords":"","version":"Next"},{"title":"Configuration​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#configuration","content":" Learn how to configure your node, and the details behind all the configuration options Bee provides.  ","version":"Next","tagName":"h2"},{"title":"Bee API​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#bee-api","content":" Access the HTTP API directly for detailed information about your Bee.  ","version":"Next","tagName":"h2"},{"title":"Logs and Files​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#logs-and-files","content":" Find out where Bee stores your logs and files.  ","version":"Next","tagName":"h2"},{"title":"Bee Dashboard and Swarm CLI​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#bee-dashboard-and-swarm-cli","content":" Try out the Bee Dashboard app which offers an easy to use graphical interface for your Bee node.  You can use the swarm-clicommand line tool to monitor your Bee's status, cash out your cheques, upload data to the swarm and more!  ","version":"Next","tagName":"h2"},{"title":"Cashing Out​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#cashing-out","content":" Get your cheques cashed and bank your xBZZ. See this guide to receiving payments from your peers.  ","version":"Next","tagName":"h2"},{"title":"Monitoring and Metrics​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#monitoring-and-metrics","content":" There is a lot going on inside Bee, we provide tools and metrics to help you find out what's going on.  ","version":"Next","tagName":"h2"},{"title":"Backups​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#backups","content":" Keep your important data safe, Bee stores important state and key information on your hardrive, make sure you keep a secure copy in case of disaster.  ","version":"Next","tagName":"h2"},{"title":"Upgrading​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#upgrading","content":" Find out how to keep your Bee up to date with the latest and greatest releases, and make sure you're tuned into our release announcements.  ","version":"Next","tagName":"h2"},{"title":"Uninstalling Bee​","type":1,"pageTitle":"Introduction","url":"/docs/bee/working-with-bee/introduction#uninstalling-bee","content":" We hope you won't need to remove Bee. If you do, please let us know if you had issues so we can help resolve them for our beloved network. Here's the guide to removing Bee from your system. ","version":"Next","tagName":"h2"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/configuration","content":"","keywords":"","version":"Next"},{"title":"Default Data and Config Directories​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#default-data-and-config-directories","content":" Depending on the operating system and startup method used, the default data and configuration directories for your node will differ.  ","version":"Next","tagName":"h2"},{"title":"Bee Service Default Directories​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#bee-service-default-directories","content":" When installed using a package manager, Bee is set up to run as a service with default data and configuration directories set up automatically during the installation. The examples below include default directories for Linux and macOS. You can find the complete details of default directories for different operating systems in the bee.yaml files included in the packaging folder of the Bee repo.  LinuxMacOS arm64 (Apple Silicon)MacOS amd64 (Intel) The default data folder and config file locations: data-dir: /var/lib/bee config: /etc/bee/bee.yaml   ","version":"Next","tagName":"h3"},{"title":"bee start Default Directories​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#bee-start-default-directories","content":" For all operating systems, the default data and config directories for the bee start startup method can be found using the bee printconfig command:  This will print out a complete default Bee node configuration file to the terminal, the config and data-dir values show the default directories for your system:  config: /root/.bee.yaml data-dir: /root/.bee   info The default directories for your system may differ from the example above, so make sure to run the bee printconfig command to view the default directories for your system.  ","version":"Next","tagName":"h3"},{"title":"Configuration Methods and Priority​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#configuration-methods-and-priority","content":" There are three methods of configuration which each have different priority levels. Configuration is processed in the following ascending order of preference:  Command Line ArgumentsEnvironment VariablesYAML Configuration File  info All three methods may be used when running Bee using bee start. However when Bee is started as a service with tools like systemctl or brew services, only the YAML configuration file is supported by default.  ","version":"Next","tagName":"h2"},{"title":"Command Line Arguments​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#command-line-arguments","content":" Run bee start --help in your Terminal to list the available command line arguments as follows:  Start a Swarm node Usage: bee start [flags] Flags: --admin-password string bcrypt hash of the admin password to get the security token --allow-private-cidrs allow to advertise private CIDRs to the public network --api-addr string HTTP API listen address (default &quot;:1633&quot;) --block-time uint chain block time (default 15) --blockchain-rpc-endpoint string rpc blockchain endpoint --bootnode strings initial nodes to connect to --bootnode-mode cause the node to always accept incoming connections --cache-capacity uint cache capacity in chunks, multiply by 4096 to get approximate capacity in bytes (default 1000000) --cache-retrieval enable forwarded content caching (default true) --chequebook-enable enable chequebook (default true) --clef-signer-enable enable clef signer --clef-signer-endpoint string clef signer endpoint --clef-signer-ethereum-address string blockchain address to use from clef signer --cors-allowed-origins strings origins with CORS headers enabled --data-dir string data directory (default &quot;/home/noah/.bee&quot;) --db-block-cache-capacity uint size of block cache of the database in bytes (default 33554432) --db-disable-seeks-compaction disables db compactions triggered by seeks (default true) --db-open-files-limit uint number of open files allowed by database (default 200) --db-write-buffer-size uint size of the database write buffer in bytes (default 33554432) --debug-api-addr string debug HTTP API listen address (default &quot;:1635&quot;) --debug-api-enable enable debug HTTP API --full-node cause the node to start in full mode -h, --help help for start --mainnet triggers connect to main net bootnodes. (default true) --nat-addr string NAT exposed address --neighborhood-suggester string suggester for target neighborhood (default &quot;https://api.swarmscan.io/v1/network/neighborhoods/suggestion&quot;) --network-id uint ID of the Swarm network (default 1) --p2p-addr string P2P listen address (default &quot;:1634&quot;) --p2p-ws-enable enable P2P WebSocket transport --password string password for decrypting keys --password-file string path to a file that contains password for decrypting keys --payment-early-percent int percentage below the peers payment threshold when we initiate settlement (default 50) --payment-threshold string threshold in BZZ where you expect to get paid from your peers (default &quot;13500000&quot;) --payment-tolerance-percent int excess debt above payment threshold in percentages where you disconnect from your peer (default 25) --postage-stamp-address string postage stamp contract address --postage-stamp-start-block uint postage stamp contract start block number --pprof-mutex enable pprof mutex profile --pprof-profile enable pprof block profile --price-oracle-address string price oracle contract address --redistribution-address string redistribution contract address --resolver-options strings ENS compatible API endpoint for a TLD and with contract address, can be repeated, format [tld:][contract-addr@]url --restricted enable permission check on the http APIs --resync forces the node to resync postage contract data --staking-address string staking contract address --statestore-cache-capacity uint lru memory caching capacity in number of statestore entries (default 100000) --static-nodes strings protect nodes from getting kicked out on bootnode --storage-incentives-enable enable storage incentives feature (default true) --swap-deployment-gas-price string gas price in wei to use for deployment and funding --swap-enable enable swap --swap-endpoint string swap blockchain endpoint --swap-factory-address string swap factory addresses --swap-initial-deposit string initial deposit if deploying a new chequebook (default &quot;0&quot;) --target-neighborhood string neighborhood to target in binary format (ex: 111111001) for mining the initial overlay --token-encryption-key string admin username to get the security token --tracing-enable enable tracing --tracing-endpoint string endpoint to send tracing data (default &quot;127.0.0.1:6831&quot;) --tracing-host string host to send tracing data --tracing-port string port to send tracing data --tracing-service-name string service name identifier for tracing (default &quot;bee&quot;) --use-postage-snapshot bootstrap node using postage snapshot from the network --verbosity string log verbosity level 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace (default &quot;info&quot;) --warmup-time duration time to warmup the node before some major protocols can be kicked off (default 5m0s) --welcome-message string send a welcome message string during handshakes --withdrawal-addresses-whitelist strings withdrawal target addresses Global Flags: --config string config file (default is $HOME/.bee.yaml)   ","version":"Next","tagName":"h3"},{"title":"Environment variables​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#environment-variables","content":" Bee config may also be passed using environment variables.  Environment variables are set as variables in your operating system's session or systemd configuration file. To set an environment variable, type the following in your terminal session.  export VARIABLE_NAME=variableValue   Verify if it is correctly set by running echo $VARIABLE_NAME.  All available configuration options are available as BEE prefixed, capitalised, and underscored environment variables, e.g. --api-addr becomes BEE_API_ADDR.  ","version":"Next","tagName":"h3"},{"title":"YAML configuration file​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#yaml-configuration-file","content":" You can view the default contents of the bee.yaml configuration file using the bee printconfig command:  bee printconfig   # bcrypt hash of the admin password to get the security token admin-password: &quot;&quot; # allow to advertise private CIDRs to the public network allow-private-cidrs: false # HTTP API listen address api-addr: 127.0.0.1:1633 # chain block time block-time: &quot;15&quot; # rpc blockchain endpoint blockchain-rpc-endpoint: &quot;&quot; # initial nodes to connect to bootnode: [] # cause the node to always accept incoming connections bootnode-mode: false # cache capacity in chunks, multiply by 4096 to get approximate capacity in bytes cache-capacity: &quot;1000000&quot; # enable forwarded content caching cache-retrieval: true # enable chequebook chequebook-enable: true # enable clef signer clef-signer-enable: false # clef signer endpoint clef-signer-endpoint: &quot;&quot; # blockchain address to use from clef signer clef-signer-ethereum-address: &quot;&quot; # config file (default is $HOME/.bee.yaml) config: /root/.bee.yaml # origins with CORS headers enabled cors-allowed-origins: [] # data directory data-dir: /root/.bee # size of block cache of the database in bytes db-block-cache-capacity: &quot;33554432&quot; # disables db compactions triggered by seeks db-disable-seeks-compaction: true # number of open files allowed by database db-open-files-limit: &quot;200&quot; # size of the database write buffer in bytes db-write-buffer-size: &quot;33554432&quot; # cause the node to start in full mode full-node: false # help for printconfig help: false # triggers connect to main net bootnodes. mainnet: true # NAT exposed address nat-addr: &quot;&quot; # suggester for target neighborhood neighborhood-suggester: https://api.swarmscan.io/v1/network/neighborhoods/suggestion # ID of the Swarm network network-id: &quot;1&quot; # P2P listen address p2p-addr: :1634 # enable P2P WebSocket transport p2p-ws-enable: false # password for decrypting keys password: &quot;&quot; # path to a file that contains password for decrypting keys password-file: &quot;&quot; # percentage below the peers payment threshold when we initiate settlement payment-early-percent: 50 # threshold in BZZ where you expect to get paid from your peers payment-threshold: &quot;13500000&quot; # excess debt above payment threshold in percentages where you disconnect from your peer payment-tolerance-percent: 25 # postage stamp contract address postage-stamp-address: &quot;&quot; # postage stamp contract start block number postage-stamp-start-block: &quot;0&quot; # enable pprof mutex profile pprof-mutex: false # enable pprof block profile pprof-profile: false # price oracle contract address price-oracle-address: &quot;&quot; # redistribution contract address redistribution-address: &quot;&quot; # ENS compatible API endpoint for a TLD and with contract address, can be repeated, format [tld:][contract-addr@]url resolver-options: [] # forces the node to resync postage contract data resync: false # staking contract address staking-address: &quot;&quot; # lru memory caching capacity in number of statestore entries statestore-cache-capacity: &quot;100000&quot; # protect nodes from getting kicked out on bootnode static-nodes: [] # enable storage incentives feature storage-incentives-enable: true # gas price in wei to use for deployment and funding swap-deployment-gas-price: &quot;&quot; # enable swap swap-enable: false # swap blockchain endpoint swap-endpoint: &quot;&quot; # swap factory addresses swap-factory-address: &quot;&quot; # initial deposit if deploying a new chequebook swap-initial-deposit: &quot;0&quot; # neighborhood to target in binary format (ex: 111111001) for mining the initial overlay target-neighborhood: &quot;&quot; # admin username to get the security token token-encryption-key: &quot;&quot; # enable tracing tracing-enable: false # endpoint to send tracing data tracing-endpoint: 127.0.0.1:6831 # host to send tracing data tracing-host: &quot;&quot; # port to send tracing data tracing-port: &quot;&quot; # service name identifier for tracing tracing-service-name: bee # bootstrap node using postage snapshot from the network use-postage-snapshot: false # log verbosity level 0=silent, 1=error, 2=warn, 3=info, 4=debug, 5=trace verbosity: info # time to warmup the node before some major protocols can be kicked off warmup-time: 5m0s # send a welcome message string during handshakes welcome-message: &quot;&quot; # withdrawal target addresses withdrawal-addresses-whitelist: []   info Note that depending on whether Bee is started directly with the bee start command or started as a service with systemctl / brew services, the default directory for the YAML configuration file (shown in the config option above) will be different.  To change your node's configuration, simply edit the YAML file and restart Bee:  LinuxMacOS arm64 (Apple Silicon)MacOS amd64 (Intel) Open the config file for editing: sudo vi /etc/bee/bee.yaml After saving your changes, restart your node: sudo systemctl restart bee   ","version":"Next","tagName":"h3"},{"title":"Manually generating a config file for bee start​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#manually-generating-a-config-file-for-bee-start","content":" No YAML file is generated during installation in the default config directory used when Bee is started with bee start, so you must generate one if you wish to use a YAML file to specify your configuration options. To do this you can use the bee printconfig command to print out a set of default options and save it to a new file in the default location:  bee printconfig &amp;&gt; $HOME/.bee.yaml   ","version":"Next","tagName":"h2"},{"title":"Restoring default config files for Bee service​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#restoring-default-config-files-for-bee-service","content":" You can find the default configurations for your system in the packaging folder of the Bee repo. If your configuration file is missing you can simply copy the contents of the file into a new bee.yaml file in the default configuration directory shown in the bee.yaml file for your system.  ","version":"Next","tagName":"h2"},{"title":"Sepolia Testnet Configuration​","type":1,"pageTitle":"Configuration","url":"/docs/bee/working-with-bee/configuration#sepolia-testnet-configuration","content":" In order to operate a Bee node on the Sepolia testnet, update your configuration to use the options shown in the example below. Make sure that you replace the blockchain-rpc-endpoint option value with your own valid Sepolia RPC endpoint. If you choose to use a 3rd party RPC provider like Infura, make sure to check in the docs that the endpoint format is up to date, and also make sure that you have filled in your own API key which you can find from the Infura web app.  Also make sure to fund your node with Sepolia ETH rather than xDAI to pay for gas on the Sepolia testnet. There are many public faucets you can use to obtain Sepolia ETH, such as this one from Infura.  To get Sepolia BZZ (sBZZ) you can use this Uniswap market, just make sure that you've switched to the Sepolia network in your browser wallet.  data-dir: /home/username/bee/sepolia full-node: true mainnet: false password: password blockchain-rpc-endpoint: wss://sepolia.infura.io/ws/v3/&lt;API-KEY&gt; swap-enable: true verbosity: 5 welcome-message: &quot;welcome-from-the-hive&quot; warmup-time: 30s   Here bootnode was changed to use the testnet bootnode, mainnet has been set to false, network-id has been changed from the default Swarm network id of 1 to the Swarm testnet id of 10, and blockchain-rpc-endpoint has been changed to use an RPC endpoint for the Sepolia testnet rather than Gnosis Chain. ","version":"Next","tagName":"h2"},{"title":"Light Nodes","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/light-nodes","content":"Light Nodes info When a light node is requesting data from the network - it will not benefit from plausible deniability. This is because a light node does not forward on behalf of other nodes, and so it is always the originator of the request. Configuration​ To run Bee as a light node full-node must be set to false and swap-enable must be set to true, and a stable Gnosis Chain RPC endpoint URL must be specified with blockchain-rpc-endpoint in the configuration. Mode of Operation​ At present, light mode represents a pragmatic and elegant approach to improving network stability, reliability and resilience. In general, light mode may be thought of as simply not participating in the activity of forwarding or storing chunks for other members of the swarm, these nodes are strictly consumers, who will pay xBZZ in return for services rendered by full nodes - those contributing towards moving data around the network. This means that although the node will participate in the pull syncing protocol by filling up its local storage with the chunks closest to its overlay address, the node will not serve these chunks to other peers. Additionally, a light node will not participate in the forwarding protocol, as it will not forward chunks to peers closer to the destination address.","keywords":"","version":"Next"},{"title":"Cashing Out","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/cashing-out","content":"","keywords":"","version":"Next"},{"title":"Withdrawing xBZZ Rewards and Native xDAI​","type":1,"pageTitle":"Cashing Out","url":"/docs/bee/working-with-bee/cashing-out#withdrawing-xbzz-rewards-and-native-xdai","content":" You can withdraw xBZZ rewards or native xDAI tokens using the /wallet/withdraw/ endpoint. The endpoint allows you to withdraw tokens to any address which you have whitelisted using the withdrawal-addresses-whitelist option.  You can specify either a single address:  # withdrawal target addresses withdrawal-addresses-whitelist: 0x62d04588e282849d391ebff1b9884cb921b9b94a   Or an array of addresses:  # withdrawal target addresses withdrawal-addresses-whitelist: [ 0x62d04588e282849d391ebff1b9884cb921b9b94a, 0x71a5aae026e2ab87612a5824d492a095e7d790bf ]   The token you desire to withdraw is specified in the path directly:  http://localhost:1633/wallet/withdraw/{coin}   For coin, you can use the value NativeToken for xDAI or BZZ for xBZZ.  The amount query parameter is used to specify how much of the token you wish to withdraw. The value should be specified in the lowest denomination for each token (wei for xDAI and PLUR for xBZZ).  The address query parameter is used to specify the target address to withdraw to. This address must be specified using the withdrawal-addresses-whitelist option in your configuration.  The following command will withdraw a single PLUR of xBZZ to address 0x62d04588e282849d391ebff1b9884cb921b9b94a:  curl -X POST &quot;http://localhost:1633/wallet/withdraw/bzz?amount=1&amp;address=0x62d04588e282849d391ebff1b9884cb921b9b94a&quot;   ","version":"Next","tagName":"h2"},{"title":"Cashing out Cheques (SWAP)​","type":1,"pageTitle":"Cashing Out","url":"/docs/bee/working-with-bee/cashing-out#cashing-out-cheques-swap","content":" As your Bee forwards and serves chunks to its peers, it is rewarded in xBZZ in the form of cheques. Once these cheques accumulate sufficient value, you may cash them out using Bee's API. This process transfers money from your peer's chequebooks into your own, which you can then withdraw to your wallet to do with as you please!  important Do not cash out your cheques too regularly! Once a week is more than sufficient! Besides the transaction costs, this prevents and relieves unnecessary congestion on the blockchain. 💩  info Learn more about how SWAP and other accounting protocols work by readingThe Book of Swarm.  Bee contains a rich set of features to enable you to query the current accounting state of your node. First, let's query our node's current balance by sending a POST request to the balances endpoint.  curl localhost:1633/chequebook/balance | jq   { &quot;totalBalance&quot;: &quot;10000000&quot;, &quot;availableBalance&quot;: &quot;9640360&quot; }   It is also possible to examine your per-peer balances.  curl localhost:1633/balances | jq   { &quot;balances&quot;: [ //... { &quot;peer&quot;: &quot;d0bf001e05014fa036af97f3d226bee253d2b147f540b6c2210947e5b7b409af&quot;, &quot;balance&quot;: &quot;-85420&quot; }, { &quot;peer&quot;: &quot;f1e2872581de18bdc68060dc8edd3aa96368eb341e915aba86b450486b105a47&quot;, &quot;balance&quot;: &quot;-75990&quot; } //... ] }   In Swarm, these per-peer balances represent trustful agreements between nodes. Tokens only actually change hands when a node settles a cheque. This can either be triggered manually or when a certain threshold is reached with a peer. In this case, a settlement takes place. You may view these using the settlements endpoint.  More info can be found by using the chequebook API.  curl localhost:1633/settlements| jq   { &quot;totalreceived&quot;: &quot;718030&quot;, &quot;totalsent&quot;: &quot;0&quot;, &quot;settlements&quot;: [ //... { &quot;peer&quot;: &quot;dce1833609db868e7611145b48224c061ea57fd14e784a278f2469f355292ca6&quot;, &quot;received&quot;: &quot;8987000000000&quot;, &quot;sent&quot;: &quot;0&quot; } //... ] }   More information about the current received or sent cheques can also be found using the chequebook api.  curl localhost:1633/chequebook/cheque | jq   { &quot;lastcheques&quot;: [ { &quot;peer&quot;: &quot;dce1833609db868e7611145b48224c061ea57fd14e784a278f2469f355292ca6&quot;, &quot;lastreceived&quot;: { &quot;beneficiary&quot;: &quot;0x21b26864067deb88e2d5cdca512167815f2910d3&quot;, &quot;chequebook&quot;: &quot;0x4A373Db93ba54cab999e2C757bF5ca0356B42a3f&quot;, &quot;payout&quot;: &quot;8987000000000&quot; }, &quot;lastsent&quot;: null } //... ] }   As our node's participation in the network increases, we will begin to see more and more of these balances arriving. In the case that we have received a settlement from another peer, we can ask our node to perform the relevant transactions on the blockchain, and cash our earnings out.  To do this, we simply POST the relevant peer's address to the cashout endpoint.  curl -XPOST http://localhost:1633/chequebook/cashout/d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b   { &quot;transactionHash&quot;: &quot;0xba7b500e21fc0dc0d7163c13bb5fea235d4eb769d342e9c007f51ab8512a9a82&quot; }   You may check the status of your transaction using the xDAI Blockscout.  Finally, we can now see the status of the cashout transaction by sending a GET request to the same URL.  curl http://localhost:1633/chequebook/cashout/d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b | jq   { &quot;peer&quot;: &quot;d7881307e793e389642ea733451db368c4c9b9e23f188cca659c8674d183a56b&quot;, &quot;chequebook&quot;: &quot;0xae315a9adf0920ba4f3353e2f011031ca701d247&quot;, &quot;cumulativePayout&quot;: &quot;179160&quot;, &quot;beneficiary&quot;: &quot;0x21b26864067deb88e2d5cdca512167815f2910d3&quot;, &quot;transactionHash&quot;: &quot;0xba7b500e21fc0dc0d7163c13bb5fea235d4eb769d342e9c007f51ab8512a9a82&quot;, &quot;result&quot;: { &quot;recipient&quot;: &quot;0x312fe7fde9e0768337c9b3e3462189ea6f9f9066&quot;, &quot;lastPayout&quot;: &quot;179160&quot;, &quot;bounced&quot;: false } }   Success, we earned our first xBZZ! 🐝  Now we have earned tokens, to withdraw our xBZZ from the chequebook contract back into our node's own wallet, we simply POST a request to the chequebook withdraw endpoint.  curl -XPOST http://localhost:1633/chequebook/withdraw\\?amount\\=1000 | jq   And conversely, if we have used more services than we have provided, we may deposit extra xBZZ into the chequebook contract by sending a POST request to the deposit endpoint.  curl -XPOST http://localhost:1633/chequebook/deposit\\?amount\\=1000 | jq   { &quot;transactionHash&quot;: &quot;0xedc80ebc89e6d719e617a50c6900c3dd5dc2f283e1b8c447b9065d7c8280484a&quot; }   You may then use Blockscout to track your transaction and make sure it completed successfully.  ","version":"Next","tagName":"h2"},{"title":"Managing uncashed cheques​","type":1,"pageTitle":"Cashing Out","url":"/docs/bee/working-with-bee/cashing-out#managing-uncashed-cheques","content":" For the Bee process, the final step of earning xBZZ is cashing a cheque. It is worth noting that a cheque is not yet actual xBZZ. In Bee, a cheque, just like a real cheque, is a promise to hand over money upon request. In real life, you would present the cheque to a bank. In swarm life, we present the cheque to a smart-contract.  Holding on to a swap-cheque is risky; it is possible that the owner of the chequebook has issued cheques worth more xBZZ than is contained in their chequebook contract. For this reason, it is important to cash out your cheques every so often.  With the set of API endpoints, as offered by Bee, it is possible to develop a script that fully manages the uncashed cheques for you. As an example, we offer you a very basic script, where you can manually cash out all cheques with a worth above a certain value. To use the script:  Download and save the script:  wget -O cashout.sh https://gist.githubusercontent.com/ralph-pichler/3b5ccd7a5c5cd0500e6428752b37e975/raw/cashout.sh   Make the file executable:  chmod +x cashout.sh   List all uncashed cheques and cash out your cheques above a certain value:  List: ./cashout.sh info If running ./cashout.sh returns nothing, you currently have no uncashed cheques. Cashout all cheques: ./cashout.sh cashout-all   info Are you a Windows-user who is willing to help us? We are currently missing a simple cashout script for Windows. Please see theissue.  info You can find the officially deployed smart-contract by the Swarm team in the swap-swear-and-swindle repository. ","version":"Next","tagName":"h2"},{"title":"Monitoring Your Node","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/monitoring","content":"Monitoring Your Node Your Bee node is equipped with tools to help you understand what your Bee has been up to! Navigate to http://localhost:1633/metrics. This is the current state of Bee's metrics as they stand at this moment. In order to use these metrics and view, we need to keep a record of these metrics over time. To do this we will use Prometheus. Simply install, configure as follows, and restart! For Ubuntu and other Debian based Linux distributions install using apt: sudo apt install prometheus And configure localhost:1633 as a target in the static_configs. sudo vim /etc/prometheus/prometheus.yml static_configs: - targets: [&quot;localhost:9090&quot;, &quot;localhost:1633&quot;] Navigate to http://localhost:9090 to see the Prometheus user interface. Now that our metrics are being scraped into Prometheus' database, we can use it as a data source which is used by Grafana to display the metrics as a time series graph on the dashboard. Type bee_ in the 'expression' or 'metrics' field in Prometheus or Grafana respectively to see the list of metrics available. Here's a few to get you started! rate(bee_swap_cheques_received[1d]) rate(bee_swap_cheques_sent[1d]) rate(bee_swap_cheques_rejected[1d]) Share your creations in the #node-operators channel of our Discord server!","keywords":"","version":"Next"},{"title":"Logs and Files","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/logs-and-files","content":"","keywords":"","version":"Next"},{"title":"Linux​","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#linux","content":" If you have installed Bee on Linux using a package manager you will now be able to manage your Bee service using systemctl.  systemctl status bee   ● bee.service - Bee - Ethereum Swarm node Loaded: loaded (/lib/systemd/system/bee.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2020-11-20 23:50:15 GMT; 6s ago   Logs are available using the journalctl command:  journalctl --lines=100 --follow --unit bee   INFO[2021-02-09T18:55:11Z] swarm public key 03379f7aa673b7f03737064fd23ba1453619924a4602e70bbccc133ba67d0968bd DEBU[2021-02-09T18:55:11Z] using existing libp2p key DEBU[2021-02-09T18:55:11Z] using existing pss key INFO[2021-02-09T18:55:11Z] pss public key 03bae655ce94431e1f2c2de8d017f88c8c5c293ef0057379223084aba9e318596e INFO[2021-02-09T18:55:11Z] using ethereum address 99c9e7868d22244106a5ffbc2f5d6b7c88e2c85a INFO[2021-02-09T18:55:14Z] using default factory address for chain id 5: f0277caffea72734853b834afc9892461ea18474 INFO[2021-02-09T18:55:14Z] no chequebook found, deploying new one. WARN[2021-02-09T18:55:15Z] cannot continue until there is sufficient ETH (for Gas) and at least 10 BZZ available on 99c9e7868d22244106a5ffbc2f5d6b7c88e2c85a   ","version":"Next","tagName":"h3"},{"title":"MacOS​","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#macos","content":" Services are managed using Homebrew services.  brew services restart swarm-bee   Logs are available at /usr/local/var/log/swarm-bee/bee.log  tail -f /usr/local/var/log/swarm-bee/bee.log   ","version":"Next","tagName":"h3"},{"title":"Data Locations​","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#data-locations","content":" ","version":"Next","tagName":"h2"},{"title":"Bee​","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#bee","content":" Configuration files are stored in /etc/bee/. State, chunks and other data are stored in /var/lib/bee/  ","version":"Next","tagName":"h3"},{"title":"Logging Guidelines​","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#logging-guidelines","content":" The Bee logs provide a robust information on the workings of a Bee node which are useful both to node operators and to Bee developers. Log messages have four levels described below, and logs can be adjusted for verbosity and granularity to suit the needs of the user.  ","version":"Next","tagName":"h2"},{"title":"Log Levels​","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#log-levels","content":" The log messages are divided into four basic levels:  Error - Errors in the node. Although the node operation may continue, the error indicates that it should be addressed.Warning - Warnings should be checked in case the problem recurs to avoid potential damage.Info - Informational messages useful for node operators that do not indicate any fault or error.Debug - Information concerning program logic decisions, diagnostic information, internal state, etc. which is primarily useful for developers.  There is a notion of V-level attached to the Debug level. V-levels provide a simple way of changing the verbosity of debug messages. V-levels provide a way for a given package to distinguish the relative importance or verbosity of a given log message. Then, if a particular logger or package logs too many messages, the package can simply change the V level for that logger.  ","version":"Next","tagName":"h3"},{"title":"Logging API usage​","type":1,"pageTitle":"Logs and Files","url":"/docs/bee/working-with-bee/logs-and-files#logging-api-usage","content":" In the current Bee code base, it is possible to change the granularity of logging for some services on the fly without the need to restart the node. These services and their corresponding loggers can be found using the /loggers endpoint. Example of the output:  { &quot;tree&quot;: { &quot;node&quot;: { &quot;/&quot;: { &quot;api&quot;: { &quot;+&quot;: [ &quot;info|node/api[0][]&gt;&gt;824634933256&quot; ] }, &quot;batchstore&quot;: { &quot;+&quot;: [ &quot;info|node/batchstore[0][]&gt;&gt;824634933256&quot; ] }, &quot;leveldb&quot;: { &quot;+&quot;: [ &quot;info|node/leveldb[0][]&gt;&gt;824634933256&quot; ] }, &quot;pseudosettle&quot;: { &quot;+&quot;: [ &quot;info|node/pseudosettle[0][]&gt;&gt;824634933256&quot; ] }, &quot;pss&quot;: { &quot;+&quot;: [ &quot;info|node/pss[0][]&gt;&gt;824634933256&quot; ] }, &quot;storer&quot;: { &quot;+&quot;: [ &quot;info|node/storer[0][]&gt;&gt;824634933256&quot; ] } }, &quot;+&quot;: [ &quot;info|node[0][]&gt;&gt;824634933256&quot; ] } }, &quot;loggers&quot;: [ { &quot;logger&quot;: &quot;node/api&quot;, &quot;verbosity&quot;: &quot;info&quot;, &quot;subsystem&quot;: &quot;node/api[0][]&gt;&gt;824634933256&quot;, &quot;id&quot;: &quot;bm9kZS9hcGlbMF1bXT4-ODI0NjM0OTMzMjU2&quot; }, { &quot;logger&quot;: &quot;node/storer&quot;, &quot;verbosity&quot;: &quot;info&quot;, &quot;subsystem&quot;: &quot;node/storer[0][]&gt;&gt;824634933256&quot;, &quot;id&quot;: &quot;bm9kZS9zdG9yZXJbMF1bXT4-ODI0NjM0OTMzMjU2&quot; }, { &quot;logger&quot;: &quot;node/pss&quot;, &quot;verbosity&quot;: &quot;info&quot;, &quot;subsystem&quot;: &quot;node/pss[0][]&gt;&gt;824634933256&quot;, &quot;id&quot;: &quot;bm9kZS9wc3NbMF1bXT4-ODI0NjM0OTMzMjU2&quot; }, { &quot;logger&quot;: &quot;node/pseudosettle&quot;, &quot;verbosity&quot;: &quot;info&quot;, &quot;subsystem&quot;: &quot;node/pseudosettle[0][]&gt;&gt;824634933256&quot;, &quot;id&quot;: &quot;bm9kZS9wc2V1ZG9zZXR0bGVbMF1bXT4-ODI0NjM0OTMzMjU2&quot; }, { &quot;logger&quot;: &quot;node&quot;, &quot;verbosity&quot;: &quot;info&quot;, &quot;subsystem&quot;: &quot;node[0][]&gt;&gt;824634933256&quot;, &quot;id&quot;: &quot;bm9kZVswXVtdPj44MjQ2MzQ5MzMyNTY=&quot; }, { &quot;logger&quot;: &quot;node/leveldb&quot;, &quot;verbosity&quot;: &quot;info&quot;, &quot;subsystem&quot;: &quot;node/leveldb[0][]&gt;&gt;824634933256&quot;, &quot;id&quot;: &quot;bm9kZS9sZXZlbGRiWzBdW10-PjgyNDYzNDkzMzI1Ng==&quot; }, { &quot;logger&quot;: &quot;node/batchstore&quot;, &quot;verbosity&quot;: &quot;info&quot;, &quot;subsystem&quot;: &quot;node/batchstore[0][]&gt;&gt;824634933256&quot;, &quot;id&quot;: &quot;bm9kZS9iYXRjaHN0b3JlWzBdW10-PjgyNDYzNDkzMzI1Ng==&quot; } ] }   The recorders come in two versions. The first is the tree version and the second is the flattened version. The subsystem field is the unique identifier of the logger. The id field is a version of the subsystem field encoded in base 64 for easier reference to a particular logger. The node name of the version tree is composed of the subsystem with the log level prefix and delimited by the | character. The number in the first square bracket indicates the logger's V-level.  The logger endpoint uses HTTP PUT requests to modify the verbosity of the logger(s). The request must have the following parameters /loggers/{subsystem}/{verbosity}. The {subsytem} parameter is the base64 version of the subsytem field or regular expression corresponding to multiple subsystems. Since the loggers are arranged in tree structure, it is possible to turn on/off or change the logging level of the entire tree or just its branches with a single command. The verbosity can be one of none, error, warning, info, debug or a number in the range 1 to 1&lt;&lt;&lt;31 - 1 to enable the verbosity of a particular V-level, if available for a given logger. A value of all will enable the highest verbosity of V-level.  Examples:  curl -XPUT http://localhost:1633/loggers/bm9kZS8q/none - will disable all loggers; bm9kZS8q is base64 encoded node/* regular expression.  curl -XPUT http://localhost:1633/loggers/bm9kZS9hcGlbMV1bXT4-ODI0NjM0OTMzMjU2/error - will set the verbosity of the logger with the subsystem node/api[1][]&gt;&gt;824634933256 to error. ","version":"Next","tagName":"h3"},{"title":"Ultra Light Nodes","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/ultra-light-nodes","content":"Ultra Light Nodes info When running without a blockchain connection, bandwidth incentive payments (SWAP) cannot be made so there is a risk of getting blocklisted by other peers for unpaid services. Configuration​ In order to run an ultra-light node use the same configuration as for the light node but leave the blockchain-rpc-endpoint parameter value to empty (or just comment it out). caution Make sure you set the swap-enable configuration parameter to false, otherwise you will get an error. Mode of Operation​ The target audience for this mode of operations are users who want to try out running a node but don't want to go through the hassle of blockchain onboarding. Ultra-light nodes will be able to download data as long as the data consumed does not exceed the payment threshold (payment-threshold in configuration) set by peers they connect to. Running Bee without a connected blockchain backend, however, imposes some limitations: Can't do overlay verificationCan't do SWAP settlements Since we can't buy postage stamps: Can't send PSS messagesCan't upload data to the network","keywords":"","version":"Next"},{"title":"Swarm CLI","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/swarm-cli","content":"Swarm CLI Swarm CLI is a command line based application which simplifies interactions with the Bee API. It allows you to accomplish most common tasks using easy to understand commands. It also greatly simplifies certain more complex tasks, such as as the management of feeds. For installation and usage instructions, see the README. To check the latest version, see the Swarm CLI releases page. For further support and information, join the Swarm Discord server.","keywords":"","version":"Next"},{"title":"Uninstalling Bee","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/uninstalling-bee","content":"","keywords":"","version":"Next"},{"title":"Uninstalling Bee​","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#uninstalling-bee","content":" Choose the appropriate uninstall method based on the install method used:  ","version":"Next","tagName":"h2"},{"title":"Package Manager Install​","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#package-manager-install","content":" This method can be used for package manager based installs of the official Debian, RPM, and Homebrew packages.  danger This will remove your keyfiles so make certain that you have a full backup of your keys and configuration before uninstalling.  Debian​  To uninstall Bee and completely remove all associated files including keys and configuration, run:  sudo apt-get purge bee   RPM​  sudo yum remove bee   ","version":"Next","tagName":"h3"},{"title":"Binary Install​","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#binary-install","content":" If Bee is installed using the automated shell script or by building from source, Bee can be uninstalled by directly removing the installed file.  sudo rm `/usr/local/bin/bee`   ","version":"Next","tagName":"h3"},{"title":"Remove Bee Data Files​","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#remove-bee-data-files","content":" To completely remove all Bee files from your system you will also need to remove the config and data files.  danger Node keys, password, chunks and state files are stored in the data folder. Make backups of your data folder to prevent losing keys and data.  ","version":"Next","tagName":"h2"},{"title":"Bee​","type":1,"pageTitle":"Uninstalling Bee","url":"/docs/bee/working-with-bee/uninstalling-bee#bee","content":" Config folder: Configuration file is stored in /etc/bee/  Data folder: State, keys, chunks, and other data are stored in /var/lib/bee/ ","version":"Next","tagName":"h3"},{"title":"Upgrading Bee","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/upgrading-bee","content":"","keywords":"","version":"Next"},{"title":"Upgrade Procedure Warnings​","type":1,"pageTitle":"Upgrading Bee","url":"/docs/bee/working-with-bee/upgrading-bee#upgrade-procedure-warnings","content":" warning Bee sure to back up your keys and cash out your cheques to ensure your xBZZ is safe before applying updates.  warning Nodes should not be shut down or updated in the middle of a round they are playing in as it may cause them to lose out on winnings or become frozen. To see if your node is playing the current round, check if lastPlayedRound equals round in the output from the /redistributionstate endpoint. See staking section for more information on staking and troubleshooting.  ","version":"Next","tagName":"h2"},{"title":"Ubuntu / Debian​","type":1,"pageTitle":"Upgrading Bee","url":"/docs/bee/working-with-bee/upgrading-bee#ubuntu--debian","content":" To upgrade Bee, first stop the Bee service:  sudo systemctl stop bee   Next, upgrade the bee package:  sudo apt-get update sudo apt-get upgrade bee   And will see output like this after a successful upgrade:  Reading package lists... Done Building dependency tree Reading state information... Done Calculating upgrade... Done The following packages will be upgraded: bee 1 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. Need to get 0 B/27.2 MB of archives. After this operation, 73.7 kB of additional disk space will be used. Do you want to continue? [Y/n] Y (Reading database ... 103686 files and directories currently installed.) Preparing to unpack .../archives/bee_2.0.0_amd64.deb ... Unpacking bee (2.0.0) over (1.17.3) ... Setting up bee (2.0.0) ... Installing new version of config file /etc/default/bee ...   Make sure to pay attention to any prompts, read them carefully, and respond to them with your preference.  You may now start your node again:  sudo systemctl start bee   Manual Installations​  To upgrade your manual installation, simply stop Bee, replace the Bee binary and restart.  Docker​  To upgrade your Docker installation, simply increment the version number in your configuration and restart. ","version":"Next","tagName":"h3"},{"title":"Access Content","type":0,"sectionRef":"#","url":"/docs/desktop/access-content","content":"Access Content Accessing content on Swarm using Swarm Desktop is easy. All you need to get started is the Swarm hash for the content you wish to access. Whenever content is uploaded to Swarm a Swarm hash is generated as a reference to that content. To access content on Swarm go to the Files tab and click Download: From there, paste the Swarm hash for the content you want to access, and click Find. We'll use the hash for a Swarm blog post explaining how to upload a website to Swarm: 6843d3be17364ea0620011430e4db2a26ff781da478493a02d6eb5aae886b8ae On the following screen you will see the data associated with the Swarm hash and see options for downloading (or browsing if it is a hash for a website): Click View Website to see the site in your browser, or Download to download the files:","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/docs/desktop/configuration","content":"","keywords":"","version":"Next"},{"title":"Setting RPC Endpoint​","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#setting-rpc-endpoint","content":" In order to interact with the Gnosis Chain to buy stamps, participate in staking, and manage assets such as xBZZ, Bee nodes require a valid Gnosis Chain RPC endpoint. By default the RPC endpoint is set to https://xdai.fairdatasociety.org, however any valid Gnosis Chain RPC endpoint may be used.  To modify the RPC endpoint, first navigate to the Settings tab:    From the Settings tab, expand the API Settings section and click the pen button next to Blockchain RPC URL to edit the default RPC. You can choose any valid Gnosis Chain RPC, either from your own Gnosis node or a service provider. You can find a list of paid and free RPC options from the Gnosis Chain docs. For this example we will use the free endpoint - https://rpc.gnosischain.com/.    Click Save and Restart to finish changing the RPC endpoint.  ","version":"Next","tagName":"h2"},{"title":"Upgrading from an Ultra-light to a Light Node​","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#upgrading-from-an-ultra-light-to-a-light-node","content":" Bee ultra-light nodes are limited to only downloading small amounts of data from Swarm. In order to download greater amounts of data or to upload data to Swarm you must upgrade to a light node. To do this we need to first fund our Swarm Desktop Bee node with some xDAI (DAI bridged from Ethereum to Gnosis Chain which serves as Gnosis Chain's native token for paying transaction fees) in order to pay for the Gnosis Chain transactions required for setting up a light node.  ","version":"Next","tagName":"h2"},{"title":"Bridging Ethereum DAI to Gnosis Chain as xDAI​","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#bridging-ethereum-dai-to-gnosis-chain-as-xdai","content":" If you already have some xDAI on a Gnosis Chain address, skip to the next step Funding Node with xDAI. If you have DAI on Ethereum and need to swap it for xDAI, you can use one of the Gnosis Chain Bridge  Five to ten xDAI is plenty to get started.  ","version":"Next","tagName":"h3"},{"title":"Funding Node with xDAI​","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#funding-node-with-xdai","content":" Once you have a few xDAI in your Gnosis Chain address, to fund your Bee node you need to send it from your wallet to your Swarm Desktop wallet. You can find your address from the Account tab of the app.    Next simply send your xDAI to that address. Before sending, make sure you have set your wallet to use the Gnosis Chain network and not the Ethereum mainnet. If Gnosis Chain is not included as default selectable network in your wallet, you may need to add the network manually. You can use this configuration to add Gnosis Chain:  Field\tValueNetwork name:\tGnosis New RPC URL:\thttps://rpc.gnosischain.com Chain ID:\t100 Symbol:\txDai Block Explorer URL (Optional):\thttps://blockscout.com/xdai/mainnet    The transaction should be confirmed in under a minute. We can check on the Account page to see when the xDAI has been received:    ","version":"Next","tagName":"h3"},{"title":"Set Up Wallet​","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#set-up-wallet","content":" Now with some xDAI in the Swarm Desktop wallet, we can upgrade our Bee node from ultra-light to a light node. Completing the setup process will swap xDAI for some xBZZ at the current price, and will issue the transactions needed to set up the chequebook contract.  To get started, navigate to the Info tab and click the Setup wallet button.Click Use xDAI.Confirm that you have sufficient xDAI balance and click Proceed.Click Swap Now and Upgrade.Wait for the upgrade to complete.After the upgrade is complete, you will see several new sections within the Account tab: Chequebook, Stamps, and Feeds.  ","version":"Next","tagName":"h3"},{"title":"Fund Chequebook​","type":1,"pageTitle":"Configuration","url":"/docs/desktop/configuration#fund-chequebook","content":" After setting up your wallet you will have access to the Chequebook section from the Accounts tab. From here you can manage your chequebook for your Swarm Desktop Bee node. ","version":"Next","tagName":"h2"},{"title":"Install","type":0,"sectionRef":"#","url":"/docs/desktop/install","content":"","keywords":"","version":"Next"},{"title":"Download and Install Swarm Desktop​","type":1,"pageTitle":"Install","url":"/docs/desktop/install#download-and-install-swarm-desktop","content":" Installing the Swarm Desktop app takes only a few clicks. To get started, simply download and install the Swarm Desktop app for your operating system. Installers are available for Windows, Linux, and OSX. You can find download links for Swarm Desktop at the Swarm homepage and you can find installers for specific operating systems at the releases page of the Swarm Desktop GitHub repo.  caution Swarm Desktop is in Beta and currently includes the Sentry application monitoring and bug reporting software which automatically collects data in order to help improve the software.  caution This project is in beta state. There might (and most probably will) be changes in the future to its API and working. Also, no guarantees can be made about its stability, efficiency, and security at this stage.  Ethswarm.org Swarm Desktop Page  Swarm Desktop GitHub Releases Page  After running the installer, a window will pop up and display the installation status:    Once the installation is complete, Swarm Desktop will open up in your default browser in a new window to the &quot;Info&quot; tab of the app:    If the installation went smoothly, you should see the message &quot;Your node is connected&quot; above the &quot;Access Content&quot; button along with a status message of &quot;Node OK&quot;.  What Just Happened?​  Running the Swarm Desktop app for the first time set up a new Bee node on your system. The installation process generated and saved private keys for your node in the Swarm Desktop's data directory. Those keys were used to start up a new Bee node in ultra-light mode.  warning If your Swarm Desktop files are accidentally deleted or become corrupted you will lose access to any assets or data which are secured using those keys. Make sure to backup your keys.  ","version":"Next","tagName":"h2"},{"title":"\"Ultra-light\" and \"Light\" Nodes​","type":1,"pageTitle":"Install","url":"/docs/desktop/install#ultra-light-and-light-nodes","content":" Swarm Desktop by default starts up a node in &quot;ultra-light&quot; mode. When running in ultra-light mode Swarm Desktop limited to only downloading data from Swarm. Moreover, it's limited to downloading only within the free threshold allowed by other nodes. For instructions on switching to light mode see the configuration section.  ","version":"Next","tagName":"h3"},{"title":"Tour of Swarm Desktop​","type":1,"pageTitle":"Install","url":"/docs/desktop/install#tour-of-swarm-desktop","content":" ","version":"Next","tagName":"h2"},{"title":"Info Tab​","type":1,"pageTitle":"Install","url":"/docs/desktop/install#info-tab","content":" The &quot;Info&quot; tab gives you a quick view of your Swarm Desktop's status. From here we can quickly see if the node is connected to Swarm, whether the node is funded, and whether its chequebook contract is set up. On a new install of Swarm Desktop, the node should be connected, but the wallet and chequebook will not have been set up yet.    ","version":"Next","tagName":"h3"},{"title":"Files Tab​","type":1,"pageTitle":"Install","url":"/docs/desktop/install#files-tab","content":" From &quot;Files&quot; tab you can input a Swarm hash in order to download the file associated with the hash. See this full guide for downloading using Swarm Desktop.    ","version":"Next","tagName":"h3"},{"title":"Account Tab​","type":1,"pageTitle":"Install","url":"/docs/desktop/install#account-tab","content":" From the &quot;Account&quot; tab you can view your Swarm Desktop node's Gnosis Chain address and associated xBZZ and xDAI balances.    ","version":"Next","tagName":"h3"},{"title":"Settings Tab​","type":1,"pageTitle":"Install","url":"/docs/desktop/install#settings-tab","content":" From the &quot;Settings&quot; tab you can view important settings values. Note that the Blockchain RPC URL and ENS resolver URL are already filled in, and only the Blockchain RPC URL is modifiable through this tab. If you wish to modify other settings see the configuration page for detailed instructions.    ","version":"Next","tagName":"h3"},{"title":"Status Tab​","type":1,"pageTitle":"Install","url":"/docs/desktop/install#status-tab","content":" From the &quot;Status&quot; tab you can see a quick overview of the health of your Swarm Desktop's Bee node.   ","version":"Next","tagName":"h3"},{"title":"Backup and Restore","type":0,"sectionRef":"#","url":"/docs/desktop/backup-restore","content":"","keywords":"","version":"Next"},{"title":"Create a Backup​","type":1,"pageTitle":"Backup and Restore","url":"/docs/desktop/backup-restore#create-a-backup","content":" To create a backup of your Bee node in Swarm Desktop, start by shutting down your node.  Right click the Bee icon in the System tray and select Stop Bee and then Quit to close and exit from Swarm Desktop:    Next navigate to the Settings tab in the app and copy the location of the data directory as indicated in the Data DIR field:    Navigate to the directory you just copied and create copies of all the files in that directory (\\data-dir), including localstore, statestore, stamperstore, kademlia-metrics and keys folders and store them in a secure and private location.    In addition to the data folders, you will also need the password found in the config.yaml file in order to restore a Bee node from backup. Move up one directory from Data DIR to the Data directory, and create a copy of the config.yaml file and save it along with the other folders you just backed up:    Alternatively you may open the config.yaml and save the password as a text file along with the rest of your backup files:    Your completed backup should contain all the files from your data directory as well as your password (either in your config.yaml file or as a separate file or written down.)    ","version":"Next","tagName":"h2"},{"title":"Back-up Gnosis Chain Key Only​","type":1,"pageTitle":"Backup and Restore","url":"/docs/desktop/backup-restore#back-up-gnosis-chain-key-only","content":" If you only wish to back-up your Gnosis Chain key, navigate to the \\data-dir\\keys directory, and copy the swarm.key to a safe location:    You also need the password found in the config.yaml file in order to access your Gnosis Chain account. Move up one directory from Data DIR to the Data directory, and create a copy of the config.yaml file and save it along with the other folders you just backed up:    Alternatively you may open the config.yaml and save the password as a text file along with the rest of your backup files:    ","version":"Next","tagName":"h3"},{"title":"Restore from Backup​","type":1,"pageTitle":"Backup and Restore","url":"/docs/desktop/backup-restore#restore-from-backup","content":" To restore from backup, begin with a new install of Swarm Desktop. Once the installation process is finished, navigate to the Settings tab in the app and copy the install file directory as indicated in the Data DIR field:    Before navigating to the directory you just copied, right click the Bee icon in the System tray and select Stop Bee and then Quit to close and exit from Swarm Desktop:    Next open your file explorer and navigate to the directory you just copied. Delete any files present in the directory, and replace them with your own backup copies (excluding the config.yaml / password file):    Move up one directory from Data DIR to Data, and replace delete the config.yaml file and replace it with the config.yaml file from your backup.  Alternatively if you have saved just the password and not the entire config file, open the default config.yaml file in a text editor such as VS Code or a plain text editor:      Replace the password string with your own password which you saved from the config.yaml backup.  Restart Swarm Desktop and check to see if the backup was restored successfully:    ","version":"Next","tagName":"h2"},{"title":"Restore Gnosis Chain Account​","type":1,"pageTitle":"Backup and Restore","url":"/docs/desktop/backup-restore#restore-gnosis-chain-account","content":" If you only wish to access your Gnosis Chain account, you can follow these instructions for exporting to Metamask in order to access your account. ","version":"Next","tagName":"h3"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/desktop/introduction","content":"Introduction While running Bee from the terminal is a powerful and flexible approach for developers and node operators, it may not be the best option for more simple use cases. The Swarm Desktop app is an alternative which provides an easy-to-use graphical user interface for running a Bee node and interacting seamlessly with the Swarm network. The Swarm Desktop App was designed to simplify the Swarm onboarding process so that anyone can benefit from decentralized storage while maintaining privacy and control over their data. Available for Windows, Mac, and Linux operating systems, the Swarm Desktop App serves as a personal gateway to the Swarm network.","keywords":"","version":"Next"},{"title":"Staking","type":0,"sectionRef":"#","url":"/docs/bee/working-with-bee/staking","content":"","keywords":"","version":"Next"},{"title":"Add stake​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#add-stake","content":" Bee has builtin endpoints for depositing the stake. Currently the minimum staking requirement is 10 xBZZ, so make sure that there is enough tokens in the node's wallet and you must have some native token as well for paying the gas.  Then you can run the following command to stake 10 xBZZ. The amount is given in PLUR which is the smallest denomination of xBZZ and 1 xBZZ == 1e16 PLUR.  curl -XPOST localhost:1633/stake/100000000000000000   If the command executed successfully it returns a transaction hash that you can use to verify on a block explorer.  It is possible to deposit more by repeatedly using the command above.  You can also check the amount staked with the following command:  curl localhost:1633/stake   ","version":"Next","tagName":"h2"},{"title":"Check redistribution status​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#check-redistribution-status","content":" Use the RedistributionState endpoint of the API to get more information about the redistribution status of the node.  curl -X GET http://localhost:1633/redistributionstate | jq   { &quot;minimumFunds&quot;: &quot;18750000000000000&quot;, &quot;hasSufficientFunds&quot;: true, &quot;isFrozen&quot;: false, &quot;isFullySynced&quot;: true, &quot;phase&quot;: &quot;commit&quot;, &quot;round&quot;: 176319, &quot;lastWonRound&quot;: 176024, &quot;lastPlayedRound&quot;: 176182, &quot;lastFrozenRound&quot;: 0, &quot;block&quot;: 26800488, &quot;reward&quot;: &quot;10479124611072000&quot;, &quot;fees&quot;: &quot;30166618102500000&quot; }   &quot;minimumFunds&quot;: &lt;integer&gt; - The minimum xDAI needed to play a single round of the redistribution game (the unit is 1e-18 xDAI).&quot;hasSufficientFunds&quot;: &lt;bool&gt; - Shows whether the node has enough xDAI balance to submit at least five storage incentives redistribution related transactions. If false the node will not be permitted to participate in next round.&quot;isFrozen&quot;: &lt;bool&gt; - Shows node frozen status.&quot;isFullySynced&quot;: &lt;bool&gt; - Shows whether node's localstore has completed full historical syncing with all connected peers.&quot;phase&quot;: &lt;string&gt; - Current phase of redistribution game (commit, reveal, or claim).&quot;round&quot;: &lt;integer&gt; - Current round of redistribution game. The round number is determined by dividing the current Gnosis Chain block height by the number of blocks in one round. One round takes 152 blocks, so using the &quot;block&quot; output from the example above we can confirm that the round number is 176319 (block 26800488 / 152 blocks = round 176319).&quot;lastWonRound&quot;: &lt;integer&gt; - Number of round last won by this node.&quot;lastPlayedRound&quot;: &lt;integer&gt; - Number of the last round where node's neighborhood was selected to participate in redistribution game.&quot;lastFrozenRound&quot;: &lt;integer&gt; The number the round when node was last frozen.&quot;block&quot;: &lt;integer&gt; - Gnosis block of the current redistribution game.&quot;reward&quot;: &lt;string (BigInt)&gt; - Record of total reward received in PLUR.&quot;fees&quot;: &lt;string (BigInt)&gt; - Record of total spent in 1E-18 xDAI on all redistribution related transactions.  warning Nodes should not be shut down or updated in the middle of a round they are playing in as it may cause them to lose out on winnings or become frozen. To see if your node is playing the current round, check if lastPlayedRound equals round in the output from the /redistributionstate endpoint.  info If your node is not operating properly such as getting frozen or not participating in any rounds, see the troubleshooting section.  ","version":"Next","tagName":"h2"},{"title":"Maximize rewards​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#maximize-rewards","content":" There are two main factors which determine the chances for a staking node to win a reward — neighborhood selection and stake density. Both of these should be considered together before starting up a Bee node for the first time. See the incentives page for more context.  ","version":"Next","tagName":"h2"},{"title":"Neighborhood selection​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#neighborhood-selection","content":" By default when running a Bee node for the first time an overlay address will be generated and used to assign the node to a random neighborhood. However, by using the target-neighborhood config option, a specific neighborhood can be selected in which to generate the node's overlay address. This is an excellent tool for maximizing reward chances as generally speaking running in a less populated neighborhood will increase the chances of winning a reward. See the config section on the installation page for more information on how to set a target neighborhood.  ","version":"Next","tagName":"h3"},{"title":"Stake density​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#stake-density","content":" Stake density is defined as:  stake density=staked xBZZ×2storageDepth\\text{stake density} = \\text{staked xBZZ} \\times {2}^\\text{storageDepth}stake density=staked xBZZ×2storageDepth  To learn more about stake density and the mechanics of the incentives system, see the incentives page.  Stake density determines the weighted chances of nodes within a neighborhood of winning rewards. The chance of winning within a neighborhood corresponds to stake density. Stake density can be increased by depositing more xBZZ as stake (note that stake withdrawals are not currently possible, so any staked xBZZ is not currently recoverable).  Generally speaking, the minimum required stake of 10 xBZZ is sufficient, and rewards can be better maximized by operating more nodes over a greater range of neighborhoods rather than increasing stake. However this may not be true for all node operators depending on how many different neighborhoods they operate nodes in, and it also may change as network dynamics continue to evolve (join the #node-operators Discord channel to stay up to date with the latest discussions about staking and network dynamics).  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#troubleshooting","content":" In this section we cover several commonly seen issues encountered for staking nodes participating in the redistribution game. If you don't see your issue covered here or require additional guidance, check out the #node-operators Discord channel where you will find support from other node operators and community members.  ","version":"Next","tagName":"h2"},{"title":"Frozen node​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#frozen-node","content":" A node will be frozen when the reserve commitment hash it submits in its commit transaction does not match the correct hash. The reserve commitment hash is used as proof that a node is storing the chunks it is responsible for. It will not be able to play in the redistribution game during the freezing period. See the penalties section for more information.  Check frozen status​  You can check your node's frozen status using the /redistributionstate endpoint:  curl -X GET http://localhost:1633/redistributionstate | jq   { &quot;minimumFunds&quot;: &quot;18750000000000000&quot;, &quot;hasSufficientFunds&quot;: true, &quot;isFrozen&quot;: false, &quot;isFullySynced&quot;: true, &quot;phase&quot;: &quot;commit&quot;, &quot;round&quot;: 176319, &quot;lastWonRound&quot;: 176024, &quot;lastPlayedRound&quot;: 176182, &quot;lastFrozenRound&quot;: 0, &quot;block&quot;: 26800488, &quot;reward&quot;: &quot;10479124611072000&quot;, &quot;fees&quot;: &quot;30166618102500000&quot; }   The relevant fields here are isFrozen and lastFrozenRound, which respectively indicate whether the node is currently frozen and the last round in which the node was frozen.  Diagnosing freezing issues​  In order to diagnose the cause of freezing issues we must compare our own node's status to that of other nodes within the same neighborhood by comparing the results from our own node returned from the /status endpoint to the other nodes in the same neighborhood which can be found from the /status/peers endpoint.  First we check our own node's status:   curl -s localhost:1633/status | jq    { &quot;peer&quot;: &quot;da7e5cc3ed9a46b6e7491d3bf738535d98112641380cbed2e9ddfe4cf4fc01c4&quot;, &quot;proximity&quot;: 0, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3747532, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 183, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }   And next we will find the status for all the other nodes in the same neighborhood as our own.   curl -s localhost:1633/status/peers | jq   The /status/peers endpoint returns all the peers of our node, but we are only concerned with peers in the same neighborhood as our own node. Nodes whose proximity value is equal to or greater than our own node's storageRadius value all fall into the same neighborhood as our node, so the rest have been omitted in the example output below:  { ... { &quot;peer&quot;: &quot;da33f7a504a74094242d3e542475b49847d1d0f375e0c86bac1c9d7f0937acc0&quot;, &quot;proximity&quot;: 9, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3782924, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 188, &quot;neighborhoodSize&quot;: 11, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4b529cc1aedc62e31849cf7f8ab8c1866d9d86038b857d6cf2f590604387fe&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3719593, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 176, &quot;neighborhoodSize&quot;: 11, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da5d39a5508fadf66c8665d5e51617f0e9e5fd501e429c38471b861f104c1504&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3777241, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 198, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4cb0d125bba638def55c0061b00d7c01ed4033fa193d6e53a67183c5488d73&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3849125, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 181, &quot;neighborhoodSize&quot;: 13, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4b1cd5d15e061fdd474003b5602ab1cff939b4b9e30d60f8ff693141ede810&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3778452, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 183, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133827002368, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da49e6c6174e3410edad2e0f05d704bbc33e9996bc0ead310d55372677316593&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3779560, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 185, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4cdab480f323d5791d3ab8d22d99147f110841e44a8991a169f0ab1f47d8e5&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3778518, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 189, &quot;neighborhoodSize&quot;: 11, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da4ccec79bc34b502c802415b0008c4cee161faf3cee0f572bb019b117c89b2f&quot;, &quot;proximity&quot;: 10, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3779003, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 179, &quot;neighborhoodSize&quot;: 10, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da69d412b79358f84b7928d2f6b7ccdaf165a21313608e16edd317a5355ba250&quot;, &quot;proximity&quot;: 11, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3712586, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 189, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133827002368, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da61967b1bd614a69e5e83f73cc98a63a70ebe20454ca9aafea6b57493e00a34&quot;, &quot;proximity&quot;: 11, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3780190, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 182, &quot;neighborhoodSize&quot;: 13, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da7b6a268637cfd6799a9923129347fc3d564496ea79aea119e89c09c5d9efed&quot;, &quot;proximity&quot;: 13, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3721494, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 188, &quot;neighborhoodSize&quot;: 14, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true }, { &quot;peer&quot;: &quot;da7a974149543df1b459831286b42b302f22393a20e9b3dd9a7bb5a7aa5af263&quot;, &quot;proximity&quot;: 13, &quot;beeMode&quot;: &quot;full&quot;, &quot;reserveSize&quot;: 3852986, &quot;pullsyncRate&quot;: 0, &quot;storageRadius&quot;: 10, &quot;connectedPeers&quot;: 186, &quot;neighborhoodSize&quot;: 12, &quot;batchCommitment&quot;: 133828050944, &quot;isReachable&quot;: true } ] }   Now that we have the status for our own node and all its neighborhood peers we can begin to diagnose the issue through a series of checks outlined below:  info If you are able to identify and fix a problem with your node from the checklist below, it's possible that your node's reserve has become corrupted. Therefore, after fixing the problem, stop your node, and repair your node according to the instructions in the section following the checklist.  Compare reserveSize with peers The reserveSize value is the number of chunks stored by a node in its reserve. The value for reserveSize for a healthy node should be around +/- 1% the size of most other nodes in the neighborhood. In our example, for our node's reserveSize of 3747532, it falls within that normal range. This does not guarantee our node has no missing or corrupted chunks, but it does indicate that it is generally storing the same chunks as its neighbors. If it falls outside this range, see the next section for instructions on repairing reserves. Compare batchCommitment with peers The batchCommitment value shows how many chunks would be stored if all postage batches were fully utilised. It also represents whether the node has fully synced postage batch data from on-chain. If your node's batchCommitment value falls below that of its peers in the same neighborhood, it could indicate an issue with your blockchain RPC endpoint that is preventing it from properly syncing on-chain data. If you are running your own node, check your setup to make sure it is functioning properly, or check with your provider if you are using a 3rd party service for your RPC endpoint. Check pullsyncRate The pullsyncRate value measures the speed at which a node is syncing chunks from its peers. Once a node is fully synced, pullsyncRate should go to zero. If pullsyncRate is above zero it indicates that your node is still syncing chunks, so you should wait until it goes to zero before doing any other checks. If pullsyncRate is at zero but your node's reserveSize does not match its peers, you should check whether your network connection and RPC endpoint are stable and functioning properly. A node should be fully synced after several hours at most. Check most recent block number The block value returned from the /redistributionstate endpoint shows the most recent block a node has synced. If this number is far behind the actual more recent block then it indicates an issue with your RPC endpoint or network. If you are running your own node, check your setup to make sure it is functioning properly, or check with your provider if you are using a 3rd party service for your RPC endpoint. curl -X GET http://localhost:1633/redistributionstate | jq { &quot;minimumFunds&quot;: &quot;18750000000000000&quot;, &quot;hasSufficientFunds&quot;: true, &quot;isFrozen&quot;: false, &quot;isFullySynced&quot;: true, &quot;phase&quot;: &quot;commit&quot;, &quot;round&quot;: 176319, &quot;lastWonRound&quot;: 176024, &quot;lastPlayedRound&quot;: 176182, &quot;lastFrozenRound&quot;: 0, &quot;block&quot;: 26800488, &quot;reward&quot;: &quot;10479124611072000&quot;, &quot;fees&quot;: &quot;30166618102500000&quot; } Check peer connectivity Compare the value of your node's neighborhoodSize from the /status endpoint and the neighborhoodSize of its peers in the same neighborhood from the /status/peers endpoint. The figure should be generally the same (although it may fluctuate slightly up or down at any one point in time). If your node's neighborhoodSize value is significantly different and remains so over time then your node likely has a connectivity problem. Make sure to check your network environment to ensure your node is able to communicate with the network.  If no problems are identified during these checks it likely indicates that your node was frozen in error and there are no additional steps you need to take.  ","version":"Next","tagName":"h3"},{"title":"Repairing corrupt reserve​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#repairing-corrupt-reserve","content":" If you have identified and fixed a problem causing your node to become frozen or have other reason to believe that your node's reserves are corrupted then you should repair your node's reserve using the db repair-reserve command.  First stop your node, and then run the following command:  caution Make sure to replace /home/bee/.bee with your node’s data directory if it differs from the one shown in the example. Make sure that the directory you specify is the root directory for your node’s data files, not the localstore directory itself. This is the same directory specified using the data-dir option in your node’s configuration.  bee db repair-reserve --data-dir=/home/bee/.bee   After the command has finished running, you may restart your node.  ","version":"Next","tagName":"h3"},{"title":"Node occupies unusually large space on disk​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#node-occupies-unusually-large-space-on-disk","content":" During normal operation of a Bee node, it should not take up more than ~30 GB of disk space. In the rare cases when the node's occupied disk space grows larger, you may need to use the compaction db compact command.  danger To prevent any data loss, operators should run the compaction on a copy of the localstore directory and, if successful, replace the original localstore with the compacted copy.  The command is available as a sub-command under db as such (make sure to replace the value for --data-dir with the correct path to your bee node's data folder if it differs from the path shown in the example):  bee db compact --data-dir=/home/bee/.bee   ","version":"Next","tagName":"h3"},{"title":"Node not participating in redistribution​","type":1,"pageTitle":"Staking","url":"/docs/bee/working-with-bee/staking#node-not-participating-in-redistribution","content":" First check that the node is fully synced, is not frozen, and has sufficient funds to participate in staking. To check node sync status, call the redistributionstate endpoint:  curl -X GET http://localhost:1633/redistributionstate | jq   Response:  { &quot;minimumFunds&quot;: &quot;18750000000000000&quot;, &quot;hasSufficientFunds&quot;: true, &quot;isFrozen&quot;: false, &quot;isFullySynced&quot;: true, &quot;phase&quot;: &quot;commit&quot;, &quot;round&quot;: 176319, &quot;lastWonRound&quot;: 176024, &quot;lastPlayedRound&quot;: 176182, &quot;lastFrozenRound&quot;: 0, &quot;block&quot;: 26800488, &quot;reward&quot;: &quot;10479124611072000&quot;, &quot;fees&quot;: &quot;30166618102500000&quot; }   Confirm that hasSufficientFunds is true, and isFullySynced is true before moving to the next step. If hasSufficientFunds if false, make sure to add at least the amount of xDAI shown in minimumFunds (unit of 1e-18 xDAI). If the node was recently installed and isFullySynced is false, wait for the node to fully sync before continuing. After confirming the node's status, continue to the next step.  Run sampler process to benchmark performance​  One of the most common issues affecting staking is the sampler process failing. The sampler is a resource intensive process which is run by nodes which are selected to take part in redistribution. The process may fail or time out if the node's hardware specifications aren't high enough. To check a node's performance the /rchash/{depth}/{anchor_01}/{anchor_02} endpoint of the API may be used. The anchor_01 and anchor_02 must be a hex string with an even number of digits. For simplicity, you can just use aaaa for both anchors as we do in the example further down.  The {anchor} value can be set to any random hexadecimal string, while {depth} should be set to the current depth.  To get the current depth, call the /reservestate endpoint  sudo curl -sX GET http://localhost:1633/reservestate | jq   Copy the storageRadius value from the output (this represents the ACTUAL depth for your node, in other words, the depth to which your node is responsible for storing files. The radius value is the hypothetical depth your node would be at if every postage batch was fully utilised.)  { &quot;radius&quot;: 15, &quot;storageRadius&quot;: 10, &quot;commitment&quot;: 128332464128 }   Call the endpoint like so:  sudo curl -sX GET http://localhost:1633/rchash/10/aaaa/aaaa | jq   If the sampler runs successfully, you should see output like this:  { &quot;Sample&quot;: { &quot;Items&quot;: [ &quot;000003dac2b2f75842e410474dfa4c1e6e0b9970d81b57b33564c5620667ba96&quot;, &quot;00000baace30916f7445dbcc44d9b55cb699925acfbe157e4498c63bde834f40&quot;, &quot;0000126f48fb1e99e471efc683565e4b245703c922b9956f89cbe09e1238e983&quot;, &quot;000012db04a281b7cc0e6436a49bdc5b06ff85396fcb327330ca307e409d2a04&quot;, &quot;000014f365b1a381dda85bbeabdd3040fb1395ca9e222e72a597f4cc76ecf6c2&quot;, &quot;00001869a9216b3da6814a877fdbc31f156fc2e983b52bc68ffc6d3f3cc79af0&quot;, &quot;0000198c0456230b555d5261091cf9206e75b4ad738495a60640b425ecdf408f&quot;, &quot;00001a523bd1b688472c6ea5a3c87c697db64d54744829372ac808de8ec1d427&quot; ], &quot;Hash&quot;: &quot;7f7d93c6235855fedc34e32c6b67253e27910ca4e3b8f2d942efcd758a6d8829&quot; }, &quot;Time&quot;: &quot;2m54.087909745s&quot; }   If the Time value is higher than 6 minutes, then the hardware specifications for the node may need to be upgraded.  If there is an evictions related error such as the one below, try running the call to the /rchash/ endpoint again.  error: &quot;level&quot;=&quot;error&quot; &quot;logger&quot;=&quot;node/storageincentives&quot; &quot;msg&quot;=&quot;make sample&quot; &quot;error&quot;=&quot;sampler: failed creating sample: sampler stopped due to ongoing evictions&quot;   While evictions are a normal part of Bee's standard operation, the event of an eviction will interrupt the sampler process.  If you are still experiencing problems, you can find more help in the node-operators Discord channel (for your safety, do not accept advice from anyone sending a private message on Discord). ","version":"Next","tagName":"h3"},{"title":"Publish a Website","type":0,"sectionRef":"#","url":"/docs/desktop/publish-a-website","content":"","keywords":"","version":"Next"},{"title":"Step by Step Guide​","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#step-by-step-guide","content":" ","version":"Next","tagName":"h2"},{"title":"Install Swarm Desktop and Deposit Funds​","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#install-swarm-desktop-and-deposit-funds","content":" First, download and install the Swarm Desktop App. Next, add xDAI (transaction fees) to your Node Wallet address. If you possess xBZZ (storage fees), you can deposit it alongside the xDAI. Otherwise, you can exchange your xDAI for xBZZ using the Swarm Desktop app.  Follow these steps to deposit funds:  Launch the Swarm Desktop App and go to the Account section in the left menu.Transfer xDAI to your node wallet address. For safety, we suggest sending no more than 5 to 10 xDAI.After funding your wallet, click the Top Up Wallet button on the right side of the screen.Select the Use xDAI option.Verify your xDAI balance and click Proceed.Specify the amount of xDAI to convert to xBZZ and click Swap Now.Your Node Wallet address will be credited with xBZZ.    ","version":"Next","tagName":"h3"},{"title":"Setup Chequebook​","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#setup-chequebook","content":" Your node address is now funded with xDAI and xBZZ. However, to upload data on Swarm, you will need to transfer your funds to the Chequebook contract address.  Follow these steps:  Go to the Account section in the left menu.Select the Chequebook tab in the top menu.Click the Deposit button.Specify the amount of xBZZ to deposit into your Chequebook, which will be used for storage costs.  ","version":"Next","tagName":"h3"},{"title":"Publish Website​","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#publish-website","content":" To publish your website on Swarm, follow these steps:  Navigate to the Files tab.Click the Add Website button.Select your website folder. NOTE: The index.html file should be in the root folder.Purchase a Postage Stamp to publish your page. NOTE: Postage stamps cover storage costs for a specified duration.Upload the website.    https://api.gateway.ethswarm.org/bzz/6843d3be17364ea0620011430e4db2a26ff781da478493a02d6eb5aae886b8ae/  ","version":"Next","tagName":"h3"},{"title":"Connecting an ENS Domain to Your Website​","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#connecting-an-ens-domain-to-your-website","content":" Associating your ENS domain with a Swarm hash generates a memorable, user-friendly identifier for your website, allowing users to easily locate and access your website without having to recall a lengthy, complex Swarm hash.  Initially, you’ll need to register your domain name. To register and manage your ENS domain, you can use the ENS Domains Dapp along with the MetaMask browser extension.  After registering your name and connecting MetaMask to the relevant Ethereum account, set the resolver to use the public ENS if you haven’t already.  Navigate to My Names and select the name you want to link to your Swarm content.Click on ADD/EDIT RECORD.From the &quot;add record&quot; dropdown menu, select Content.Enter your Swarm Hash, beginning with &quot;bzz://&quot; and click &quot;Save.&quot;    Your website is now available on:  https://api.gateway.ethswarm.org/bzz/swarm-devrel.eth/  ","version":"Next","tagName":"h3"},{"title":"Update the Website: Set up and update a feed​","type":1,"pageTitle":"Publish a Website","url":"/docs/desktop/publish-a-website#update-the-website-set-up-and-update-a-feed","content":" Swarm feeds allow you to easily create a permanent address for your content stored on Swarm that you can update at any time.  If you plan to update your website in the future, it’s recommended that you set up a “Feed” before uploading your website to Swarm. This way, the Swarm Hash connected to your ENS domain will stay the same, even as you change the content behind that hash. This will enable you to update your website’s content without changing the Swarm Hash and incurring Ethereum transaction costs each time you do so.  Set up a Feed:​  Navigate to to AccountClick on Feeds in the top menuClick on Create New FeedDefine Identity nameAnd click Create Feed.  Upload Website on Swarm and connect it to the Feed:​  Navigate to to AccountClick on Feeds in the top menuChoose the Feed you want to updateClick View Feed PageClick the Add Website button.Select your website folder. NOTE: The index.html file should be in the root folder.Add Postage Stamp to publish your page. NOTE: Postage stamps cover storage costs for a specified duration.Upload the website to your Node.Connect Feed hash to ENS domain as described above.    By following these instructions, you can now leverage the benefits of decentralised storage, maintain a censorship-resistant website, and create a user-friendly experience by connecting your site to an ENS domain. ","version":"Next","tagName":"h3"},{"title":"Postage Stamps","type":0,"sectionRef":"#","url":"/docs/desktop/postage-stamps","content":"","keywords":"","version":"Next"},{"title":"How to Buy a Postage Stamp Batch​","type":1,"pageTitle":"Postage Stamps","url":"/docs/desktop/postage-stamps#how-to-buy-a-postage-stamp-batch","content":" Stamps can be purchased by selecting Stamps from the Account tab:    And then clicking the Buy New Postage Stamp button:    ","version":"Next","tagName":"h2"},{"title":"Depth and Amount​","type":1,"pageTitle":"Postage Stamps","url":"/docs/desktop/postage-stamps#depth-and-amount","content":" Batch depth and amount are the two required parameters which must be set when purchasing a postage stamp batch. Depth determines how many chunks can be stamped with a batch while amount determines how much xBZZ is assigned per chunk.    Inputting a value for depth allows you to preview the upper limit of data which can be uploaded for that depth.  Inputting a value for amount and depth together will allow you to also preview the total cost of the postage stamp batch as well as the TTL (time to live - how long the batch can store data on Swarm). Click the Buy New Stamp button to purchase the stamp batch.    After purchasing stamps you can view stamp details from the Postage Stamps drop down menu:    ","version":"Next","tagName":"h3"},{"title":"Managing Postage Batches​","type":1,"pageTitle":"Postage Stamps","url":"/docs/desktop/postage-stamps#managing-postage-batches","content":" After purchasing a postage batch, it is important to monitor the usage and TTL (time to live) of your batch.  TTL is shown next to the &quot;Expired in&quot; label in the screenshot below.    For this stamp batch, it has only 6 hours left. Once the TTL has run out completely, the content uploaded using that batch will no longer be kept on Swarm, and will be lost forever. To prevent this from happening, you can &quot;top up&quot; your batch by adding more xBZZ to the batch balance to increase the batch TTL.  ","version":"Next","tagName":"h2"},{"title":"Top-up a Batch​","type":1,"pageTitle":"Postage Stamps","url":"/docs/desktop/postage-stamps#top-up-a-batch","content":" To get started, click on the &quot;Topup and Dilute&quot; button.    From the &quot;Action&quot; dropdown menu, make sure that you have &quot;Topup&quot; selected and then fill in the amount by which you wish to top up the batch. Note that the number entered here is in PLUR (1e-16 xBZZ), and it is the same `amount`` parameter described in the section above on purchasing postage stamp batches, it is NOT equal to the total amount of xBZZ spent for this top up transaction.  After inputting the amount, click &quot;Topup&quot; to submit the transaction.  After a few moments, you will see a notice that the transaction was successful in a green alert box. A few moments after that, you will see the updated TTL in the stamp details window.    ","version":"Next","tagName":"h2"},{"title":"Dilute a Batch​","type":1,"pageTitle":"Postage Stamps","url":"/docs/desktop/postage-stamps#dilute-a-batch","content":" If our batch begins to come close to becoming fully utilised, we can choose to increase the depth of the batch to increase the amount of data it can store. This is referred to as &quot;dilution&quot;, since by increasing the depth without updating the amount, we dilute the amount of xBZZ which is assigned to each chunk. In other words, the dilute transaction will increase the amount which can be uploaded by a batch while also decreasing the TTL. Therefore it is important to both top up and also dilute your stamp batch if you wish to increase the amount stored by the batch without decreasing its TTL.  To get started, click on the &quot;Topup and Dilute&quot; button. Make sure to select &quot;Dilute&quot; from the &quot;Action&quot; dropdown menu.    From here, we can select the new depth value for our postage stamp batch. In this instance, we will increase it from 20 to 21.    After a few moments the transaction will be completed and you should see the updated Depth, Capacity, and TTL.    Note that both the Depth and Capacity have increased while the TTL has decreased. ","version":"Next","tagName":"h2"},{"title":"Upload Content","type":0,"sectionRef":"#","url":"/docs/desktop/upload-content","content":"Upload Content After purchasing a batch of postage stamps you will be able to upload files. First go to the “Files” tab. Here you can choose between three options, depending on what you want to upload: a single file, a folder or a website. After choosing your option you’ll need to add a postage stamp. Click “Add postage stamp” and choose a postage stamp. Click “Proceed with the selected stamp” and upload your data. Once your file is uploaded a Swarm hash and Swarm Gateway link will be displayed (which you can use to share the file with others) along with other pertinent information.","keywords":"","version":"Next"},{"title":"Start a Blog","type":0,"sectionRef":"#","url":"/docs/desktop/start-a-blog","content":"","keywords":"","version":"Next"},{"title":"A Guide to Starting Your Blog on Swarm​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#a-guide-to-starting-your-blog-on-swarm","content":" There are many different approaches to starting a blog on Swarm, however the easiest is to use the Etherjot Web blogging tool. Etherjot Web is a straightforward tool for publishing and editing your blog on Swarm. It handles all uploading of files, page customization, basic UI template, and even comes with a &quot;comments&quot; feature so any other Swarm user can leave a comment on your blog.  ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#requirements","content":" npmNode.jsgitSwarm Desktop with a valid postage stamp batch  ","version":"Next","tagName":"h2"},{"title":"Getting Started​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#getting-started","content":" To get started you must first have installed Swarm Desktop and have it running on your computer with a valid stamp batch. Note that your blog will only stay online as long as the postage batch is still valid, therefore you must make sure to stay aware of the postage batch TTL (time to live), and top up your batch regularly in order to keep your content online.  ","version":"Next","tagName":"h2"},{"title":"Open Etherjot​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#open-etherjot","content":" To open Etherjot, right click the Swarm Desktop icon in your dashboard and navigate to &quot;Apps&quot;, and then click on &quot;Etherjot&quot;.    ","version":"Next","tagName":"h3"},{"title":"Initialize Your Blog​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#initialize-your-blog","content":" When first starting Etherjot Web, you will be greeted with this page:    On this page, as long as you have fulfilled the requirements outlined above, you will see two green checkmarks confirming you have Swarm Desktop running with a valid postage stamp batch. You will also see a warning reminding you of the importance of topping up your stamp batch to prevent the batch TTL from running out.  danger In addition to monitoring your postage stamp batch TTL, it is also important that you back up your blog, or else you may lose access to your blog in Etherjot (although it will still remain live on Swarm as long as its stamp batch has not expired).  Fill in your blog name, check the box with the TTL warning, and click the &quot;Create&quot; button to initialize your blog. This will issue a Swarm transaction to set up a feed for your blog. The transaction will take a few moments, after which you will be greeted with the Etherjot Web blog editor.    The &quot;Swarm Hash&quot; displayed at the top of the editor is the address for the homepage of your blog. Click &quot;Open&quot; to navigate to your blog. We can see now that the blog has been initialized, but no content has been uploaded.    ","version":"Next","tagName":"h2"},{"title":"Don't Lose Your Work!​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#dont-lose-your-work","content":" Due to the decentralised nature of Swarm and applications built on Swarm, there are several precautions you should take which you may be unfamiliar with when coming from a Web 2.0 application.  ","version":"Next","tagName":"h2"},{"title":"Back-up Your Blog​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#back-up-your-blog","content":" No username and password are required for editing your blog and uploading new posts. However, you do need to make sure to back up your blog in order to prevent losing access to it. You should do this after initializing your blog, and you should also back up your blog again after publishing any changes. To back up your blog, start by clicking &quot;Settings.&quot;    From the Settings page, click &quot;Export.&quot;    Copy the displayed text to a .json file, make certain to copy the entire displayed text. This is your backup file and is used to import your blog. Note that the backup contains the private key of your blog, so should not be revealed to anyone else.    ","version":"Next","tagName":"h3"},{"title":"Avoid Losing Changes (DANGER)​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#avoid-losing-changes-danger","content":" Etherjot currently does not allow you to save drafts locally, so if you navigate away from the blog post you are currently editing, you will lose any changes you have made which have not yet been uploaded to Swarm. Take note of the three UI elements highlighted in the screenshot - using the &quot;+&quot; or &quot;Settings&quot; buttons will cause you to lose any changes not uploaded to Swarm, and hitting the &quot;Reset&quot; button will cause you to lose everything which has not been backed up.  danger Hitting the &quot;Reset&quot; button will cause you to lose any content which has not yet been published and backed up.    If you click the &quot;+&quot; button or the &quot;Settings&quot; button, you will see a warning to notify you that any unsaved changes will be lost.    You will NOT see a warning for refreshing your browser page, however, so be careful not to refresh your browser before publishing any changes to Swarm.  ","version":"Next","tagName":"h3"},{"title":"Writing Your Blog​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#writing-your-blog","content":" ","version":"Next","tagName":"h2"},{"title":"Add Some Text​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#add-some-text","content":" The text editor for your blog has two main panels. The one on the left is where you can write your content using Markdown. On the right side is where you can see a preview of your rendered markdown as it will appear to a visitor to your blog.    Let's fill in some content and examine the preview.    Here you can see the new content we just wrote, note that there is no auto-save functionality, so any changes we make will not be saved until we click &quot;Publish&quot; to upload the changes to Swarm. However you will see that the &quot;Publish&quot; button is greyed currently, as we have not yet filled in all the required fields for publishing.  ","version":"Next","tagName":"h3"},{"title":"Add Media Files​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#add-media-files","content":" Next let's try to add an image. To get started, we need to click the &quot;Asset Browser&quot; button.    This will open up the Asset Browser where you can manage your blog assets such as images.    To upload your file, click the &quot;browse&quot; button and choose the file you wish to upload.  info In addition to images, video and audio files may also be uploaded, however currently the URL to the Swarm hash must be manually inserted into html &lt;video&gt; and &lt;audio&gt; tags such as: &lt;video controls width=&quot;630&quot; height=&quot;300&quot; src=&quot;http://localhost:1633/bzz/f8abc6161fb6305437d2bf514d3a34ce32c234eddad1ca907d438a6f4f43183b/&quot;&gt;&lt;/video&gt; &lt;audio controls width=&quot;630&quot; height=&quot;300&quot; src=&quot;http://localhost:1633/bzz/7613d1b9e5d409301b6aaaf7b6a41f0457cd4130471d87de6b49bced70aaafce/&quot;&gt;&lt;/audio&gt; This will become automatic in a near future update to Etherjot Web.  After selecting your file and clicking &quot;OK&quot;, your file will be uploaded to Swarm, this will take a few moments.    Once it has finished uploading, you will then be able to view your file in the Asset Browser.    To add the image to your blog, simple click the &quot;Insert&quot; button.    After hitting &quot;Insert&quot;, the URL with the Swarm hash of the file you just uploaded will be automatically inserted into a markdown image tag, and will then become viewable in the preview panel on the right.  ","version":"Next","tagName":"h3"},{"title":"Set Background Image​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#set-background-image","content":" To set the banner image which will display as the preview image for your blog, click the &quot;Select&quot; button under the &quot;Banner image&quot; label. This will open up the same Asset Browser mentioned in the section above on adding media, and you may choose an image from there or upload a new one.    ","version":"Next","tagName":"h3"},{"title":"Set Blog \"Type\"​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#set-blog-type","content":" Next let's set the &quot;Type&quot; of our blog. &quot;Type&quot; determines where within the layout your blog post will appear. Simply click the dropdown button next to &quot;Type&quot; and make your choice.    ","version":"Next","tagName":"h3"},{"title":"Set Blog \"Category\" (REQUIRED)​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#set-blog-category-required","content":" info Note that &quot;Category&quot; must be set in order to publish any blog post.  Next we must also choose the &quot;Category&quot; for your blog post. This is used to group articles within your blog. You may choose whatever name you wish for the blog category.    ","version":"Next","tagName":"h3"},{"title":"Publishing Your Blog​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#publishing-your-blog","content":" Now we are ready to publish our first blog post! Simply click the &quot;Publish&quot; button to initiate the transaction on Swarm to upload your blog.    Once your blog has finished uploading, you will see it in the panel on the left. From here you can click &quot;Edit&quot; or &quot;Delete&quot; to continue editing your post or delete it.    ","version":"Next","tagName":"h3"},{"title":"Add a New Post​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#add-a-new-post","content":" To add a new post, click the &quot;+&quot; button. Note that this will discard any changes from the current post you are editing which have not yet been published. Make certain to publish any changes you want saved first.    ","version":"Next","tagName":"h3"},{"title":"Settings and Optional Features​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#settings-and-optional-features","content":" Click &quot;Settings&quot; to open up the Settings page. Note that any unpublished changes will be lost.    From here you can back up and restore blogs, and can set a variety of other options.  ","version":"Next","tagName":"h2"},{"title":"Setting Custom Text and Links​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#setting-custom-text-and-links","content":" Highlighted in this first screenshot you can see the options for setting custom text and links.    ","version":"Next","tagName":"h3"},{"title":"Changing Default Text​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#changing-default-text","content":" ","version":"Next","tagName":"h3"},{"title":"Reset Your Blog (DANGER)​","type":1,"pageTitle":"Start a Blog","url":"/docs/desktop/start-a-blog#reset-your-blog-danger","content":" danger Hitting the &quot;Reset&quot; button will cause you to lose any content which has not yet been published and backed up. Make sure you have backed up your blog before clicking &quot;reset!&quot;  To reset your blog and start a new blog, click the &quot;Reset&quot; button. This will immediately reset your blog, and you will lose any changes which have not yet been published. You will ALSO lose anything which you have not backed up, so it is important to back up your blog before resetting.   ","version":"Next","tagName":"h2"},{"title":"Host Your Website on Swarm","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/host-your-website","content":"","keywords":"","version":"Next"},{"title":"Enable ENS on Your Node​","type":1,"pageTitle":"Host Your Website on Swarm","url":"/docs/develop/access-the-swarm/host-your-website#enable-ens-on-your-node","content":" In order to resolve ENS names using your API endpoints, you must specify a valid ENS resolver endpoint when starting your Bee node. We recommend that users run their own Geth node, which can be trusted absolutely, however service providers such as https://cloudflare-eth.com or Infura may suffice. Public gateways such as gateway.ethswarm.org will also usually provide ENS resolution.  bee start --resolver-options &quot;https://cloudflare-eth.com&quot;   If specifying using your bee.yaml configuration file, the syntax is as follows:  resolver-options: [ &quot;https://cloudflare-eth.com&quot; ]   Once you have restarted your node, you should be able to see the Swarm homepage at:  http://localhost:1633/bzz/swarm.eth/  info Use the resolver-options flag to point the Bee resolver to any ENS compatible smart-contract on any EVM compatible chain  warning Make sure you trust the gateway you are interacting with! To ensure that you are retrieving the correct content, run your own ENS resolver and Bee node.  ","version":"Next","tagName":"h3"},{"title":"Link an ENS domain to a website.​","type":1,"pageTitle":"Host Your Website on Swarm","url":"/docs/develop/access-the-swarm/host-your-website#link-an-ens-domain-to-a-website","content":" First we will need to upload the website assets to Swarm in order to get its Swarm reference hash, seeuploading a directoryfor more information.  This time we will also include the Swarm-Index-Document header set to the index.html. This will cause Bee to serve each directories index.html file as default when browsing to the directory root / url. We will also provide a custom error page, using the Swarm-Error-Document header.  In the case that your website is a single page app, where you would like to direct to the JavaScript history API powered router, you may provide the index.html page for both settings.   curl -X POST -H &quot;Content-Type: application/x-tar&quot; -H &quot;swarm-postage-batch-id: 81c4520b47a434738d14fd38053a32c20aaf1a36d7f35f0d86ef25c70403d7a8&quot; -H &quot;Swarm-Index-Document: index.html&quot; -H &quot;Swarm-Error-Document: index.html&quot; --data-binary @website.tar http://localhost:1633/bzz   { &quot;reference&quot;: &quot;b25c89a401d9f26811680476619a1eb4a4e189e614bc6161cbfd8b343214917b&quot; }   Next, we add a Content record to your ENS domain's resolver contract.  We recommend the excellent ENS Domains Dapp used with the MetaMask browser extension for registering and administrating your ENS domain.  Once you have registered your name, and have connected MetaMask with the relevant Ethereum account, you must first set the resolver to use the public ENS if you have not already done so.  First, navigate to 'My Names', and select the name you would like to link your Swarm content to.  Press 'Set' next to your resolver record.    Select 'Use Public Resolver'.    Select '+' to add a record.    Choose the 'Content' record type from the drop down menu.    Add the Swarm reference you created earlier and press 'Save'.    Verify the Content Record has been created!    Done! 👏  Now you will be able to see your website hosted using the ENS name instead of the Swarm Reference!   ","version":"Next","tagName":"h3"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/introduction","content":"","keywords":"","version":"Next"},{"title":"Decentralise Your Files​","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#decentralise-your-files","content":" Bee provides several convenient ways to upload your data into the Swarm. Once your data has been uploaded, it will be distributed, stored and retrievable by a worldwide network of p2p nodes, and made available from Swarm's web gateway.  ","version":"Next","tagName":"h3"},{"title":"Upload Whole Directories​","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#upload-whole-directories","content":" Find out how to upload whole directories at once using Bee's HTTP API.  ","version":"Next","tagName":"h3"},{"title":"Host Your Website on the Decentralised Web​","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#host-your-website-on-the-decentralised-web","content":" Swarm is a distributed international network of nodes that provides hosting for your unstoppable websites. See this guide to hosting your website on swarm.  ","version":"Next","tagName":"h3"},{"title":"Sync With the Network​","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#sync-with-the-network","content":" Watch as your uploaded data is synced with the network of thousands of nodes worldwide!  ","version":"Next","tagName":"h3"},{"title":"Keep Your Data Alive​","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#keep-your-data-alive","content":" Learn how to assign xBZZ to your data using postage stamps so that it remains live on the Swarm network.  ","version":"Next","tagName":"h3"},{"title":"Pinning​","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#pinning","content":" Learn how to pin your data so that it always remains available locally on your Bee node.  ","version":"Next","tagName":"h3"},{"title":"Light Nodes​","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#light-nodes","content":" When accessing the Swarm network for certain use cases a Bee might not want to take part in forwarding and storing data. Find out how to use Bee in Light Node mode.  ","version":"Next","tagName":"h3"},{"title":"Ultra Light Nodes​","type":1,"pageTitle":"Introduction","url":"/docs/develop/access-the-swarm/introduction#ultra-light-nodes","content":" For the cases when we want to get started quickly we can run the Bee instance without a blockchain. Find out how to use Bee in Ultra Light Node mode. ","version":"Next","tagName":"h3"},{"title":"Erasure Coding","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/erasure-coding","content":"","keywords":"","version":"Next"},{"title":"Uploading With Erasure Coding​","type":1,"pageTitle":"Erasure Coding","url":"/docs/develop/access-the-swarm/erasure-coding#uploading-with-erasure-coding","content":" Erasure coding is available for the /bytes and /bzz endpoints, however it is not available for the /chunks endpoint which deals with single chunks. Since erasure coding relies on splitting data into chunks and the chunk is the smallest unit of data within Swarm which cannot be further subdivided, erasure coding is not applicable for the /chunks endpoint which deals with single chunks.  To upload data to Swarm using erasure coding, the swarm-redundancy-level: &lt;integer&gt; header is used:   curl \\ -X POST http://localhost:1633/bzz?name=test.txt \\ -H &quot;swarm-redundancy-level: 1&quot; \\ -H &quot;swarm-postage-batch-id: 54ba8e39a4f74ccfc7f903121e4d5d0fc40732b19efef5c8894d1f03bdd0f4c5&quot; \\ -H &quot;Content-Type: text/plain&quot; \\ --data-binary @test.txt {&quot;reference&quot;:&quot;c02e7d943fbc0e753540f377853b7181227a83e773870847765143681511c97d&quot;}   The accepted values for the swarm-redundancy-level header range from the default of 0 up to 4. Each level corresponds to a different level of data protection, with erasure coding turned off at 0, and at its maximum at 4. Each increasing level provides increasing amount of data redundancy offering greater protection against data loss. Each level has been formulated to guarantee against a certain percentage of chunk retrieval errors, shown in the table below. As long as the error rate is below the expected chunk retrieval rate for the given level, there is a less than 1 in a million chance of failure to retrieve the source data.  Redundancy Level\tPseudonym\tExpected Chunk Retrieval Error Rate0\tNone\t0% 1\tMedium\t1% 2\tStrong\t5% 3\tInsane\t10% 4\tParanoid\t50%  ","version":"Next","tagName":"h2"},{"title":"Redundancy Level Costs Explained​","type":1,"pageTitle":"Erasure Coding","url":"/docs/develop/access-the-swarm/erasure-coding#redundancy-level-costs-explained","content":" Erasure encoding is applied to sets of chunks of at most size 128 (including both data chunks and parity chunks). For higher levels of redundancy, the ratio of parity chunks to data chunks increases, increasing the percent cost of the upload compared to uploading at lower levels of redundancy.  In the table below, the percent cost is displayed for each redundancy level. The cost of encrypted uploads is also shown, and is double the cost of un-encrypted uploads.  Redundancy\tParities\tData Chunks\tPercent\tChunks Encrypted\tPercent EncryptedMedium\t9\t119\t7.6%\t59\t15% Strong\t21\t107\t19.6%\t53\t40% Insane\t31\t97\t32%\t48\t65% Paranoid\t89\t37\t240.5%\t18\t494%  For larger uploads (where the source data chunks are equal to or greater than the &quot;Data Chunks&quot; for each redundancy level respectively) you can use the percent values shown in the &quot;Percent&quot; column as a general estimate of the percent cost of uploading. If the number of chunks is slightly less than the number shown in the &quot;Data Chunks&quot; column, you can also use the value in the &quot;Percent&quot; column as a good general estimate of the percent cost.  However, if the number of source data chunks are significantly less than the value in the &quot;Data Chunks&quot; column for each respective level, then the percent cost will differ significantly from the one shown in the &quot;Percent&quot; column. For more precise calculations, see the relevant appendix.  ","version":"Next","tagName":"h3"},{"title":"Cost Calculator Widget​","type":1,"pageTitle":"Erasure Coding","url":"/docs/develop/access-the-swarm/erasure-coding#cost-calculator-widget","content":" This calculator takes as input an amount of data and an erasure coding redundancy level, and outputs the number of additional parity chunks required to erasure code that amount of data as well as the increase in cost to upload vs. a non-erasure encoded upload:  Data Size: Data Unit: ChunksKBGB Redundancy Level: Select Redundancy LevelMediumStrongInsaneParanoid Use Encryption? Calculate  ","version":"Next","tagName":"h2"},{"title":"Downloading Erasure Encoded Data​","type":1,"pageTitle":"Erasure Coding","url":"/docs/develop/access-the-swarm/erasure-coding#downloading-erasure-encoded-data","content":" For a downloader, the process for downloading a file which has been erasure encoded does not require any changes from the normal download process. There are several options for adjusting the default behaviour for erasure encoded downloads, however there is no need to adjust them.  ","version":"Next","tagName":"h2"},{"title":"Default Download Behaviour​","type":1,"pageTitle":"Erasure Coding","url":"/docs/develop/access-the-swarm/erasure-coding#default-download-behaviour","content":" Erasure coding retrieval for downloads is enabled by default, so there is no need for a downloader to explicitly enable the feature. The default download behaviour is to use the DATA strategy with fallback enabled. With these settings, first an attempt will be made to download the data chunks only. If any of the data chunks are missing, then the retrieval method will fall back to the RACE strategy (PROX is not currently implemented and so will be skipped). With the RACE strategy, an attempt will be made to download all data and parity chunks, and chunks will continue to be downloaded until enough have been retrieved to reconstruct the original data.  ","version":"Next","tagName":"h3"},{"title":"Options​","type":1,"pageTitle":"Erasure Coding","url":"/docs/develop/access-the-swarm/erasure-coding#options","content":" warning Do not adjust these options unless you know exactly what you are doing. The default settings are the best option for almost all cases.  When downloading erasure encoded data, there are three related headers which may be used: swarm-redundancy-strategy, swarm-redundancy-fallback-mode: &lt;integer&gt;, and swarm-chunk-retrieval-timeout.  swarm-redundancy-strategy: This header allows you to set the retrieval strategy for fetching chunks. The accepted values range from 0 to 3. Each number corresponds to a different chunk retrieval strategy. The numbers stand for the NONE, DATA, PROX and RACE strategies respectively which are described in greater detail in the API reference (also see the erasure code paper for even more in-depth descriptions). With each increasing level, there will be a potentially greater bandwidth cost. Retrieval Strategies NONE: This strategy is based on direct retrieval of data chunks without pre-fetching, with parity chunks ignored. No pre-fetching is used (data chunks are fetched sequentially).DATA: The same as NONE, except that data chunks are pre-fetched (data chunks are fetched in parallel in order to reduce latency).PROX: For this strategy, the chunks closest (in Kademlia distance) to the node are retrieved first. (Not yet implemented.)RACE: Initiates requests for all data and parity chunks and continues to retrieve chunks until enough chunks are retrieved that the original data can be reconstructed. swarm-redundancy-fallback-mode: &lt;boolean&gt;: Enables the fallback feature for the redundancy strategies so that if one of the retrieval strategies fails, it will fallback to the more intensive strategy until retrieval is successful or retrieval fails. Default is true. swarm-chunk-retrieval-timeout: &lt;boolean&gt;: Allows you to specify the timeout time for chunk retrieval with a default value of 30 seconds. (This is primarily used by the Bee development team for testing and it's recommended that Bee users do not need to use this option.)  An example download request may look something like this:   curl -OJL \\ -H &quot;swarm-redundancy-strategy: 3&quot; \\ -H &quot;swarm-redundancy-fallback-mode: true&quot; \\ http://localhost:1633/bzz/c02e7d943fbc0e753540f377853b7181227a83e773870847765143681511c97d/ % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0   For this request, the redundancy strategy is set to 3 (RACE), which means that it will initiate a request for all data and parity chunks and continue to retrieve chunks until enough have been retrieved to reconstruct the source data. This is in contrast with the default strategy of DATA where only the data chunks will be retrieved.  However, as noted above, it is recommended to not adjust the default settings for these options, so a typical request would actually look like this (which is the exact same as a normal download without any additional options set):   curl -OJL http://localhost:1633/bzz/c02e7d943fbc0e753540f377853b7181227a83e773870847765143681511c97d/ % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0   This means that there is no need for you to inform downloaders that a file you have uploaded uses erasure coding, as even with the default download behaviour reconstruction of the source file will be attempted if any chunks are missing. ","version":"Next","tagName":"h3"},{"title":"Pinning","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/pinning","content":"","keywords":"","version":"Next"},{"title":"Pin During Upload​","type":1,"pageTitle":"Pinning","url":"/docs/develop/access-the-swarm/pinning#pin-during-upload","content":" To store content so that it will persist even when Bee's garbage collection routine is deleting old chunks, we simply pass the Swarm-Pin header set to true when uploading.  curl -H &quot;Swarm-Pin: true&quot; -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; --data-binary @bee.mp4 localhost:1633/bzz\\?bee.mp4   { &quot;reference&quot;: &quot;1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30&quot; }   ","version":"Next","tagName":"h2"},{"title":"Administer Pinned Content​","type":1,"pageTitle":"Pinning","url":"/docs/develop/access-the-swarm/pinning#administer-pinned-content","content":" To check what content is currently pinned on your node, query the pins endpoint of your Bee API:  curl localhost:1633/pins   { &quot;references&quot;: [ &quot;1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30&quot; ] }   or, to check for specific references:  curl localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30   A 404 response indicates the content is not available.  ","version":"Next","tagName":"h2"},{"title":"Unpinning Content​","type":1,"pageTitle":"Pinning","url":"/docs/develop/access-the-swarm/pinning#unpinning-content","content":" We can unpin content by sending a DELETE request to the pinning endpoint using the same reference:  curl -XDELETE http://localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30 `` ```json {&quot;message&quot;:&quot;OK&quot;,&quot;code&quot;:200}   Now, when check again, we will get a 404 error as the content is no longer pinned.  curl localhost:1633/pins/1bfe7c3ce4100ae7f02b62e38d3e8d4c3a86ea368349614a87827402f20cbb30   { &quot;message&quot;: &quot;Not Found&quot;, &quot;code&quot;: 404 }   info Pinning and unpinning is possible for files (as in the example) and also the chunks, directories, and bytes endpoints. See the API documentation for more details.  ","version":"Next","tagName":"h3"},{"title":"Pinning Already Uploaded Content​","type":1,"pageTitle":"Pinning","url":"/docs/develop/access-the-swarm/pinning#pinning-already-uploaded-content","content":" The previous example showed how we can pin content upon upload. It is also possible to pin content that is already uploaded and present in the Swarm.  To do so, we can send a POST request including the swarm reference to the files pinning endpoint.  curl -XPOST http://localhost:1633/pins/7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f   { &quot;message&quot;: &quot;OK&quot;, &quot;code&quot;: 200 }   The pins operation will attempt to fetch the content from the network if it is not available on the local node.  Now, if we query our files pinning endpoint again, the swarm reference will be returned.  curl http://localhost:1633/pins/7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f   { &quot;reference&quot;: &quot;7b344ea68c699b0eca8bb4cfb3a77eb24f5e4e8ab50d38165e0fb48368350e8f&quot; }   warning While the pin operation will attempt to fetch content from the network if it is not available locally, we advise you to ensure that the content is available locally before calling the pin operation. If the content, for whatever reason, is only fetched partially from the network, the pin operation only partly succeeds and leaves the internal administration of pinning in an inconsistent state. ","version":"Next","tagName":"h3"},{"title":"Encrypt and Upload a File","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/store-with-encryption","content":"In Swarm, all data is public by default. To protect sensitive content, it must be encrypted so that only authorised users are able to decrypt and then view the plaintext content. The Bee client provides a facility to encrypt files and directories while uploading which are only able to be read by users with access to the corresponding decryption key. Encrypt and Upload a File To encrypt a file simply include the Swarm-Encrypt: true header with your HTTP request. curl -F file=@bee.jpg -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; -H &quot;Swarm-Encrypt: true&quot; http://localhost:1633/bzz When successful, the Bee client will return a 64 byte reference, instead of the usual 32 bytes. More information on how to buy a postage stamp batch and get its batch id can be found here. { &quot;reference&quot;: &quot;f7b1a45b70ee91d3dbfd98a2a692387f24db7279a9c96c447409e9205cf265baef29bf6aa294264762e33f6a18318562c86383dd8bfea2cec14fae08a8039bf3&quot; } Here we see that, when using the Bee node's encryption function, the reference hash that is returned is 128 hex characters long. The first 64 characters of this is the familiar Swarm address - the reference that allows us to retrieve the data from the swarm. This is the same reference we would get uploading unencrypted files using Bee, so it is safe to share. The second part of the reference is a 64 character decryption key which is required to decrypt the referenced content and view the original data in the clear. This is sensitive key material and must be kept private. It is important that this data not be sent in requests to a public gateway as this would mean that gateway would be able to decrypt your data. However, if you are running a node on your local machine, you may safely use the API bound to localhost. The key material is never exposed to the network so your data remains safe. info Encryption is disabled by default on all Swarm Gateways to keep your data safe. Install Bee on your computer to use the encryption feature. Download and Decrypt a File To retrieve your file, simply supply the full 64 byte string to the files endpoint, and the Bee client will download and decrypt all the relevant chunks and restore them to their original format. curl -OJ http://localhost:1633/bzz/f7b1a45b70ee91d3dbfd98a2a692387f24db7279a9c96c447409e9205cf265baef29bf6aa294264762e33f6a18318562c86383dd8bfea2cec14fae08a8039bf3 danger Never use public gateways when requesting full encrypted references. The hash contains sensitive key information which should be kept private. Run your own node to use Bee's encryption features.","keywords":"","version":"Next"},{"title":"Ultra Light Nodes","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/ultra-light-nodes","content":"Ultra Light Nodes danger When running without a blockchain connections, bandwidth incentive payments (SWAP) cannot be made so there is a risk of getting blocklisted by other peers for unpaid services. Configuration​ To run Bee as an ultra-light node full-node and swap-enable must both be set to false, and the blockchain-rpc-endpoint value should be set to an empty string &quot;&quot; or commented out in the configuration. Mode of Operation​ The target audience for this mode of operations are users who want to try out running a node but don't want to go through the hassle of blockchain onboarding. Ultra-light nodes will be able to download data as long as the data consumed does not exceed the payment threshold (payment-threshold in configuration) set by peers they connect to. Running Bee without a connected blockchain backend, however, imposes some limitations: Can't do overlay verificationCan't do SWAP settlements Since we can't buy postage stamps: Can't send PSS messagesCan't upload data to the network","keywords":"","version":"Next"},{"title":"Track Upload Status","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/syncing","content":"","keywords":"","version":"Next"},{"title":"Generate the tag​","type":1,"pageTitle":"Track Upload Status","url":"/docs/develop/access-the-swarm/syncing#generate-the-tag","content":" While the automatically-generated tag is convenient, with big uploads it might take a while until the Bee API returns the headers. What you want to do in this case is to pre-generate the tag and pass this tag along with the upload command.  Generate a tag:  curl -X POST http://localhost:1633/tags   The uid value is your tag:   { &quot;split&quot;: 0, &quot;seen&quot;: 0, &quot;stored&quot;: 0, &quot;sent&quot;: 0, &quot;synced&quot;: 0, &quot;uid&quot;: 5, &quot;address&quot;: &quot;&quot;, &quot;startedAt&quot;: &quot;2023-08-31T06:46:41.574003454Z&quot; }   info In order to upload your data to swarm, you must agree to burn some of your xBZZ to signify that the content is important. Before you progress to the next step, you must buy stamps! See this guide on how to purchase an appropriate batch of stamps.  Pass the tag along with the upload:  curl --data-binary @bee.jpg \\ -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; \\ -H &quot;Swarm-Tag: 5&quot; \\ &quot;http://localhost:1633/bzz?name=bee.jpg&quot;   ","version":"Next","tagName":"h3"},{"title":"Ask for the Current Status​","type":1,"pageTitle":"Track Upload Status","url":"/docs/develop/access-the-swarm/syncing#ask-for-the-current-status","content":" To get the current status of an upload, send a GET request to the tag/&lt;Swarm-Tag&gt; API endpoint.  curl http://localhost:1633/tags/5 | jq   The response contains all the information that you need to follow the status of your file as it is synced with the network.  info The numbers that the tags endpoint returns under total, processed and synced are denominated in chunks, i.e. Swarm's 4kb data units. ","version":"Next","tagName":"h3"},{"title":"Contribute to Bee Development","type":0,"sectionRef":"#","url":"/docs/develop/contribute/introduction","content":"","keywords":"","version":"Next"},{"title":"Testing a connection with PingPong protocol​","type":1,"pageTitle":"Contribute to Bee Development","url":"/docs/develop/contribute/introduction#testing-a-connection-with-pingpong-protocol","content":" To check if two nodes are connected and to see the round trip time for message exchange between them, get the overlay address from one node, for example local node 2:  curl localhost:1833/addresses   Make sure addresses are configured as in examples above.  And use that address in the API call on another node, for example, local node 1:  curl -XPOST localhost:1735/pingpong/d4440baf2d79e481c3c6fd93a2014d2e6fe0386418829439f26d13a8253d04f1   ","version":"Next","tagName":"h2"},{"title":"Generating protobuf​","type":1,"pageTitle":"Contribute to Bee Development","url":"/docs/develop/contribute/introduction#generating-protobuf","content":" To process protocol buffer files and generate the Go code from it two tools are needed:  protocprotoc-gen-gogofaster  Makefile rule protobuf can be used to automate protoc-gen-gogofaster installation and code generation:  make protobuf  ","version":"Next","tagName":"h2"},{"title":"Upload and Download","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/upload-and-download","content":"","keywords":"","version":"Next"},{"title":"Uploads and Download Endpoints Overview​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#uploads-and-download-endpoints-overview","content":" There are three endpoints which can be used for uploading and downloading data from Swarm, and each endpoint has different usage.  /bytes - Used for uploading raw data, lacks convenience features present in the /bzz endpoint but allows for greater customization for advanced use cases./bzz - Used for general download and uploads of files or collections of files./chunks - Used for downloading and uploading individual chunks, and also for uploading streams of chunks.  Generally speaking, the /bzz endpoint is appropriate for general common use cases such as uploading websites, sharing files, etc., while the /chunks and bytes endpoints allow for more complex uses cases. In this guide, we focus on the usage of the /bzz endpoint.  ","version":"Next","tagName":"h2"},{"title":"Upload a File​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#upload-a-file","content":" To upload data to the swarm, you must perform the following steps:  Fund your node's wallet with xBZZ.Purchase a batch of stamps with your xBZZ.Wait for the batch to propagate across the network.Upload your content, specifying the batch id so that Bee can attach stamps to your chunks.Download your content using your content's hash.  ","version":"Next","tagName":"h2"},{"title":"Purchasing Your Batch of Stamps​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#purchasing-your-batch-of-stamps","content":" In order to upload your data to swarm, you must agree to burn (spend) some of your xBZZ to signify to storer and fowarder nodes that this content is valued. Before you proceed to the next step, you must buy stamps! See this guide on how to purchase an appropriate batch of stamps.  ","version":"Next","tagName":"h2"},{"title":"Using Stamps to Upload a File​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#using-stamps-to-upload-a-file","content":" Once your Bee node is running, a HTTP API is enabled for you to interact with. The command line utility curl is a great way to interact with a Bee node's API. Swarm CLI alternative commands are also included as a more user-friendly way of interacting with your Bee node's API.  APISwarm CLI API​ First, let's check to see if the API is running as expected... curl http://localhost:1633 Ethereum Swarm Bee Once running, a file can be uploaded by making an HTTP POST request to the bzz endpoint of the Bee API. Here, you must specify your Batch ID in the Swarm-Postage-Batch-Id header, the file name in the name query parameter, and also pass the appropriate mime type in the Content-Type header. You may also wish to employ the erasure coding feature to add greater protection for your data, see erasure coding page for more details on its usage. curl -XPOST -H &quot;Swarm-Postage-Batch-Id: 54ba8e39a4f74ccfc7f903121e4d5d0fc40732b19efef5c8894d1f03bdd0f4c5&quot; -H &quot;Content-Type: text/plain&quot; -H &quot;Swarm-Encrypt: false&quot; -v --data-binary &quot;@test.txt&quot; localhost:1633/bzz danger Data uploaded to Swarm is always public. In Swarm, sensitive files must be encryptedbefore uploading to ensure their contents always remains private. When succesful, a JSON formatted response will be returned, containing a swarm reference or hash which is the address of the uploaded file, for example: { &quot;reference&quot;: &quot;22cbb9cedca08ca8d50b0319a32016174ceb8fbaa452ca5f0a77b804109baa00&quot; } Keep this address safe, as we'll use it to retrieve our content later on.  In Swarm, every piece of data has a unique address which is a unique and reproducible cryptographic hash digest. If you upload the same file twice, you will always receive the same hash. This makes working with data in Swarm super secure!  info If you are uploading a large file it is useful to track the status of your upload as it is processed into the network. To improve the user experience, learn how to follow the status of your upload. Once your file has been completely synced with the network, you will be able to turn off your computer and other nodes will take over to serve the data for you!  ","version":"Next","tagName":"h2"},{"title":"Download a File​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#download-a-file","content":" Once your file is uploaded to Swarm it can be easily downloaded.  APISwarm CLI API​ Uploaded files can be retrieved with a simple HTTP GET request. Substitute the hash in the last part of the URL with the reference to your own data. tip Make sure to include the trailing slash after the hash. curl -OJL http://localhost:1633/bzz/c02e7d943fbc0e753540f377853b7181227a83e773870847765143681511c97d/ You may even simply navigate to the URL in your browser: http://localhost:1633/bzz/22cb...aa00  ","version":"Next","tagName":"h2"},{"title":"Upload a Directory​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#upload-a-directory","content":" It is possible to use Bee to upload directories of files all at once.  tip Comfortable with nodeJS and JavaScript? Check out swarm-cli, a command line tool you can use to easily interact with your Bee node!  info If an uploaded directory contains an index.html file, when you navigate to the directory in a web browser it will automatically be served to users from our Swarm gateways as if it were a website hosted by a normal web server. Use this feature to host your unstoppable website on Swarm!  This feature makes use of the tar command line utility to package the directory into a single file that can then be uploaded to the Bee API for processing and distributed into the swarm for later retrieval.  caution GZIP compression is not supported in the current version of Bee, so make sure not to use the -z flag when using the tar command!  ","version":"Next","tagName":"h2"},{"title":"Upload the Directory Containing Your Website​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#upload-the-directory-containing-your-website","content":" First, use the tar command line utility to create an archive containing all the files of your directory. If uploading a website, we must take care to ensure that the index.html file is at the root of the directory tree.  tree my_website &gt; my_website ├── assets │ └── style.css ├── index.html └── error.html   Use the following command to ensure that the tar package maintains the correct directory structure:  cd my_website tar -cf ../my_website.tar . cd ..   Next, simply POST the tar file as binary data to Bee's dir endpoint, taking care to include the header Content Type: application/x-tar.  info In order to upload your data to swarm, you must agree to burn some of your xBZZ to signify to storer and fowarder nodes that the content is important. Before you progress to the next step, you must buy stamps! See this guide on how to purchase an appropriate batch of stamps.  curl \\ -X POST \\ -H &quot;Content-Type: application/x-tar&quot; \\ -H &quot;Swarm-Index-Document: index.html&quot; \\ -H &quot;Swarm-Error-Document: error.html&quot; \\ -H &quot;Swarm-Collection: true&quot; \\ -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; \\ --data-binary @my_website.tar http://localhost:1633/bzz   info For instances where a single page app has a JavaScript router that handles url queries itself, simply pass index.html as the error document. Bee will pass over control to the JavaScript served by the index.html file in the circumstance that a path does not yield a file from the manifest.  When the upload is successful, Bee will return a JSON document containing the Swarm Reference.  { &quot;reference&quot;: &quot;b25c89a401d9f26811680476619a1eb4a4e189e614bc6161cbfd8b343214917b&quot; }   Now, simply navigate your browser to view the reference using the bzz endpoint and your website will be served!  http://localhost:1633/bzz/b25c89a...214917b/  Other files are served at their relative paths, e.g:  http://localhost:1633/bzz/b25c89a...214917b/assets/style.css  Once your data has been fully processed into the network, you will then be able to retrieve it from any Bee node.  https://gateway.ethswarm.org/bzz/b25c89a...214917b/index.html  If you are not able to download your file from a different Bee node, you may be experiencing connection issues, see troubleshooting connectivity for assistance.  ","version":"Next","tagName":"h3"},{"title":"Public Gateways​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#public-gateways","content":" To share files with someone who isn't running a Bee node yet, simply change the host in the link to be one of our public gateways. Send the link to your friends, and they will be able to download the file too!  https://download.gateway.ethswarm.org/bzz/22cb...aa00/  ","version":"Next","tagName":"h2"},{"title":"Deferred and Direct Uploads​","type":1,"pageTitle":"Upload and Download","url":"/docs/develop/access-the-swarm/upload-and-download#deferred-and-direct-uploads","content":" By default your bee instance will handle uploads in a deferred manner, meaning that the data will be completely uploaded to your node locally before being then being uploaded to the Swarm network.  In contrast, for a direct upload, the data will be completely uploaded to the Swarm network directly.  If you want to upload directly to the network you have to set the swarm-deferred-upload header value to &quot;false&quot; in your request.  curl \\ -X POST \\ -H &quot;swarm-deferred-upload: false&quot; \\ -H &quot;Content-Type: application/x-tar&quot; \\ -H &quot;swarm-postage-batch-id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; \\ --data-binary @my_data.tar http://localhost:1633/bzz  ","version":"Next","tagName":"h2"},{"title":"Develop on Swarm","type":0,"sectionRef":"#","url":"/docs/develop/introduction","content":"","keywords":"","version":"Next"},{"title":"Learn How to Access the Swarm​","type":1,"pageTitle":"Develop on Swarm","url":"/docs/develop/introduction#learn-how-to-access-the-swarm","content":" Learn more about how to upload and download data from Swarm, host a website, secure your data's privacy with encryption, guarantee your data's persistence with erasure coding, manage postage stamp batches and more.  ","version":"Next","tagName":"h2"},{"title":"Explore Tools and Features​","type":1,"pageTitle":"Develop on Swarm","url":"/docs/develop/introduction#explore-tools-and-features","content":" Explore the tools and features available to help in your Swarm development journey. Learn more about chunk types, the bee-js Javascript library for Bee, prototyping with bee dev mode and test networks, use the Gateway Proxy tool to make your decentralised applications available to anyone with an internet connection, and more.  ","version":"Next","tagName":"h2"},{"title":"Review the API Reference Docs​","type":1,"pageTitle":"Develop on Swarm","url":"/docs/develop/introduction#review-the-api-reference-docs","content":" Find detailed information on all the endpoints available in the API reference documentation.  ","version":"Next","tagName":"h2"},{"title":"Contribute to the Development of the Bee Client​","type":1,"pageTitle":"Develop on Swarm","url":"/docs/develop/introduction#contribute-to-the-development-of-the-bee-client","content":" Join our efforts! Are you up to the challenge of helping to create Bee and the other incredible technologies that are being build on top of it? You are invited to contribute code to the Bee client or any of the other projects in Swarm'sEthersphere.  ","version":"Next","tagName":"h2"},{"title":"Community​","type":1,"pageTitle":"Develop on Swarm","url":"/docs/develop/introduction#community","content":" There is a vibrant and buzzing community behind Swarm - get involved in one of our group channels:  SwarmDiscordTwitter @ethswarmreddit channelBlog ","version":"Next","tagName":"h2"},{"title":"Buy a Batch of Stamps","type":0,"sectionRef":"#","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch","content":"","keywords":"","version":"Next"},{"title":"Fund your node's wallet.​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#fund-your-nodes-wallet","content":" In order to purchase a postage stamp batch, your node's Gnosis Chain address needs to be funded with sufficient xDAI to pay gas for transaction fees on Gnosis Chain as well as sufficient xBZZ to pay for the cost of the postage stamp batch itself.  xBZZ can be obtained from a variety of different centralized and decentralized exchanges. You can find more information on where to obtain xBZZ on the Ethswarm homepage.  xDAI can be obtained from a wide range of centralized and decentralized exchanges. See this list of exchanges from the Gnosis Chain documentation to get started.  ","version":"Next","tagName":"h2"},{"title":"Buying a stamp batch​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#buying-a-stamp-batch","content":" When interacting with the Bee API directly, amount and depth are passed as path parameters:  curl -s -XPOST http://localhost:1633/stamps/&lt;amount&gt;/&lt;depth&gt;   And with Swarm CLI, they are set using option flags:  swarm-cli stamp buy --depth &lt;depth value&gt; --amount &lt;amount value&gt;   APISwarm CLI API​ curl -s -XPOST http://localhost:1633/stamps/100000000/20 { &quot;batchID&quot;: &quot;8fcec40c65841e0c3c56679315a29a6495d32b9ed506f2757e03cdd778552c6b&quot;, &quot;txHash&quot;: &quot;0x51c77ac171efd930eca8f3a77e3fcd5aca0a7353b84d5562f8e9c13f5907b675&quot; }   info Once your batch has been purchased, it will take a few minutes for other Bee nodes in the Swarm to catch up and register your batch. Allow some time for your batch to propagate in the network before proceeding to the next step.  ","version":"Next","tagName":"h2"},{"title":"Setting stamp batch parameters and options​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#setting-stamp-batch-parameters-and-options","content":" When purchasing a batch of stamps there are several parameters and options which must be considered. The depth parameter will control how many chunks can be uploaded with a batch of stamps. The amount parameter determines how much xBZZ will be allocated per chunk, and therefore also controls how long the chunks will be stored. While the immutable header option sets the batch as either mutable or immutable, which can significantly alter the behavior of the batch utilisation (more details below).  ","version":"Next","tagName":"h2"},{"title":"Choosing depth​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#choosing-depth","content":" caution The minimum value for depth is 17, however a higher depth value is recommended for most use cases due to the mechanics of stamp batch utilisation. See the depths utilisation table to help decide which depth is best for your use case.  One notable aspect of batch utilisation is that the entire batch is considered fully utilised as soon as any one of its buckets are filled. This means that the actual amount of chunks storable by a batch is less than the nominal maximum amount.  See the postage stamp contract page for a more complete explanation of how batch utilisation works and a table with the specific amounts of data which can be safely uploaded for each depth value.  ","version":"Next","tagName":"h3"},{"title":"Choosing amount​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#choosing-amount","content":" caution The minimum amount value for purchasing stamps is required to be at least enough to pay for 24 hours of storage. To find this value multiply the lastPrice value from the postage stamp contract times 17280 (the number of blocks in 24 hours). You can also use the calculator below. This requirement is in place in order to prevent spamming the network.  The amount parameter determines how much xBZZ is assigned per chunk for a postage stamp batch. You can use the calculators below to find the appropriate amount value for your target duration of storage and can also preview the price. For more information see the postage stamp batch contract page where a more complete description is included.  ","version":"Next","tagName":"h3"},{"title":"Mutable or Immutable?​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#mutable-or-immutable","content":" Depending on the use case, uploaders may desire to use mutable or immutable batches. The fundamental difference between immutable and mutable batches is that immutable batches become unusable once their capacity is filled, while for mutable batches, once their capacity is filled, they may continue to be used, however older chunks of data will be overwritten with the newer once over capacity. The default batch type is immutable. In order to set the batch type to mutable, the immutable header should be set to false. See this section on postage stamp batch utilisation to learn more about mutable vs immutable batches, and about which type may be right for your use case.  ","version":"Next","tagName":"h3"},{"title":"Calculators​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#calculators","content":" The following postage batch calculators allow you to conveniently find the depth and amount values for a given storage duration and storage volume, or to find the storage duration and storage volume for a given depth and amount. The results will display the cost in xBZZ for the postage batch. The current pricing information is sourced from the Swarmscan API and will vary over time.  info The 'effective volume' is the volume of data that can safely stored for each storage depth. The 'theoretical max volume' is significantly lower than the effective volume at lower depths and the two values trend towards the same value at higher depths. The lowest depth with an effective volume above zero is 22, with an effective depth of 4.93 GB. Lower depth values can be used for smaller uploads but do not come with the same storage guarantees. Learn more here.  ","version":"Next","tagName":"h2"},{"title":"Depth & Amount to Time & Volume Calculator​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#depth--amount-to-time--volume-calculator","content":" Depth:Amount (current minimum is 0 PLUR):Calculate  ","version":"Next","tagName":"h3"},{"title":"Time & Volume to Depth & Amount Calculator​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#time--volume-to-depth--amount-calculator","content":" The recommended depth in this calculator's results is the lowest depth value whose effective volume is greater than the entered volume.  Time:HoursDaysWeeksYears Volume:MegabytesGigabytesTerabytesPetabytes Calculate  ","version":"Next","tagName":"h3"},{"title":"Viewing Stamps​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#viewing-stamps","content":" To check on your stamps, send a GET request to the stamp endpoint.  APISwarm CLI API​ curl http://localhost:1633/stamps { &quot;stamps&quot;: [ { &quot;batchID&quot;: &quot;f4b9830676f4eeed4982c051934e64113dc348d7f5d2ab4398d371be0fbcdbf5&quot;, &quot;utilization&quot;: 0, &quot;usable&quot;: true, &quot;label&quot;: &quot;&quot;, &quot;depth&quot;: 20, &quot;amount&quot;: &quot;100000000&quot;, &quot;bucketDepth&quot;: 16, &quot;blockNumber&quot;: 30643611, &quot;immutableFlag&quot;: true, &quot;exists&quot;: true, &quot;batchTTL&quot;: 20588, &quot;expired&quot;: false } ] }   info It is not possible to reupload unencrypted content which was stamped using an expired postage stamp.  ","version":"Next","tagName":"h2"},{"title":"Checking the remaining TTL (time to live) of your batch​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#checking-the-remaining-ttl-time-to-live-of-your-batch","content":" info At present, TTL is a primitive calculation based on the current storage price and the assumption that storage price will remain static in the future. As more data is uploaded into Swarm, the price of storage will begin to increase. For data that it is important to keep alive, make sure your batches have plenty of time to live!  In order to make sure your batch has sufficient remaining balance to be stored and served by nodes in its area of responsibility, you must regularly check on its time to live and act accordingly. The time to live is the number of seconds before the chunks will be considered for garbage collection by nodes in the network.  The remaining time to live in seconds is shown in the API in the returned json object as the value for batchTTL, and with Swarm CLI you will see the formatted TTL as the TTL value.  APISwarm CLI API​ curl http://localhost:1633/stamps { &quot;stamps&quot;: [ { &quot;batchID&quot;: &quot;f4b9830676f4eeed4982c051934e64113dc348d7f5d2ab4398d371be0fbcdbf5&quot;, &quot;utilization&quot;: 0, &quot;usable&quot;: true, &quot;label&quot;: &quot;&quot;, &quot;depth&quot;: 20, &quot;amount&quot;: &quot;100000000&quot;, &quot;bucketDepth&quot;: 16, &quot;blockNumber&quot;: 30643611, &quot;immutableFlag&quot;: true, &quot;exists&quot;: true, &quot;batchTTL&quot;: 20588, &quot;expired&quot;: false } ] }   ","version":"Next","tagName":"h2"},{"title":"Top up your batch​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#top-up-your-batch","content":" danger Don't let your batch run out! If it does, you will need to restamp and resync your content.  If your batch is starting to run out, or you would like to extend the life of your batch to protect against storage price rises, you can increase the batch TTL by topping up your batch using the stamps endpoint, passing in the relevant batchID into the HTTP PATCH request.  APISwarm CLI API​ curl -X PATCH &quot;http://localhost:1633/stamps/topup/6d32e6f1b724f8658830e51f8f57aa6029f82ee7a30e4fc0c1bfe23ab5632b27/10000000&quot;   ","version":"Next","tagName":"h2"},{"title":"Dilute your batch​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#dilute-your-batch","content":" In order to store more data with a batch of stamps, you must &quot;dilute&quot; the batch. Dilution simply refers to increasing the depth of the batch, thereby allowing it to store a greater number of chunks. As dilution only increases the the depth of a batch and does not automatically top up the batch with more xBZZ, dilution will decrease the TTL of the batch. Therefore if you wish to store more with your batch but don't want to decrease its TTL, you will need to both dilute and top up your batch with more xBZZ.  APISwarm CLI API​ Here we call the /stamps endpoint and find a batch with depth 24 and a batchTTL of 2083223 which we wish to dilute: curl http://localhost:1633/stamps { &quot;stamps&quot;: [ { &quot;batchID&quot;: &quot;0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5&quot;, &quot;utilization&quot;: 0, &quot;usable&quot;: true, &quot;label&quot;: &quot;&quot;, &quot;depth&quot;: 24, &quot;amount&quot;: &quot;10000000000&quot;, &quot;bucketDepth&quot;: 16, &quot;blockNumber&quot;: 29717348, &quot;immutableFlag&quot;: false, &quot;exists&quot;: true, &quot;batchTTL&quot;: 2083223, &quot;expired&quot;: false } ] } Next we call the dilute endpoint to increase the depth of the batch using the batchID and our new depth of 26: curl -s -XPATCH http://localhost:1633/stamps/dilute/0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5/26 And a txHash of our successful transaction: { &quot;batchID&quot;: &quot;0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5&quot;, &quot;txHash&quot;: &quot;0x298e80358b3257292752edb2535a1cd84440c074451b61f78fab349aea4962b7&quot; } And finally we use the /stamps endpoint again to confirm the new depth and decreased batchTTL: curl http://localhost:1633/stamps We can see the new depth of 26 and a decreased batchTTL of 519265. { &quot;stamps&quot;: [ { &quot;batchID&quot;: &quot;0e4dd16cc435730a25ba662eb3da46e28d260c61c31713b6f4abf8f8c2548ae5&quot;, &quot;utilization&quot;: 0, &quot;usable&quot;: true, &quot;label&quot;: &quot;&quot;, &quot;depth&quot;: 26, &quot;amount&quot;: &quot;10000000000&quot;, &quot;bucketDepth&quot;: 16, &quot;blockNumber&quot;: 29717348, &quot;immutableFlag&quot;: false, &quot;exists&quot;: true, &quot;batchTTL&quot;: 519265, &quot;expired&quot;: false } ] }   ","version":"Next","tagName":"h2"},{"title":"Stewardship​","type":1,"pageTitle":"Buy a Batch of Stamps","url":"/docs/develop/access-the-swarm/buy-a-stamp-batch#stewardship","content":" The stewardship endpoint in combination with pinning can be used to guarantee that important content is always available. It is used for checking whether the content for a Swarm reference is retrievable and for re-uploading the content if it is not.  An HTTP GET request to the stewardship endpoint checks to see whether the content for the specified Swarm reference is retrievable:  info stewardship is not currently supported by Swarm CLI  curl &quot;http://localhost:1633/stewardship/c0c2b70b01db8cdfaf114cde176a1e30972b556c7e72d5403bea32e c0207136f&quot;   { &quot;isRetrievable&quot;: true }   If the content is not retrievable, an HTTP PUT request can be used to re-upload the content:  curl -X PUT &quot;http://localhost:1633/stewardship/c0c2b70b01db8cdfaf114cde176a1e30972b556c7e72d5403bea32ec0207136f&quot;   Note that for the re-upload to succeed, the associated content must be available locally, either pinned or cached. Since it isn't easy to predict if the content will be cached, for important content pinning is recommended. ","version":"Next","tagName":"h2"},{"title":"Protocols","type":0,"sectionRef":"#","url":"/docs/develop/contribute/protocols","content":"","keywords":"","version":"Next"},{"title":"Protocols specifications​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#protocols-specifications","content":" An attempt to describe the desired behaviour of the main DISC protocols, which corresponds to the rational choice a node should make in order to maximize its profitability.  A communications protocol is a set of formal rules describing how to transmit or exchange data, especially across a network.  We'll begin by describing the Hive, Retrieval, PushSync, PullSync communication protocols as well as the Kademlia topology component.  ","version":"Next","tagName":"h2"},{"title":"Proposal​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#proposal","content":" The purpose of this document is to specify these concepts:  A pattern of exchange of messages which in semantic units corresponds to the high level function of what a node accomplishes in an exchange.Strategies of behaviour that a node should adopt in situations like network disconnects, timeouts, invalid chunks etc.An incentivisation strategy such that constructive behaviour should be rewarded and encouraged while deviating from the protocol rules should result in punishing measures.  ","version":"Next","tagName":"h2"},{"title":"Hive​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#hive","content":" The Hive protocol defines how nodes exchange information about their peers in order to reach and maintain a saturated Kademlia connectivity.  The exchange of this information happens upon connection, however nodes can broadcast newly received peers to their peers during the lifetime of connection.  While the simplest approach is to share all known peers (during an exchange) it might be more optimal to narrow down to a useful subset of peers - for instance all the peers up to a certain depth or belonging to a certain bin.  The exchanged information includes both overlay and underlay addresses of the known remote peers.  The overlay address serves to select peers to achieve the connectivity pattern needed for the desired network topology, while the underlay address is needed to establish the peer connections by dialing selected peers.  Upon receiving a peers message, nodes should store the peer information in their address book, i.e., a data structure containing info about peers known to the node that is meant to be persisted across sessions.  ","version":"Next","tagName":"h2"},{"title":"Appendix​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#appendix","content":" The protobuf definitions  // Copyright 2020 The Swarm Authors. All rights reserved. // Use of this source code is governed by a BSD-style // license that can be found in the LICENSE file. syntax = &quot;proto3&quot;; package hive; option go_package = &quot;pb&quot;; message Peers { repeated BzzAddress peers = 1; } message BzzAddress { bytes Underlay = 1; bytes Signature = 2; bytes Overlay = 3; bytes Nonce = 4; }   ","version":"Next","tagName":"h3"},{"title":"Kademlia​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#kademlia","content":" Kademlia topology is a specific connectivity pattern used by all DISC protocols and its purpose is to route messages between nodes in a network using overlay addressing.  The message routing happens in such a fashion that with every network hop we will get closer to the target node, specifically at half of the distance covered by the previous hop.  Swarm uses the recursive/forwarding style of Kademlia.  This approach implies that every forwarding node - once it received a request - will keep an in-memory record that captures the request related information (requester, time of the request etc.) until the request is satisfied, rejected or times out.  Because a forwarder can not reliably tell how much time the downstream peer will need to satisfy the request - the choice of a reasonable value for waiting period is a point of contention.  It is constrained by these factors:  keeping the in-memory record for too long means that there's going to be a limit on how many concurrent requests a peer can keep &quot;in-flight&quot;, because memory is limited.if the peer decides to time out prematurely (while downstream peers are still processing the request) then the effort of all the downstream peers will be wasted.we should distinguish between unsolicited chunks and chunks that we received from the downstream after we stopped waiting for the response (timed out). After a certain period of time all responses will be treated the same because of the need to free the allocated resources (the in-memory record).  Conversely the downstream should be informed when the upstream is no longer interested in the previously sent request, so it could free the used resources. This way the downstream won't return the chunk after the request has timed out, risking being punished.  ","version":"Next","tagName":"h2"},{"title":"Push and pull: chunk retrieval and syncing​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#push-and-pull-chunk-retrieval-and-syncing","content":" Swarm involves a direct storage scheme of fixed size where chunks are stored on nodes with address corresponding to the chunk address.  The syncing protocols act in such a way that they reach those neighbourhoods whenever a request is initiated.  Such a route is sure to exist as a result of the Kademlia topology of keep-alive connections between peers.  The process of relaying the request from the initiator to the storer is called forwarding and also the process of passing the chunk data along the same path is called backwarding.  Conversely - Backwarding and Forwarding are both notions defined on a keep alive network of peers as strategies of reaching certain addresses.  If we zoom into a particular node in the forwarding (or backwarding) path we see the following strategy:  Receive a requestDecide who to forward the request to (decision strategy)Have a way to match the the response to the original request.  The crucial step is the second one - strategy of choosing the peer to forward the request to and how they react to failure like stream closure or nodes dropping offline or closing the protocol connection and whether we proactively initiate several requests to peers.  The last step does not apply for the storer nodes, since they do not forward the request but they satisfy it.  The key element of these notions is that the decision about the next action is being done on the node level, which will select the next peers(s) and delegate them with handling the request.  The simplest representation of this would be a recursive algorithm that with every iteration gets closer to the target address and stops when it runs out of peers or successfully reaches the target node.  ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#requirements","content":" We need a way to determine the &quot;best&quot; candidate peer to forward the request to, and if this option fails, continue with the &quot;next-best&quot; candidate until we exhaust available peers. The decision of picking the &quot;best&quot; peer is delegate to an overlay driver that has the best knowledge of this peer's past history and performance and topology structure.We need a strategy of parallelisation of requests that we pass downstream, where appropriate. Parallel requests to different peers allow us increase the chances of successfully syncing the chunk but it comes with the cost of using our bandwidth allowance, so it's imperative to zoom in on an optimal balance between the two.We need a way to ensure that when we issue a syncing request we don’t end up in a situation when this request comes back around to us, wasting network resources.In the case when we are a &quot;forwarder&quot; node, we might consider a decision strategy on whether we want to cache the chunk in the event of a repeated request.To every forwarding/backwarding exchange we attach an incentivisation action that would take into account variables like the success of the action and the cost of performing the action.We need to design the optimal incentivisation scheme, to determine the optimal payment/settlement frequency and correctness of computation of the payment/charged amount. This also applies to both chunk storage scheme and the relayed request-response scheme.We need to have a sensible strategy when it comes to waiting for a peer to respond to our request; as a forwarder, we want to make our best effort to sync the chunk but without waiting for an excessive amount of time, which would lead to waste of resources.When receiving a response to an expired request - or we are unable to conclude if such a request has ever been issued - punishing measures should be imposed on the upstream peer.  ","version":"Next","tagName":"h3"},{"title":"Incentivisation strategy​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#incentivisation-strategy","content":" An incentivisation strategy should be put in place in such way that it encourages honest collaboration between nodes.  This implies that a given peer will make the best effort to satisfy any request while not allowing any abuse and waste of its resources.  Having an accounting component that would keep track of the exchange activity between peers ensures that we do not allow excessive freeloading from the misbehaving peers.  Having a granular punishment strategy ensures that the peers who misbehave (perhaps due to network latencies) will not be sanctioned to the same extent as peers who engage in grave protocol breaches, but are given a chance to &quot;clean up their act&quot;.  ","version":"Next","tagName":"h3"},{"title":"Retrieval​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#retrieval","content":" The retrieval of a chunk is a process which fetches a given chunk from the network by its address.  It follows the general semantics of the chunk syncing described above and follows the same network path as the push sync protocol, but in reverse.  ","version":"Next","tagName":"h2"},{"title":"Protocol breach​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#protocol-breach","content":" Receiving a repeated request for a non-existent chunk should lead to rate limiting in order to discourage resource wasteful actions.Receiving a response in the form of an invalid chunk constitutes a protocol breach and punishing measures are being taken against the peer at fault.  step I 1) check if this exact request has been received within last N minutes and it is for a non-existent chunk 2) if such request is found - take punishing measures against the requester (blocklisting) 3) request a peer from Kademlia 4) request chunk from the peer 5) if the peer does not return a valid chunk - go back to step 3 6) if the chunk is found and valid, log the event details in the local state and return the chunk to the requester 7) consider caching the chunk in case there might be a repeated request for it Error states - if we exhaust the list of peers (candidates) for this action, return a 'failure to get chunk' response to the requester. We might consider increasing our peer connections pool to avoid such situation in the future - if we are able to conclude that the chunk is non-existent (TBD) we return 'chunk not found' and consider rate limiting measures against the requester. - if we ran out of allowed time while looking for the chunk we return a 'timeout' response to the requester - if the chunk is retrieved successfully but does not pass validation, take punishing measures against the peer (blocklisting). - if the attempt fails, log the relevant attempt details in the local state and repeat the attempt against a new peer - if the peer times out responding to our request we log the attempt details and repeat step II against a new peer   ","version":"Next","tagName":"h3"},{"title":"Request chunk - sequence diagram​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#request-chunk---sequence-diagram","content":"   ","version":"Next","tagName":"h3"},{"title":"Request chunk - flow diagram​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#request-chunk---flow-diagram","content":"   ","version":"Next","tagName":"h3"},{"title":"Appendix​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#appendix-1","content":" The protobuf definitions  // Copyright 2020 The Swarm Authors. All rights reserved. // Use of this source code is governed by a BSD-style // license that can be found in the LICENSE file. syntax = &quot;proto3&quot;; package retrieval; option go_package = &quot;pb&quot;; message Request { bytes Addr = 1; } message Delivery { bytes Data = 1; bytes Stamp = 2; }   ","version":"Next","tagName":"h3"},{"title":"Pushsync​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#pushsync","content":" Pushsync protocol is responsible for ensuring delivery of the chunk to its prescribed storer after it has been uploaded to any arbitrary node.  It works in a similar way to the Retrieval protocol in the sense that the chunk is being passed to a peer whose address is closest to the chunk address and a custody receipt is received in response.  Then the same process is repeated until the chunk eventually reaches the storer node located in a certain &quot;neighbourhood&quot;.  Since the Pushsync protocol is a &quot;mirror&quot; version of the Retrieval protocol - it ensures that a successfully uploaded chunk is retrievable from the same &quot;neighbourhood&quot; by the virtue of the fact that nodes in a neighbourhood are connected to each other.  ","version":"Next","tagName":"h2"},{"title":"Multiplexing​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#multiplexing","content":" Multiplexing is a recommended node strategy for the push sync protocol that involves early replication and opportunistic receipting. Its intention is to reduce the dependence on single closest nodes and to improve on network performance, i.e., push sync success rate, bandwidth overhead and latency.  Context​  The current implementation of the push sync protocol aims to push a chunk to the closest node in the neighbourhood which is then supposed to give out a receipt.  This is motivated by the retrieval protocol that aims to find the chunk at this closest node.  When the closest node hands out a receipt, this node also replicates the chunk to 3 peers in the neighbourhood which are further away from the chunk than him.  This replication takes place to ensure that the chunk is not lost when the closest node shuts down before the chunk is not pull-sync'ed and to speed up the spreading of the chunk in the neighbourhood, in advance of pull sync.  Problem​  Treating the closest node as a single target of push sync is fragile. If this peer has a badly-performing blockchain backend, slow or incomplete connectivity or is malicious, it may not spread the chunk and/or does not respond with a receipt.  In this case, currently, the originator must retry the entire push-sync operation many times before the other peers within neighbourhood recognise the improper behaviour.  In-neighbourhood retries are ideally avoided because such retries might cause the downstream timeouts to expire.  In case of incomplete connectivity, the push sync protocol can end at a different branch of the neighbourhood than the retrieval protocol--causing the chunk not to be retrievable.  It should be noted that the pull sync protocol (may?) remedies this problem with a small time-delay.  Multiplexing: early replication within neighbourhood​  The first node in the push sync forward chain that falls within the neighbourhood acts as multiplexer, i.e., it forwards the request to a number of closest nodes and responds with a self-signed receipt.  Thus in achieving retrievability and security via early replication, we do not critically rely on the closest node to be available any more.  Push sync flow​  We define the different roles peers have as part of the push sync forwarding chain:  originator -- creator of the requestforwarder -- closer to the chunk than the originator, further away away than the 1-before node.multiplexer -- first node in the forward chain who is in the neighbourhoodclosest nodes -- according to the downstream node (usually the multiplexer), within the n closest nodes to the chunks (not including self)  we describe the envisioned flow of push sync by describing the intended behaviour strategy of the various roles.  originator sends chunk to a peer closer to the chunk.forwarder(s) forwards chunk that ends up with a node already within the neighbourhood that acts as multiplexerThe multiplexer concurrently sends the chunk to 3 closest nodes attaching a multiplexing-list as part of the protocol message. At the same time they respond to their upstream peer with a self-signed receipt (unless the multiplexer is itself the originator).Non-multiplexing closest nodes, i.e., nodes in the neighbourhood that receive the pushsync message from a not-closest neighbour with a multiplexing list included, validate whether, based on their view, the multiplexing list covers all 3 closest nodes (potentially including the peer and/or the upstream peer themselves). If not, the node forwards the chunk to the peers left out. These peers are also added to the multiplexing list received from upstream and the extended list is attached with the chunk pushed.  If the multiplexer node does not know a closest peer p but several of its chosen closest nodes do, then that node p will receive the same pushsynced chunk multiple times  ","version":"Next","tagName":"h3"},{"title":"Appendix​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#appendix-2","content":" The protobuf definitions  // Copyright 2020 The Swarm Authors. All rights reserved. // Use of this source code is governed by a BSD-style // license that can be found in the LICENSE file. syntax = &quot;proto3&quot;; package pushsync; option go_package = &quot;pb&quot;; message Delivery { bytes Address = 1; bytes Data = 2; bytes Stamp = 3; } message Receipt { bytes Address = 1; bytes Signature = 2; bytes Nonce = 3; }   ","version":"Next","tagName":"h3"},{"title":"Pullsync​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#pullsync","content":" While the other described protocols are request scoped, Pullsync is a subscription type protocol.  It's worth mentioning that the chunks that are being synchronized between nodes always travel alongside their corresponding postage stamps.  Pullsync's role is to help synchronization of the chunks between neighbourhood nodes. It bootstraps new nodes by filling up their storage with the chunks in range of their storage radius and also ensures eventual consistency - by making sure that the chunks will gradually migrate to their storer nodes.  There are two kinds of syncing:  historical syncing: catching up with content that arrived to relevant neighbourhood before this session started (after an outage or for completely new nodes).live syncing: fetching the chunks that are received after the session has started.  The chunks are served in batches (ordered by timestamp) and they cover contiguous ranges.  The downstream peers coordinate their syncing by requesting ranges from the upstream with the help of the &quot;interval store&quot; - to keep track of which ranges are left to be synchronized.  Because live syncing happens in sessions - it is inevitable that after a session is completed - the downstream peer disconnects and will be missing chunks that arrive later.  For this purpose the downstream peer will make a note about the timestamp of the last synced chunk on disconnect.  The point of the interval based approach is to cover those gaps that inevitably arise in between syncing sessions.  To save bandwidth, before the contents of the chunk is being sent over the wire, the upstream will sent a range of chunk addresses for approval. If the downstream decides that some (or all) addresses are desired - a confirmation message is sent to the upstream, to which it responds with the chunks mentioned in the request.    ","version":"Next","tagName":"h2"},{"title":"Appendix​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#appendix-3","content":" The protobuf definitions  // Copyright 2020 The Swarm Authors. All rights reserved. // Use of this source code is governed by a BSD-style // license that can be found in the LICENSE file. syntax = &quot;proto3&quot;; package pullsync; option go_package = &quot;pb&quot;; message Syn {} message Ack { repeated uint64 Cursors = 1; } message Get { int32 Bin = 1; uint64 Start = 2; } message Chunk { bytes Address = 1; bytes BatchID = 2; } message Offer { uint64 Topmost = 1; repeated Chunk Chunks = 2; } message Want { bytes BitVector = 1; } message Delivery { bytes Address = 1; bytes Data = 2; bytes Stamp = 3; }   ","version":"Next","tagName":"h3"},{"title":"Peer rating​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#peer-rating","content":" When choosing a peer in relation to a given address - in addition to the distance between them - the Kademlia component will take into account two other factors:  the historical performance of the given peer, both in terms of latencies and past occurrences of protocol misalignments.the accounting aspect, peers with whom we have higher credit will be preferred.we should also prioritise those downstream peers that managed to produce responses in a previously computed amount of time (that would take into consideration the average time needed for a hop multiplied by the expected number of hops needed to reach a target neighbourhood).  Kademila should be indexing peers by their proximity order and peers rating in order to prioritize peers based on their expected performance.  ","version":"Next","tagName":"h3"},{"title":"Decision strategy​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#decision-strategy","content":" An optimal decision strategy will take into account both proximity order and peer rating to select (out of all connected peers) the best one to pass down the request.  At the implementation level the Kademlia component will offer (in exchange for a given address) a stateful iterator that the client (protocol) will use to get the &quot;next-best&quot; peer.  ","version":"Next","tagName":"h3"},{"title":"Transport​","type":1,"pageTitle":"Protocols","url":"/docs/develop/contribute/protocols#transport","content":" A reliable network transport is required for the proper functionality of DISC protocols.  It can be a distinct component whose responsibilities would be ensuring delivery, re-tries on network issues and timeouts and optimal use of network related resources.  One example of usage for such a component could be embedding into the Kademlia driver so that the topology component is only concerned with overlay related operations, abstracting away any low level transport concerns. ","version":"Next","tagName":"h3"},{"title":"Access Control","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/act","content":"","keywords":"","version":"Next"},{"title":"Upload​","type":1,"pageTitle":"Access Control","url":"/docs/develop/tools-and-features/act#upload","content":" Uploading data without ACT to the netwkork remains unchanged.  To upload with ACT use the act and act-history-address flags following the upload command:  swarm-cli upload test.txt --act --stamp $stamp_id --act-history-address $swarm_history_address   Here act indicates that the file provided shall be uploaded using ACT. The act-history-address flag is the reference of the historical version of the ACT. It can be ommitted, in which case the data is uploaded to a new history. If provided, then the data will be uploaded to that history as the latest version. In both cases the timestamp of the upload is taken as the key of the history entry. If the provided act-history-address is invalid then the request will fail with a not found error.  The response returns the newly created refrence encrypted with ACT and the header contains history reference.  ","version":"Next","tagName":"h3"},{"title":"Download​","type":1,"pageTitle":"Access Control","url":"/docs/develop/tools-and-features/act#download","content":" Downloading data which was uploaded without ACT from the netwkork remains unchanged.  To download with ACT use the act, act-publisher, act-timestamp and act-history-address flags following the download command:  swarm-cli download $swarm_hash test.txt --act --act-history-address $swarm_history_address --act-publisher $public_key --timestamp $timestamp   Here act indicates that the swarm_hash shall be decrypted using the content publisher's public key as act-publisher and the lookup table mentioned above. The act-history-address flag is the reference of the historical version of the ACT based on the timestamp provided, however the act-timestamp flag can be ommitted in which case the current timestamp is used.  If the act-history-address or act-publisher flags are omitted then the request is treated as a &quot;usual&quot; download. If the data was uploaded with ACT and we try to download it without the ACT flags then the request will fail with a not found error.  ","version":"Next","tagName":"h3"},{"title":"Grantee management​","type":1,"pageTitle":"Access Control","url":"/docs/develop/tools-and-features/act#grantee-management","content":" Updating a grantee list literally means patching a json file containing the list of grantee swarm public keys.  Create​  A brand new grantee list can be created using the following command:  swarm-cli grantee create grantees.json --stamp $stamp_id   where grantees.json shall contain the key grantees with the list of public keys:  { &quot;grantees&quot;: [ &quot;03ec55e9fb2aefb8600f69142abaad79311516c232b28919d66efb4d41bce15bfa&quot;, &quot;03fdcab22b455ce08a481d929a4cb9f447752545818eded1ad1785c51581e822c6&quot; ] }   The response returns the newly created and encrypted grantee list and the history reference. Only the publisher can decrypt and therefore access the list. If act-history-address is provided then the grantee list is uploaded as the newest version under that history.  Patch​  swarm-cli grantee patch grantees-patch.json --reference $grantee_reference --history $grantee_history_reference --stamp $stamp_id   where grantees.json shall contain the keys add and revoke with the list of public keys for granting and revoking access, respectively:  { &quot;add&quot;: [&quot;03fdcab22b455ce08a481d929a4cb9f447752545818eded1ad1785c51581e822c6&quot;], &quot;revoke&quot;: [ &quot;03ec55e9fb2aefb8600f69142abaad79311516c232b28919d66efb4d41bce15bfa&quot; ] }   The reference flag indicates the already existing encrypted grantee list reference that needs to be updated. The grantee_history_reference indicates the reference of historical version of the list, where the encrpyted list reference is added as a metadata to the history entry with the key &quot;encryptedglref&quot;  Limitation: If an update is called again within a second from the latest upload/update of a grantee list, then mantaray save fails with an invalid input error, because the key (timestamp) already exists, hence a new fork is not created.  Get​  As stated above, only the publisher can decrypt and therefore access the list with the following command:  swarm-cli grantee get $grantee_reference   which simply returns the latest version of the list.  Non-authorized access causes the request to fail with a not found error. For each of the above operations, if the provided act-history-address or reference is invalid then the request will fail with a not found error. ","version":"Next","tagName":"h3"},{"title":"Developer mode","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/bee-dev-mode","content":"","keywords":"","version":"Next"},{"title":"Configuration options​","type":1,"pageTitle":"Developer mode","url":"/docs/develop/tools-and-features/bee-dev-mode#configuration-options","content":" It accepts the same configuration options as a normal bee but it will ignore the ones that are not relevant (accounting, networking, blockchain etc). ","version":"Next","tagName":"h2"},{"title":"Bee JS","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/bee-js","content":"Bee JS Bee-js is Bee's complementary JavaScript library. It is the technology underpinning the swarm-cli and bee-dashboard tools and is a powerful tool for building completely decentralised apps. See the bee-js documentation for detailed information on using and installing the library.","keywords":"","version":"Next"},{"title":"Chunk Types","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/chunk-types","content":"","keywords":"","version":"Next"},{"title":"Content Addressed Chunks​","type":1,"pageTitle":"Chunk Types","url":"/docs/develop/tools-and-features/chunk-types#content-addressed-chunks","content":" Content addressed chunks are chunks whose addresses are determined by the BMT hashing algorithm. This means you can be sure that all content addressed chunks content is already verified - no more need to check md5 hashes of your downloaded data!  warning To be able trust your data, you must run your own Bee node that automatically verifies data, using gateways puts your trust in the gateway operators.  ","version":"Next","tagName":"h2"},{"title":"Trojan Chunks​","type":1,"pageTitle":"Chunk Types","url":"/docs/develop/tools-and-features/chunk-types#trojan-chunks","content":" Trojan chunks are a special version of content addressed chunks that have been 'mined' so that their natural home is in a particular area of the Swarm. If the destination node is in the right neighbourhood, it will be able to receive and decrypt the message. See PSS for more information, or check out the bee-js bindings.  ","version":"Next","tagName":"h2"},{"title":"Single Owner Chunks​","type":1,"pageTitle":"Chunk Types","url":"/docs/develop/tools-and-features/chunk-types#single-owner-chunks","content":" Single Owner Chunks are distinct from Trojan and Content Addressed Chunks and are the only other type of chunk which is allowed in Swarm. These chunks represent part of Swarm's address space which is reserved just for your personal Ethereum key pair! Here you can write whatever you'd please. Single Owner Chunks are the technology that powers Swarm's feeds, but they are capable of much more! Look out for more chats about this soon, and for more info read The Book of Swarm.  ","version":"Next","tagName":"h2"},{"title":"Custom Chunk Types​","type":1,"pageTitle":"Chunk Types","url":"/docs/develop/tools-and-features/chunk-types#custom-chunk-types","content":" Although all chunks must satisfy the constraints of either being addressed by the BMT hash of their payload, or assigned by the owner of an Ethereum private key pair, so much more is possible. How else can you use the DISC to distribute and store your data? We're excited to see what you come up with! 💡  Share your creations in the #develop-on-swarm channel of our Discord Server. ","version":"Next","tagName":"h2"},{"title":"Feeds","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/feeds","content":"","keywords":"","version":"Next"},{"title":"What are Feeds?​","type":1,"pageTitle":"Feeds","url":"/docs/develop/tools-and-features/feeds#what-are-feeds","content":" A feed is a collection of Single Owner Chunks with predicatable addresses. This enables creators to upload pointers to data so that consumers of the feed are able to find the data in Swarm using only an Ethereum address and Topic ID.  ","version":"Next","tagName":"h3"},{"title":"Creating and Updating a Feed​","type":1,"pageTitle":"Feeds","url":"/docs/develop/tools-and-features/feeds#creating-and-updating-a-feed","content":" In order to edit a feed, you will need to sign your chunks using an Ethereum keypair. For the intrepid, check out the The Book of Swarm on precise details on how to do this. For the rest of us, both bee-jsand swarm-cli provide facilities to achieve this using JavaScript and a node-js powered command line tool respectively.  ","version":"Next","tagName":"h3"},{"title":"No More ENS Transaction Charges​","type":1,"pageTitle":"Feeds","url":"/docs/develop/tools-and-features/feeds#no-more-ens-transaction-charges","content":" Swarm's feeds provide the ability to update your immutable content in a mutable world. Simply reference your feed's manifest address as the content hash in your ENS domain's resolver, and Bee will automatically provide the latest version of your website.  ","version":"Next","tagName":"h3"},{"title":"Use Cases for Feeds​","type":1,"pageTitle":"Feeds","url":"/docs/develop/tools-and-features/feeds#use-cases-for-feeds","content":" Feeds are a hugely versatile data structure.  Key Value Store​  Use bee-js to use feeds to store values as a simple key value store in your JavaScript application. No more need for servers and databases!  Store the History of a File​  Use swarm-cli to store a file at the same location, and update whenever you like without changing the address. ","version":"Next","tagName":"h3"},{"title":"Gateway Proxy","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/gateway-proxy","content":"","keywords":"","version":"Next"},{"title":"Public Gateway​","type":1,"pageTitle":"Gateway Proxy","url":"/docs/develop/tools-and-features/gateway-proxy#public-gateway","content":" The tool can be used to set up a public gateway which can be used to host public facing applications or websites to users on the web who aren't running Bee nodes.  ","version":"Next","tagName":"h3"},{"title":"Stamp Management​","type":1,"pageTitle":"Gateway Proxy","url":"/docs/develop/tools-and-features/gateway-proxy#stamp-management","content":" In addition to acting as a proxy, it also includes convenient features for managing stamps such as automatically buying new stamps or automatically extending the life of existing stamps.  ","version":"Next","tagName":"h3"},{"title":"Security​","type":1,"pageTitle":"Gateway Proxy","url":"/docs/develop/tools-and-features/gateway-proxy#security","content":" Authentication can also be enabled so that only authorized requests are fulfilled. This is a useful feature for protecting your node if its API endpoint is publicly exposed. ","version":"Next","tagName":"h3"},{"title":"Hosting Your Dapps & Storing Their Data","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/introduction","content":"","keywords":"","version":"Next"},{"title":"Tools and Features​","type":1,"pageTitle":"Hosting Your Dapps & Storing Their Data","url":"/docs/develop/tools-and-features/introduction#tools-and-features","content":" Swarm is designed with decentralised applications in mind, and much time has been devoted to designing tools and features to support their prototyping and development.  ","version":"Next","tagName":"h2"},{"title":"Bee JS​","type":1,"pageTitle":"Hosting Your Dapps & Storing Their Data","url":"/docs/develop/tools-and-features/introduction#bee-js","content":" Our maverick JavaScript team, the Bee-Gees (🕺), have been working hard in the last few months to build some impressive tools for all you budding dapp developer Bees to get stuck into! Find out how to use the bee-js JavaScript library to start creating your own that live and work on Swarm!  ","version":"Next","tagName":"h3"},{"title":"Chunk Types​","type":1,"pageTitle":"Hosting Your Dapps & Storing Their Data","url":"/docs/develop/tools-and-features/introduction#chunk-types","content":" Swarm contains 3 types of chunks which enable us to build novel structures of how data can be stored in the swarm - in a completely decentralised way. Learn more aboutchunk typesto change the way you deal with data in your dapps forever!  ","version":"Next","tagName":"h3"},{"title":"Feeds​","type":1,"pageTitle":"Hosting Your Dapps & Storing Their Data","url":"/docs/develop/tools-and-features/introduction#feeds","content":" Swarm's single owner chunks have been cleverly combined to create user generated feeds in the swarm, see this example of how chunks are combined into a useful data structure you can use to build amazing applications.  ","version":"Next","tagName":"h3"},{"title":"PSS​","type":1,"pageTitle":"Hosting Your Dapps & Storing Their Data","url":"/docs/develop/tools-and-features/introduction#pss","content":" Hey there! Pss! 🤫 Swarm's trojan chunks are implemented in Bee to deliver Postal Service on Swarm - a pub-sub system that provides a totally leak-proof messaging system over the swarm.  ","version":"Next","tagName":"h3"},{"title":"Gateway Proxy​","type":1,"pageTitle":"Hosting Your Dapps & Storing Their Data","url":"/docs/develop/tools-and-features/introduction#gateway-proxy","content":" If you want your users to be able to access Swarm without running their own Bee node, for the time being you will need to make use of the Gateway Proxy tool. Join us in the#develop-on-swarm room in ourDiscord Server for more information on how to make your Swarm based applications accessible to everyone.  ","version":"Next","tagName":"h3"},{"title":"Bee Dev Mode​","type":1,"pageTitle":"Hosting Your Dapps & Storing Their Data","url":"/docs/develop/tools-and-features/introduction#bee-dev-mode","content":" If you want to test out Swarm based applications without needing to spend real xBZZ, Bee dev mode is an invaluable tool. Learn how to set up Bee in dev mode to begin prototyping your applications.  ","version":"Next","tagName":"h3"},{"title":"Starting a Test Network​","type":1,"pageTitle":"Hosting Your Dapps & Storing Their Data","url":"/docs/develop/tools-and-features/introduction#starting-a-test-network","content":" While bee dev mode allows you to simulate running a single Bee node, setting up a test network will allow you to better simulate interactions between multiple nodes. ","version":"Next","tagName":"h3"},{"title":"PSS Messaging","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/pss","content":"","keywords":"","version":"Next"},{"title":"Subscribe and Receive Messages​","type":1,"pageTitle":"PSS Messaging","url":"/docs/develop/tools-and-features/pss#subscribe-and-receive-messages","content":" Once your Bee node is up and running, you will be able to subscribe to feeds using WebSockets. For testing, it is useful to use the websocat command line utility.  Here we subscribe to the topic test-topic  websocat ws://localhost:1633/pss/subscribe/test-topic   Our node is now watching for new messages received in its nearest neighbourhood.  info Because a message is disguised as a normal chunk in Swarm, you will receive the message upon syncing the chunk, even if your node is not online at the moment when the message was send to you.  ","version":"Next","tagName":"h3"},{"title":"Send Messages​","type":1,"pageTitle":"PSS Messaging","url":"/docs/develop/tools-and-features/pss#send-messages","content":" Messages can be sent simply by sending a POST request to the PSS API endpoint.  When sending messages, we must specify a 'target' prefix of the recipient's Swarm address, a partial address representing their neighbourhood. Currently the length of this prefix is recommended to be two bytes, which will work well until the network has grown to a size of ca. 20-50K nodes. We must also provide the public key, so that Bee can encrypt the message in such a way that it may only be read by the intended recipient.  For example, if we want to send a PSS message with topic test-topic to a node with address...  7bc50a5d79cb69fa5a0df519c6cc7b420034faaa61c175b88fc4c683f7c79d96  ...and public key...  0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd  ...we must include the target 7bc5 and the public key itself as a query argument.  curl -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; -XPOST \\ localhost:1833/pss/send/test-topic/7bc5?recipient=0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd \\ --data &quot;Hello Swarm&quot;   More information on how to buy a postage stamp batch and get its batch id can be found here.  ","version":"Next","tagName":"h3"},{"title":"Send Messages in a Test Network​","type":1,"pageTitle":"PSS Messaging","url":"/docs/develop/tools-and-features/pss#send-messages-in-a-test-network","content":" Now, let's see this in action by setting up two Bee nodes on a test network, connecting them, and sending PSS messages from one to the other.  First start two Bee nodes. We will start them with distinct ports for the API and p2p ports, since they will be running on the same computer.  Run the following command to start the first node. Note that we are passing &quot;&quot; to the --bootnode argument so that our nodes will not connect to a network.  bee start \\ --api-addr=:1833 \\ --data-dir=/tmp/bee2 \\ --bootnode=&quot;&quot; \\ --p2p-addr=:1834 \\ --blockchain-rpc-endpoint=http://localhost:8545   We must make a note of the Swarm overlay address, underlay address and public key which are created once each node has started. We find this information from the /addresses endpoint of the API.  curl -s localhost:1833/addresses | jq   { &quot;overlay&quot;: &quot;46275b02b644a81c8776e2459531be2b2f34a94d47947feb03bc1e209678176c&quot;, &quot;underlay&quot;: [ &quot;/ip4/127.0.0.1/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq&quot;, &quot;/ip4/192.168.0.10/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq&quot;, &quot;/ip6/::1/tcp/7072/p2p/16Uiu2HAmTbaZndBa43PdBHEekjQQEdHqcyPgPc3oQwLoB2hRf1jq&quot; ], &quot;ethereum&quot;: &quot;0x0b546f2817d0d889bd70e244c1227f331f2edf74&quot;, &quot;public_key&quot;: &quot;03660e8dbcf3fda791e8e2e50bce658a96d766e68eb6caa00ce2bb87c1937f02a5&quot; }   Now the same for the second node.  bee start \\ --api-addr=:1933 \\ --data-dir=/tmp/bee3 \\ --bootnode=&quot;&quot; \\ --p2p-addr=:1934 \\ --blockchain-rpc-endpoint=http://localhost:8545   curl -s localhost:1935/addresses | jq   { &quot;overlay&quot;: &quot;085b5cf15a08f59b9d64e8ce3722a95b2c150bb6a2cef4ac8b612ee8b7872253&quot;, &quot;underlay&quot;: [ &quot;/ip4/127.0.0.1/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr&quot;, &quot;/ip4/192.168.0.10/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr&quot;, &quot;/ip6/::1/tcp/7073/p2p/16Uiu2HAm5RwRgkZWxDMAff2io6L4Qd1uL9yNgZSNTCdPsukcg5Qr&quot; ], &quot;ethereum&quot;: &quot;0x9ec47bd86a82276fba57f3009c2f6b3ace4286bf&quot;, &quot;public_key&quot;: &quot;0289634662d3ed7c9fb1d7d2a3690b69b4075cf138b683380023d2edc2e6847826&quot; }   Because we configured the nodes to start with no bootnodes, neither node should have peers yet.  curl -s localhost:1833/peers | jq   curl -s localhost:1935/peers | jq   { &quot;peers&quot;: [] }   Let's connect node 2 to node 1 using the localhost (127.0.0.1) underlay address for node 1 that we have noted earlier.  curl -XPOST \\ localhost:1935/connect/ip4/127.0.0.1/tcp/1834/p2p/16Uiu2HAmP9i7VoEcaGtHiyB6v7HieoiB9v7GFVZcL2VkSRnFwCHr   Now, if we check our peers endpoint for node 1, we can see our nodes are now peered together.  curl -s localhost:1833/peers | jq   { &quot;peers&quot;: [ { &quot;address&quot;: &quot;a231764383d7c46c60a6571905e72021a90d506ef8db06750f8a708d93fe706e&quot; } ] }   Of course, since we are p2p, node 2 will show node 1 as a peer too.  curl -s localhost:1935/peers | jq   { &quot;peers&quot;: [ { &quot;address&quot;: &quot;7bc50a5d79cb69fa5a0df519c6cc7b420034faaa61c175b88fc4c683f7c79d96&quot; } ] }   We will use websocat to listen for the PSS messages' Topic IDtest-topic on our first node.  websocat ws://localhost:1833/pss/subscribe/test-topic   Now we can use PSS to send a message from our second node to our first node.  Since our first node has a 2 byte address prefix of a231, we will specify this as the targets section in our POST request's URL. We must also include the public key of the recipient as a query parameter so that the message can be encrypted in a way only our recipient can decrypt.  curl \\ -H &quot;Swarm-Postage-Batch-Id: 78a26be9b42317fe6f0cbea3e47cbd0cf34f533db4e9c91cf92be40eb2968264&quot; -XPOST &quot;localhost:1933/pss/send/test-topic/7bc5?recipient=0349f7b9a6fa41b3a123c64706a072014d27f56accd9a0e92b06fe8516e470d8dd&quot; \\ --data &quot;Hello Swarm&quot;   The PSS API endpoint will now create a PSS message for its recipient in the form of a 'Trojan Chunk' and send this into the network so that it may be pushed to the correct neighbourhood. Once it is received by its recipient it will be decrypted and determined to be a message with the topic we are listening for. Our second node will decrypt the data and we'll see a message pop up in our websocat console!  websocat ws://localhost:1833/pss/subscribe/test-topic   Hello Swarm   Congratulations! 🎉 You have sent your first encrypted, zero leak message over Swarm! ","version":"Next","tagName":"h3"},{"title":"Erasure Cost Calculation","type":0,"sectionRef":"#","url":"/docs/learn/advanced/erasure-cost-calculation","content":"","keywords":"","version":"Next"},{"title":"Precise Cost Calculations​","type":1,"pageTitle":"Erasure Cost Calculation","url":"/docs/learn/advanced/erasure-cost-calculation#precise-cost-calculations","content":" For each redundancy level, there are m + k = 128 chunks, where m are the data chunks (shown in column &quot;Data Chunks&quot;) and k are the parity chunks (shown in column &quot;Parities&quot;). If the number of chunks in the data being uploaded are an exact multiple of m, then the percent cost of the upload will simply equal the one shown in the chart above in the &quot;Percent&quot; column for the corresponding redundancy level.  Exact Multiples​  For example, if we are uploading with the Strong redundancy level, and our source data consists of 321 (3 * 107) chunks, then we can simply use the percentage from the &quot;Percent&quot; column for the Strong level - 19.6% (63 parities / 321 data chunks).  With Remainders​  However, generally speaking uploads will not come in exact multiples of m, so we need to adjust our calculations. To do so we need to use the table below which shows the number of parities for sets of chunks starting at a single chunk for each redundancy level up to the maximum number of data chunks for that level. Then we simply sum up the total parities and data chunks for the entire upload and calculate the resulting percentage.  Security\tParities\tChunks\tPercent\tChunks Encrypted\tPercent EncryptedMedium\t2\t1\t200% Medium\t3\t2-5\t150% - 60%\t1-2\t300% - 150% Medium\t4\t6-14\t66.7% - 28.6%\t3-7\t133.3% - 57.1% Medium\t5\t15-28\t33.3% - 17.9%\t7-14\t71.4% - 35.7% Medium\t6\t29-46\t20.7% - 13%\t14-23\t42.9% - 26.1% Medium\t7\t47-68\t14.9% - 10.3%\t23-34\t30.4% - 20.6% Medium\t8\t69-94\t11.6% - 8.5%\t34-47\t23.5% - 17% Medium\t9\t95-119\t9.5% - 7.6%\t47-59\t19.1% - 15.3% Strong\t4\t1\t400% Strong\t5\t2-3\t250% - 166.7%\t1\t500% Strong\t6\t4-6\t150% - 100%\t2-3\t300% - 200% Strong\t7\t7-10\t100% - 70%\t3-5\t233.3% - 140% Strong\t8\t11-15\t72.7% - 53.3%\t5-7\t160% - 114.3% Strong\t9\t16-20\t56.2% - 45%\t8-10\t112.5% - 90% Strong\t10\t21-26\t47.6% - 38.5%\t10-13\t100% - 76.9% Strong\t11\t27-32\t40.7% - 34.4%\t13-16\t84.6% - 68.8% Strong\t12\t33-39\t36.4% - 30.8%\t16-19\t75% - 63.2% Strong\t13\t40-46\t32.5% - 28.3%\t20-23\t65% - 56.5% Strong\t14\t47-53\t29.8% - 26.4%\t23-26\t60.9% - 53.8% Strong\t15\t54-61\t27.8% - 24.6%\t27-30\t55.6% - 50% Strong\t16\t62-69\t25.8% - 23.2%\t31-34\t51.6% - 47.1% Strong\t17\t70-77\t24.3% - 22.1%\t35-38\t48.6% - 44.7% Strong\t18\t78-86\t23.1% - 20.9%\t39-43\t46.2% - 41.9% Strong\t19\t87-95\t21.8% - 20%\t43-47\t44.2% - 40.4% Strong\t20\t96-104\t20.8% - 19.2%\t48-52\t41.7% - 38.5% Strong\t21\t105-107\t20% - 19.6%\t52-53\t40.4% - 39.6% Insane\t5\t1\t500% Insane\t6\t2\t300%\t1\t600% Insane\t7\t3\t233.3%\t1\t700% Insane\t8\t4-5\t200% - 160%\t2\t400% Insane\t9\t6-8\t150% - 112.5%\t3-4\t300% - 225% Insane\t10\t9-10\t111.1% - 100%\t4-5\t250% - 200% Insane\t11\t11-13\t100% - 84.6%\t5-6\t220% - 183.3% Insane\t12\t14-16\t85.7% - 75%\t7-8\t171.4% - 150% Insane\t13\t17-19\t76.5% - 68.4%\t8-9\t162.5% - 144.4% Insane\t14\t20-22\t70% - 63.6%\t10-11\t140% - 127.3% Insane\t15\t23-26\t65.2% - 57.7%\t11-13\t136.4% - 115.4% Insane\t16\t27-29\t59.3% - 55.2%\t13-14\t123.1% - 114.3% Insane\t17\t30-33\t56.7% - 51.5%\t15-16\t113.3% - 106.2% Insane\t18\t34-37\t52.9% - 48.6%\t17-18\t105.9% - 100% Insane\t19\t38-41\t50% - 46.3%\t19-20\t100% - 95% Insane\t20\t42-45\t47.6% - 44.4%\t21-22\t95.2% - 90.9% Insane\t21\t46-50\t45.7% - 42%\t23-25\t91.3% - 84% Insane\t22\t51-54\t43.1% - 40.7%\t25-27\t88% - 81.5% Insane\t23\t55-59\t41.8% - 39%\t27-29\t85.2% - 79.3% Insane\t24\t60-63\t40% - 38.1%\t30-31\t80% - 77.4% Insane\t25\t64-68\t39.1% - 36.8%\t32-34\t78.1% - 73.5% Insane\t26\t69-73\t37.7% - 35.6%\t34-36\t76.5% - 72.2% Insane\t27\t74-77\t36.5% - 35.1%\t37-38\t73% - 71.1% Insane\t28\t78-82\t35.9% - 34.1%\t39-41\t71.8% - 68.3% Insane\t29\t83-87\t34.9% - 33.3%\t41-43\t70.7% - 67.4% Insane\t30\t88-92\t34.1% - 32.6%\t44-46\t68.2% - 65.2% Insane\t31\t93-97\t33.3% - 32%\t46-48\t67.4% - 64.6% Paranoid\t19\t1\t1900% Paranoid\t23\t2\t1150%\t1\t2300% Paranoid\t26\t3\t866.7%\t1\t2600% Paranoid\t29\t4\t725%\t2\t1450% Paranoid\t31\t5\t620%\t2\t1550% Paranoid\t34\t6\t566.7%\t3\t1133.3% Paranoid\t36\t7\t514.3%\t3\t1200% Paranoid\t38\t8\t475%\t4\t950% Paranoid\t40\t9\t444.4%\t4\t1000% Paranoid\t43\t10\t430%\t5\t860% Paranoid\t45\t11\t409.1%\t5\t900% Paranoid\t47\t12\t391.7%\t6\t783.3% Paranoid\t48\t13\t369.2%\t6\t800% Paranoid\t50\t14\t357.1%\t7\t714.3% Paranoid\t52\t15\t346.7%\t7\t742.9% Paranoid\t54\t16\t337.5%\t8\t675% Paranoid\t56\t17\t329.4%\t8\t700% Paranoid\t58\t18\t322.2%\t9\t644.4% Paranoid\t59\t19\t310.5%\t9\t655.6% Paranoid\t61\t20\t305%\t10\t610% Paranoid\t63\t21\t300%\t10\t630% Paranoid\t65\t22\t295.5%\t11\t590.9% Paranoid\t66\t23\t287%\t11\t600% Paranoid\t68\t24\t283.3%\t12\t566.7% Paranoid\t70\t25\t280%\t12\t583.3% Paranoid\t71\t26\t273.1%\t13\t546.2% Paranoid\t73\t27\t270.4%\t13\t561.5% Paranoid\t75\t28\t267.9%\t14\t535.7% Paranoid\t76\t29\t262.1%\t14\t542.9% Paranoid\t78\t30\t260%\t15\t520% Paranoid\t80\t31\t258.1%\t15\t533.3% Paranoid\t81\t32\t253.1%\t16\t506.2% Paranoid\t83\t33\t251.5%\t16\t518.8% Paranoid\t84\t34\t247.1%\t17\t494.1% Paranoid\t86\t35\t245.7%\t17\t505.9% Paranoid\t87\t36\t241.7%\t18\t483.3% Paranoid\t89\t37\t240.5%\t18\t494.4%  Let's take our previous example and adjust it slightly. Instead of a source data consisting of 321 (3 * 107) chunks, we will add 19 chunks for a total of 340 chunks. Looking at our chart, we can see that at the Strong level for 19 data chunks we need 9 parity chunks. From this we can calculate the final percentage price: 72 / 340 = 21.17%. ","version":"Next","tagName":"h3"},{"title":"Neighbourhoods","type":0,"sectionRef":"#","url":"/docs/learn/advanced/neighbourhoods","content":"","keywords":"","version":"Next"},{"title":"Example Neighbourhood​","type":1,"pageTitle":"Neighbourhoods","url":"/docs/learn/advanced/neighbourhoods#example-neighbourhood","content":" Let's take a closer look at an example. Below is a neighbourhood of six nodes at depth 10. Each node is identified by its Swarm address, which is a 256 bit hexadecimal number derived from the node's Gnosis Chain address, the Swarm network id, and a random nonce.  da4cb0d125bba638def55c0061b00d7c01ed4033fa193d6e53a67183c5488d73 da5d39a5508fadf66c8665d5e51617f0e9e5fd501e429c38471b861f104c1504 da7a974149543df1b459831286b42b302f22393a20e9b3dd9a7bb5a7aa5af263 da76f8fccc3267b589d822f1c601b21b525fdc2598df97856191f9063029d21e da7b6439c8d3803286b773a56c4b9a38776b5cd0beb8fd628b6007df235cf35c da7fd412b79358f84b7928d2f6b7ccdaf165a21313608e16edd317a5355ba250  Since we are only concerned with the leading binary bits close to the neighbourhood depth, for the rest of this example we will abbreviate the addresses to the first four prefixed hexadecimal digits only. Below are listed the hex prefixes and their binary representation, with the first ten leading bits underlined:  Hex prefix\tBinary Bitsda4c\t1101101001001100 da5d\t1101101001011101 da76\t1101101001110110 da7a\t1101101001111010 da7b\t1101101001111011 da7f\t1101101001111111  ","version":"Next","tagName":"h3"},{"title":"Chunk Neighbourhood Assignment​","type":1,"pageTitle":"Neighbourhoods","url":"/docs/learn/advanced/neighbourhoods#chunk-neighbourhood-assignment","content":" Chunks are assigned to neighbourhoods based on their addresses, which are in the same 256 bit format as node addresses. Here are two example chunks which fall within our example neighbourhood:  Chunk A address: da49a42926015cd1e2bc552147c567b1ca13e8d4302c9e6026e79a24de328b65 Chunk B address: da696a3dfb0f7f952872eb33e0e2a1435c61f111ff361e64203b5348cc06dc8a  As the address of the chunk shown above shares the same ten leading binary bits as the nodes in our example neighbourhood, it falls into that neighbourhood's area of responsibility, and all the nodes in that neighbourhood are required to store that chunk:  da49 --&gt; 1101101001001001 da69 --&gt; 1101101001101001  As with the example for nodes, we've abbreviated the chunk addresses to their leading four hexadecimal digits only and converted them to binary digits.  ","version":"Next","tagName":"h3"},{"title":"Neighbourhood Doubling​","type":1,"pageTitle":"Neighbourhoods","url":"/docs/learn/advanced/neighbourhoods#neighbourhood-doubling","content":" As more and more chunks are assigned to neighbourhoods, the chunk reserves of the nodes in that neighbourhood will begin to fill up. Once the nodes' reserves in a neighbourhood become full and can no longer store additional chunks, that neighbourhood will split, with each half of the neighbourhood taking responsibility for half of the chunks. This event is referred to as a &quot;doubling&quot;, as it results in double the number of neighbourhoods. The split is done by increasing the storage depth by one, so that the number of shared leading bits is increased by one. This results in a binary splitting of the neighbourhood and associated chunks into two new neighbourhoods and respective groups of chunks.  info Note that when chunks begin to expire and new chunks are not uploaded to Swarm, it is possible for node's reserves to empty out, once they fall below a certain threshold, a &quot;halving&quot; will occur in which the storage depth will be decreased by one and two neighbourhoods will merge to make a new one so that they are responsible for a wider set of chunks.  Using our previous example neighbourhood, during a doubling, the storage depth would increase from 10 to 11, and the neighbourhood would be split based on the 11th leading bit.  Neighbourhood A:  Hex prefix\tBinary Bitsda4c\t1101101001001100 da5d\t1101101001011101  Neighbourhood B:  Hex prefix\tBinary Bitsda76\t1101101001110110 da7a\t1101101001111010 da7b\t1101101001111011 da7f\t1101101001111111  Each of our two example chunks will also be split amongst the two new neighbourhoods based on their 11th leading bit:  Neighbourhood A:  Hex prefix\tBinary Bitsda4c\t1101101001001100 da5d\t1101101001011101 da49 (chunk)\t1101101001001001  Neighbourhood B:  Hex prefix\tBinary Bitsda76\t1101101001110110 da7a\t1101101001111010 da7b\t1101101001111011 da7f\t1101101001111111 da69 (chunk)\t1101101001101001  ","version":"Next","tagName":"h3"},{"title":"Doubling Implications for Node Operators​","type":1,"pageTitle":"Neighbourhoods","url":"/docs/learn/advanced/neighbourhoods#doubling-implications-for-node-operators","content":" One of the implications of doubling for node operators is that the reward chances for a node depends in part on how many other nodes are in its neighbourhood. If it is in a neighbourhood with fewer nodes, its chances of winning rewards are greater. Therefore node operators should make certain to place their nodes into less populated neighbourhoods, and also should look ahead to neighbourhoods at the next depth after a doubling. For more details about how to adjust node placement, see the section on setting a target neighbourhood in the installation guide. ","version":"Next","tagName":"h3"},{"title":"Awesome Swarm","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/awesome","content":"","keywords":"","version":"Next"},{"title":"Services​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#services","content":" Bee - Also referred to as the node or the client, this service allows you to join the Swarm network  ","version":"Next","tagName":"h3"},{"title":"Libraries​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#libraries","content":" Bee-JS - A high-level Javascript library to interact with Bee through its REST API  Mantaray-JS - A low-level Swarm manifest manipulation library  Sepatree - The SepaTree data structure abstracted on Swarm  BeeJeez - Javascript implementation of the handshake protocol and others based on libp2p  ","version":"Next","tagName":"h3"},{"title":"CI/CD​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#cicd","content":" Beekeeper - Orchestrate and test Bee clusters through Kubernetes  Bee Factory - Sets up a Dockerized stack of Bee nodes including Ganache blockchain  Bee Factory VPS - Provides an automatized way to set up Bee Factory on a fresh Ubuntu VPS  Beeload Action - GitHub Actions workflow for uploading data to the Swarm network  ","version":"Next","tagName":"h3"},{"title":"UI​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#ui","content":" Bee Dashboard - React project to troubleshoot and interact with your Bee node  Gateway - Gateway to the Swarm project, for uploading, downloading and sharing assets on the network  Pastebee - Pastebin, but on Swarm and with unstoppable publishing  Chess UI - Play, store and share Chess games on Swarm  ","version":"Next","tagName":"h3"},{"title":"Tools​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#tools","content":" Swarm CLI - Do everything on Swarm with the power of the terminal  Swarm Extension - Official extension that adds Swarm support and injects Bee library to the browser  Pastebee CLI - Upload to Pastebee via the CLI and share the Swarm hash  Swarm CID Converter - Convert Swarm hashes or links to CID and vice versa.  Bee-AFS - FUSE filesystem for Bee  Nextcloud Swarm Plugin - Plugin for bridging Nextcloud and Swarm.  ","version":"Next","tagName":"h3"},{"title":"Smart Contracts​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#smart-contracts","content":" Swap, Swear and Swindle - Protocols for peer-to-peer accounting  Storage Incentives - Smart contracts providing the basis for Swarm's storage incentivization model  ","version":"Next","tagName":"h3"},{"title":"Documentation​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#documentation","content":" The Book of Swarm - Storage and communication infrastructure for self-sovereign digital society back-end stack for the decentralised web  Bee Docs - Documentation for the Swarm Bee Client. View at docs.ethswarm.org.  Bee-JS Docs - Documentation for the Swarm Bee-js javascript library. View at bee-js.ethswarm.org.  ","version":"Next","tagName":"h3"},{"title":"Community / Ecosystem​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#community--ecosystem","content":" Fair data society - Ecosystem initiative for ethical Web3  FairOS - Distributed file system, key-value store and nosql store on Swarm (for developers)  Fair Data Protocol roadmap enabling data interoperability - Develop your dapp on Swarm fast and in an interoperable way  FDP play - CLI tool to spin up local development FDP environment and Bee cluster with Docker  Blossom browser extension - Browser Extension based on Fair Data Protocol that acts as a web3 framework for dApps and a Fair Data Society account manager for end-users  Fairdrive - Decentralised and unstoppable &quot;Dropbox&quot; for end-users and developers using Fair Data Protocol  Fairdrive code - Code for decentralised and unstoppable &quot;Dropbox&quot; for end-users and developers using Fair Data Protocol  Fairdrop - Decentralised file sharing  Galileo - Open Street Maps on Swarm  Dracula - Hackmd-like markdown editor that works with Swarm  SwarmScan - Get network insights  Etherna.io - Decentralised Youtube on Swarm  Social Archive - Archive your social media  Swapchat 2.0 - Decentralised, ephemeral, peer-to-peer, encrypted chat  Hacker Manifesto - The Hacker Manifesto on Swarm with a community funded postage stamp  SwarmNFT library - JavaScript library for creating NFTs on Ethereum-compatible blockchains and storing content on Swarm  videoNFT - NFT live streaming with Swarm (winner of EthBerlin3 2022 Freedom to Transact Track)  DeBoot - DeBoot is a project to research and implement approaches to bootloading OS images from a decentralized storage network such as Swarm or IPFS.  Swarm DAppNode Package - Swarm DAppNode package for Swarm Mainnet with multi-platform (x86_64 and arm64) support. Testnet DAppNode packages can be found here.  ","version":"Next","tagName":"h3"},{"title":"Miscellaneous​","type":1,"pageTitle":"Awesome Swarm","url":"/docs/learn/ecosystem/awesome#miscellaneous","content":" Swarm Bot - Discord bot handling commands related to Swarm and its community ","version":"Next","tagName":"h3"},{"title":"Starting a Test Network","type":0,"sectionRef":"#","url":"/docs/develop/tools-and-features/starting-a-test-network","content":"","keywords":"","version":"Next"},{"title":"Start a network on your own computer​","type":1,"pageTitle":"Starting a Test Network","url":"/docs/develop/tools-and-features/starting-a-test-network#start-a-network-on-your-own-computer","content":" ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Starting a Test Network","url":"/docs/develop/tools-and-features/starting-a-test-network#configuration","content":" Starting a network is easiest achieved by making use of configuration files. We need at least two nodes to start a network. Hence, below two configuration files are provided. Save them respectively as config_1.yaml and config_2.yaml.  config_1.yaml  network-id: 7357 api-addr: 127.0.0.1:1633 p2p-addr: :1634 bootnode: &quot;&quot; data-dir: /tmp/bee/node1 password: some pass phze swap-enable: false   config_2.yaml  network-id: 7357 api-addr: 127.0.0.1::1733 p2p-addr: :1734 data-dir: /tmp/bee/node2 bootnode: &quot;&quot; password: some pass phze welcome-message: &quot;Bzz Bzz Bzz&quot; swap-enable: false   Note that for each node, we provide a different api-addr. If we had not specified different addresses here, we would get an address already in use error, as no two applications can listen to the same port. We also specify a differentp2p-addr. If we had not, our nodes would not be able to communicate with each other. We also specify a separate data-dir for each node, as each node must have its own separate key and chunk data store.  We also provide a network-id, so that our network remains separate from the Swarm mainnet, which has network-id 1. Nodes will not connect to peers which have a different network id. We also set our bootnode to be the empty string &quot;&quot;. A bootnode is responsible for bootstrapping the network so that a new node can find its first few peers before it begins its own journey to find friends in the Swarm. In Swarm any node can be used as a bootnode. Later, we will manually join our nodes together so in this case a bootnode isn't required.  Finally, note the welcome-message in the first nodes configuration file. This is a friendly feature allowing you to send a message to peers that connect to you!  ","version":"Next","tagName":"h3"},{"title":"Starting Your Nodes​","type":1,"pageTitle":"Starting a Test Network","url":"/docs/develop/tools-and-features/starting-a-test-network#starting-your-nodes","content":" Now we have created our configuration files, let's start our nodes by running bee start --config config_1.yaml, then in another Terminal session, run bee start --config-file config_2.yaml.  We can now inspect the state of our network by sending HTTP requests to the API.  curl -s http://localhost:1633/topology | jq .connected   0   curl -s http://localhost:1733/topology | jq .connected   0   No connections yet? Right! Let's remedy that!  info Here we are using the jq command line utility to count the amount of objects in the peers array in the JSON response we have received from our API, learn more about how to install and use jq here.  ","version":"Next","tagName":"h3"},{"title":"Making a network​","type":1,"pageTitle":"Starting a Test Network","url":"/docs/develop/tools-and-features/starting-a-test-network#making-a-network","content":" In order to create a network from our two isolated nodes, we must first instruct our nodes to connect to each other. This step is not explicitly needed if you connect to the main Swarm network, as the default bootnodes in the Swarm network will automatically suggest peers.  First, we will need to find out the network address of the first node. To do this, we send a HTTP request to the addresses endpoint of the API.  curl localhost:1633/addresses | jq   { &quot;overlay&quot;: &quot;f57a65207f5766084d3ebb6bea5e2e4a712504e54d86a00961136b514f07cdac&quot;, &quot;underlay&quot;: [ &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot;, &quot;/ip4/192.168.0.10/tcp/1634/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot;, &quot;/ip6/::1/tcp/1634/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot;, &quot;/ip4/xx.xx.xx.xx/tcp/40317/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot; ] }   Here, we get firstly the overlay address - this is the permanent address Swarm uses as your anonymous identity in the network and secondly, a list of all the multiaddresses, which are physical network addresses at which you node can be found by peers.  Note the addresses starting with an /ip4, followed by 127.0.0.1, which is the localhost internal network in your computer. Now we can use this full address to be the bootnode of our second node so that when it starts up, it goes to this address and both nodes become peers of each other. Let's add this into our config_2.yaml file.  config_2.yaml  network-id: 7357 api-addr: 127.0.0.1::1733 p2p-addr: :1734 data-dir: /tmp/bee/node2 bootnode: &quot;/ip4/127.0.0.1/tcp/1634/p2p/16Uiu2HAmUdCRWmyQCEahHthy7G4VsbBQ6dY9Hnk79337NfadKJEs&quot; password: some pass phze welcome-message: &quot;Bzz Bzz Bzz&quot; swap-enable: false   Now, we can shut our second node and reboot with the new configuration.  Look at the the output for your first node, you should see our connection message!  Let's also verify that we can see both nodes in using each other's API's.  curl -s http://localhost:1633/peers | jq   curl -s http://localhost:1733/peers | jq   Congratulations! You have made your own tiny two bee Swarm! 🐝 🐝 ","version":"Next","tagName":"h3"},{"title":"Community","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/community","content":"","keywords":"","version":"Next"},{"title":"Links​","type":1,"pageTitle":"Community","url":"/docs/learn/ecosystem/community#links","content":" Twitter Discord server Reddit GitHub Blog Homepage ","version":"Next","tagName":"h2"},{"title":"Fair Data Society","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/fair-data-society","content":"","keywords":"","version":"Next"},{"title":"Links​","type":1,"pageTitle":"Fair Data Society","url":"/docs/learn/ecosystem/fair-data-society#links","content":" FDS YouTubeFDS HomepageFDS DiscordFDS TwitterFDS GitHub ","version":"Next","tagName":"h2"},{"title":"Grants and Bounties","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/grants-bounties","content":"Grants and Bounties Swarm grants support many interesting projects that are already building their products on top of Swarm. Swarm bounties extend the ecosystem with tooling and infrastructure. If you have an idea for a project which uses Swarm's technology we welcome you to apply for a grant. Learn more about grants for building on Swarm at the EthSwarm homepage.","keywords":"","version":"Next"},{"title":"Foundation","type":0,"sectionRef":"#","url":"/docs/learn/ecosystem/swarm-foundation","content":"","keywords":"","version":"Next"},{"title":"Swarm Foundation​","type":1,"pageTitle":"Foundation","url":"/docs/learn/ecosystem/swarm-foundation#swarm-foundation","content":" The goal of Swarm Foundation is to support and contribute to the creation of technology whose code will be open to all and will allow the storage and exchange of data in a decentralised manner. To this end, the foundation intends in particular to promote and support a community and a sustainable and independent ecosystem for the development of free open source software (FLOSS), allowing for digital services coordinated by crypto-economic incentives that process, distribute and store data.  Its mission is to empower digital freedom by promoting the development and maintenance of the Swarm network, the base layer of the emerging fair data economy, and the community surrounding this network.  It does so by supporting many different initiatives, either through financial grants or other types of support, all of which is assessed on a case-by-case basis. ","version":"Next","tagName":"h2"},{"title":"Access Control","type":0,"sectionRef":"#","url":"/docs/learn/technology/act","content":"","keywords":"","version":"Next"},{"title":"Key Concepts​","type":1,"pageTitle":"Access Control","url":"/docs/learn/technology/act#key-concepts","content":" From the perspective of access controlled content, we can identify two main roles:  Role\tRights &amp; responsibilitiesContent Publisher Publishers upload data and grant access to viewers based on their wallets’ public keys.They can also revoke access from specific viewers. Grantee (Content Viewer) Grantees can access the content version allowed by the publisher.However, they may be blocked from accessing new versions of the content.  The control is defined by a process to obtain the full (decrypted) reference to the protected content uploaded by the publisher, which makes granted access possible.  For the management of access by multiple grantees (viewers), an additional layer is introduced to derive the access key from their specific session key. This data structure, the lookup table for ACT, is implemented as key-value store in a Swarm manifest format. The publisher is able to add and remove grantees from this ACT.  ","version":"Next","tagName":"h2"},{"title":"Session​","type":1,"pageTitle":"Access Control","url":"/docs/learn/technology/act#session","content":" For each grantee, their public key is used as the session key. Using Diffie-Hellman key derivation, two additional keys will be derived from the session key: a lookup key and an access key decryption key (used for symmetric encryption of the access key). This means each grantee will have the content's access key specifically encrypted for them, and only they will be able to decrypt this, thus gain access to the content.  ","version":"Next","tagName":"h3"},{"title":"ACT lookup table​","type":1,"pageTitle":"Access Control","url":"/docs/learn/technology/act#act-lookup-table","content":" The ACT lookup table is a key-value store implemented over a Swarm manifest. It holds lookup keys and encrypted access keys prepared for the grantees when they are added to the ACT (granting access to the content).  ","version":"Next","tagName":"h3"},{"title":"History​","type":1,"pageTitle":"Access Control","url":"/docs/learn/technology/act#history","content":" The history of the ACT is maintained as well. This allows to retrieve a historical version of the ACT based on the timestamp attached to it. This also ensures that grantees will be able to retrieve the content version they were granted access to (using the relevant timestamp), even if their access to newer versions were revoked.  ","version":"Next","tagName":"h3"},{"title":"Encryption​","type":1,"pageTitle":"Access Control","url":"/docs/learn/technology/act#encryption","content":" It is important to emphasise that all elements of the process will undergo encryption. Including the grantee list itself, which is encrypted using the publisher’s own lookup key, as well as the grantee list’s content reference. This ensures that the security of the process and the data is always maintained. ","version":"Next","tagName":"h3"},{"title":"Swarm FAQ","type":0,"sectionRef":"#","url":"/docs/learn/faq","content":"","keywords":"","version":"Next"},{"title":"Community​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#community","content":" ","version":"Next","tagName":"h2"},{"title":"What are the Swarm Foundation's official channels?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-are-the-swarm-foundations-official-channels","content":" Website: https://ethswarm.org/Blog:https://blog.ethswarm.org/Github: https://github.com/etherspheree-mail: info@ethswarm.orgDiscord: https://discord.ethswarm.org/Twitter: https://twitter.com/ethswarmReddit: https://www.reddit.com/r/ethswarmYoutube: https://www.youtube.com/channel/UCu6ywn9MTqdREuE6xuRkskA  ","version":"Next","tagName":"h3"},{"title":"Where can I find technical support and get answers to my other questions?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#where-can-i-find-technical-support-and-get-answers-to-my-other-questions","content":" The Swarm community is centered around our Discord server where you will find many people willing and able to help with your every need! https://discord.ethswarm.org/  ","version":"Next","tagName":"h3"},{"title":"Where can I find support for running Bee node on Dappnode?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#where-can-i-find-support-for-running-bee-node-on-dappnode","content":" You can find support for running Bee on Dappnode on the Dappnode Discord server: https://discord.gg/dRd5CrjF  ","version":"Next","tagName":"h3"},{"title":"Who can I contact for other inquiries?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#who-can-i-contact-for-other-inquiries","content":" For any other inquiries, you can contact us at info@ethswarm.org  ","version":"Next","tagName":"h3"},{"title":"What's the relationship between Swarm and Ethereum?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#whats-the-relationship-between-swarm-and-ethereum","content":" Swarm started in the first days of Ethereum as part of the original &quot;world computer&quot; vision, consisting of Ethereum (the processor), Whisper (messaging) and Swarm (storage). The project is the result of years of research and work by the Ethereum Foundation, the Swarm Foundation, teams, individuals across the ecosystem and the community.  The conceptual idea for Swarm was started in the Ethereum team at the beginning, and the Ethereum Foundation incubated Swarm. After five years of research, Swarm and Ethereum are now two separate entities.  ","version":"Next","tagName":"h3"},{"title":"BZZ Token​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#bzz-token","content":" ","version":"Next","tagName":"h2"},{"title":"What is BZZ Token?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-bzz-token","content":" Swarm's native token BZZ, was initially issued on Ethereum. It has been bridged over to Gnosis where it is referred to as xBZZ for differentiation, and serves as a means of accessing the platform's data relay and storage services, while also providing compensation for node operators who provide these services.  ","version":"Next","tagName":"h3"},{"title":"What is PLUR?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-plur","content":" 1 PLUR is the atomic unit of xBZZ, where xBZZ then has 16 decimals (ie. 1 PLUR = 1e-16 xBZZ)  ","version":"Next","tagName":"h3"},{"title":"Where can I buy BZZ tokens?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#where-can-i-buy-bzz-tokens","content":" There are many ways to acquire BZZ tokens, either on custodial centralised exchanges where you can trade traditional currencies and cryptocurrency or through decentralised exchanges and protocols where you can trade between cryptocurrencies. For more information please visit the Get BZZ page on the Ethswarm.org homepage.  Note that for use on Swarm for staking or purchasing postage stamps, you need the Gnosis Chain version of BZZ, commonly referred to as xBZZ.  ","version":"Next","tagName":"h3"},{"title":"What is the BZZ token address?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-bzz-token-address","content":" See this page for a list of relevant token addresses.  ","version":"Next","tagName":"h3"},{"title":"What is the BZZ token supply?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-bzz-token-supply","content":" With the shutdown of the bonding curve as a result of a community vote, the BZZ supply is now fixed at 63,149,437.  ","version":"Next","tagName":"h3"},{"title":"BZZ token tokenomics​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#bzz-token-tokenomics","content":" More about BZZ token tokenomics: https://blog.ethswarm.org/hive/2021/bzz-tokenomics/  ","version":"Next","tagName":"h3"},{"title":"What is the bonding curve?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-bonding-curve","content":" A bonding curve is a mathematical function in the form of y=f(x) that determines the price of a single token, depending on the number of tokens currently in existence, or the market supply. The key difference is that with a traditional exchange platform market makers are required to provide liquidity to the market, whereas a bonding curve takes over the role of providing liquidity, negating the need for market makers.  ","version":"Next","tagName":"h3"},{"title":"What is the \"Bzzaar\" bonding curve?​","type":1,"pageTitle":"Swarm FAQ","url":"/docs/learn/faq#what-is-the-bzzaar-bonding-curve","content":" During the first several years of the life of the BZZ token, the bonding curve mechanism played a critical role in maintaining liquidity and setting a transparent pricing model for BZZ tokens.  On May 4th of 2024, as a result of a community vote, the bonding curve was shut down and the BZZ supply is now fixed at 63,149,437. ","version":"Next","tagName":"h3"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/learn/introduction","content":"","keywords":"","version":"Next"},{"title":"Welcome!​","type":1,"pageTitle":"Introduction","url":"/docs/learn/introduction#welcome","content":" Swarm is a peer-to-peer network of Bee nodes that collectively provide censorship resistant decentralised storage and communication services. Swarm's mission is to shape the future towards a self-sovereign global society and permissionless open markets by providing scalable base-layer data storage infrastructure for the decentralised internet. Its incentive system is enforced through smart contracts on the Gnosis Chain blockchain and powered by the xBZZ token, making it economically self-sustaining. ","version":"Next","tagName":"h3"},{"title":"Glossary","type":0,"sectionRef":"#","url":"/docs/learn/glossary","content":"","keywords":"","version":"Next"},{"title":"Swarm​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#swarm","content":" The Swarm network consists of a collection of Bee nodes which work together to enable decentralised data storage for the next generation of censorship-resistant, unstoppable, serverless dapps.  Swarm is also the name of the core organization that oversees the development and success of the Bee Swarm as a whole. They can be found at ethswarm.org.  ","version":"Next","tagName":"h2"},{"title":"Gnosis Chain​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#gnosis-chain","content":" Gnosis Chain (previously known as xDai chain) is a PoS, EVM compatible Ethereum sidechain which uses the same addressing scheme as Ethereum. Swarm's smart contracts have been issued on Gnosis Chain.  ","version":"Next","tagName":"h2"},{"title":"Smart Contracts​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#smart-contracts","content":" Smart contracts are automatically executable code which can be published on a blockchain to ensure immutability. Swarm uses smart contracts on Gnosis Chain for a variety of key aspects of the network including incentivization, inter-node accounting, and payments for storage.  ","version":"Next","tagName":"h2"},{"title":"Bee​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#bee","content":" Swarm nodes are referred to as &quot;Bee&quot; nodes. Bee nodes can run on a wide variety of computer types including desktop computers, hobbyist computers like Raspberry Pi 4, remotely hosted virtual machines, and much more. When running, Bee nodes interact with Swarm smart contracts on Gnosis Chain and connect with other Bee nodes to form the Swarm network.  Bee nodes can act as both client and service provider, or solely as client or service provider, depending on the needs of the node operator. Bee nodes pay each other for services on the Swarm network with the xBZZ token.  ","version":"Next","tagName":"h2"},{"title":"Overlay​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#overlay","content":" An overlay network is a virtual or logical network built on top of some lower level &quot;underlay&quot; network. Examples include the Internet as an overlay network built on top of the telephone network, and the p2p Bittorent network built on top of the Internet.  With Swarm, the overlay network is based on a Kademlia DHT with overlay addresses derived from each node's Gnosis address. Swarm's overlay network addresses are permanent identifiers for each node and do not change over time.  ","version":"Next","tagName":"h2"},{"title":"Overlay Address​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#overlay-address","content":" Overlay addresses are a Keccak256 hash of a node’s Gnosis Chain address and the Swarm network ID (the Swarm network ID is included to prevent address collisions). The overlay address for a node does not change over time and is a permanent identifier for the node. Overlay addresses are used to group nodes into neighborhoods which are responsible for storing the same chunks of data. If not otherwise specified, when referring to a &quot;node address&quot;, it typically is referring to the overlay address, not the underlay address. The overlay address is the address used to determine which nodes connect to each other and which chunks nodes are responsible for.  ","version":"Next","tagName":"h2"},{"title":"Neighborhood​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#neighborhood","content":" Neighborhoods are nodes which are grouped together based on their overlay addresses and are responsible for storing the same chunks of data. The chunks which each neighborhood are responsible for storing are defined by the proximity order of the nodes and the chunks. See DISC for more details.  ","version":"Next","tagName":"h2"},{"title":"Underlay​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#underlay","content":" An underlay network is the low level network on which an overlay network is built. It allows nodes to find each other, communicate, and transfer data. Swarm's underlay network is a p2p network built with libp2p. Nodes are assigned underlay addresses which in contrast to their overlay addresses are not permanent and may change over time.  ","version":"Next","tagName":"h2"},{"title":"Swap​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#swap","content":" Swap is the p2p accounting protocol used for Bee nodes. It allows for the automated accounting and settlement of services between Bee nodes in the Swarm network. In the case that services exchanged between nodes is balanced equally, no settlement is necessary. In the case that one node is unequally indebted to another, settlement is made to clear the node's debts. Two key elements of the Swap protocol are cheques and the chequebook contract.  ","version":"Next","tagName":"h2"},{"title":"Cheques & Chequebook​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#cheques--chequebook","content":" Cheques are the off-chain method of accounting used by the Swap protocol where the issuing node signs a cheque specifying a beneficiary, a date, and an amount, and gives it to the recipient node as a token of promise to pay at a later date.  The chequebook is the smart contract where the cheque issuer's funds are stored and where the beneficiary can cash the cheque received.  The cheque and chequebook system reduces the number of required on-chain transactions by allowing multiple cheques to accumulate and be settled together as a group, and in the case that the balance of cheques between nodes is equal, no settlement transaction is required at all.  ","version":"Next","tagName":"h2"},{"title":"Postage Stamps​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#postage-stamps","content":" Postage stamps can be purchased with xBZZ and represent the right to store data on the Swarm network. In order to upload data to Swarm, a user must purchase a batch of stamps which they can then use to upload an equivalent amount of data to the network.  ","version":"Next","tagName":"h2"},{"title":"Kademlia​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#kademlia","content":" Kademlia is a distributed hash table (DHT) which is commonly used in distributed peer-to-peer networks. A distributed hash table is a type of hash table which is designed to be stored across a decentralized group of nodes in order to be persistent and fault tolerant. It is designed so that each node is only required to store a subset of the total set of key / value pairs. One of the unique features of the Kademlia DHT design is a distance metric based on the XOR bitwise operation. It is referred to as &quot;Kademlia distance&quot; or just &quot;distance&quot;. Swarm’s DISC uses a modified version of Kademlia which has been specialized for storage purposes, and understanding the concepts behind Kademlia is necessary for understanding Swarm.  ","version":"Next","tagName":"h2"},{"title":"Kademlia distance​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#kademlia-distance","content":" Within Kademlia, nodes have numeric ids with the same length and format taken from the same namespace as the keys of the key/value pairs. Kademlia distance between node ids and keys is calculated through the XOR bitwise operation done over any ids or keys.  Note: For a Kademlia DHT, any standardized numerical format can be used for ids. However, within Swarm, ids are derived from a Keccak256 digest and are represented as 256 bit hexadecimal numbers. They are referred to as addresses or hashes.  Example: We have a Kademlia DHT consisting of only ten nodes with ids of 1 - 10. We want to find the distance between node 4 and 7. In order to do that, we perform the XOR bitwise operation:  4 | 0100 7 | 0111 ————XOR 3 | 0011  And we find that the distance between the two nodes is 3.  ","version":"Next","tagName":"h2"},{"title":"Chunk​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#chunk","content":" When data is uploaded to Swarm, it is broken down into 4kb sized pieces which are each assigned an address in the same format as node’s overlay addresses. Chunk addresses are formed by taking the BMT hash of the chunk content along with an 8 byte measure of the number of the chunk’s child chunks, the span. The BMT hashing algorithm is based on the Keccac256 hashing algorithm, so it produces an address with the same format as that for the node overlay addresses.  ","version":"Next","tagName":"h2"},{"title":"Proximity Order (PO)​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#proximity-order-po","content":" Proximity Order is a concept defined in The Book of Swarm and is closely related to Kademlia distance. Proximity order is defined as the number of shared prefix bits of any two addresses. It is found by performing the XOR bitwise operation on the two addresses and counting how many leading 0 there are before the first 1. In other words, PO is equal to the number of shared binary prefix bits.  Taking the previous example used in the Kademlia distance definition:  4 | 0100 7 | 0111 ————XOR 3 | 0011  In the result we find that the distance is 3, and that there are two leading zeros. Therefore for the PO of these two nodes is 2.  Both Proximity Order and distance are measures of the relatedness of ids, however Kademlia distance is a more exact measurement.  Taking the previous example used in the Kademlia distance definition:  5 | 0101 7 | 0111 ————XOR 2 | 0010  Here we find that the distance between 5 and 7 is 2, and the PO is also two. Although 5 is closer to 7 than 4 is to 7, they both fall within the same PO, since PO is only concerned with the shared leading bits. PO is a fundamental concept to Swarm’s design and is used as the basic unit of relatedness when discussing the addresses of chunks and nodes. PO is also closely related to the concept of depth.  ","version":"Next","tagName":"h2"},{"title":"Depth types​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#depth-types","content":" There are three fundamental categories of depth:  ","version":"Next","tagName":"h2"},{"title":"1. Topology related depth​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#1-topology-related-depth","content":" This depth is defined in relation to the connection topology of a single node as the subject in relation to all the other nodes it is connected to. It is referred to using several different terms which all refer to the same concept (Connectivity depth / Kademlia depth / neighbourhood depth / physical depth)  Connectivity depth refers to the saturation level of the node’s topology - the level to which the topology of a node’s connections has Kademlia connectivity. Defined as one level deeper than the deepest fully saturated level. A PO is defined as saturated if it has at least the minimum required level of connected nodes, which is set at 8 nodes in the current implementation of Swarm.  The output from the Bee API's topology endpoint:    Here we can see the depth is 8, meaning that PO bin 7 is the deepest fully saturated PO bin:    Here we can confirm that at least 8 nodes are connected in bin 7.  Connectivity depth is defined from the point of view of individual nodes, it is not defined as characteristic of the entire network. However, given a uniform distribution of node ids within the namespace and given enough nodes, all nodes should converge towards the same connectivity depth.  While this is sometimes referred to as Kademlia depth, the term “Kademlia depth” is not defined within the original Kademlia paper, rather it refers to the depth at which the network in question (Swarms) has the characteristics which fulfill the requirements described in the Kademlia paper.  ","version":"Next","tagName":"h3"},{"title":"2. Area of responsibility related depths​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#2-area-of-responsibility-related-depths","content":" Area of responsibility refers to which chunks a node is responsible for storing. There are two concepts of depth related to a node’s area of responsibility - storage depth and reserve depth. Both reserve depth and storage depth are measures of PO which define the chunks a node is responsible for storing.  ","version":"Next","tagName":"h3"},{"title":"2a. Reserve Depth​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#2a-reserve-depth","content":" The PO which measures the node’s area of responsibility based on the theoretical 100% utilisation of all postage stamp batches (all the chunks which are eligible to be uploaded and stored are uploaded and stored). Has an inverse relationship with area of responsibility - as depth grows, area of responsibility gets smaller.  ","version":"Next","tagName":"h3"},{"title":"2b. Storage Depth​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#2b-storage-depth","content":" The PO which measures the node’s effective area of responsibility. Storage depth will equal reserve depth in the case of 100% utilisation - however 100% utilisation is uncommon. If after syncing all the chunks within the node’s area of responsibility at its reserve depth and the node still has sufficient space left, then the storage depth will decrease so that the area of responsibility doubles.  ","version":"Next","tagName":"h3"},{"title":"3. Postage stamp batch and chunk related depths​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#3-postage-stamp-batch-and-chunk-related-depths","content":" ","version":"Next","tagName":"h3"},{"title":"3a. Batch depth​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#3a-batch-depth","content":" Batch depth is the value d which is defined in relation to the size of a postage stamp batch. The size of a batch is defined as the number of chunks which can be stamped by that batch (also referred to as the number of slots per batch, with one chunk per slot). The size is calculated by:  2d2^{d}2dddd is a value selected by the batch issuer which determines how much data can be stamped with the batch  ","version":"Next","tagName":"h3"},{"title":"3b. Bucket depth​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#3b-bucket-depth","content":" Bucket depth is the constant value which defines how many buckets the address space for chunks is divided into for postage stamp batches. Bucket depth is set to 16, and the number of buckets is defined as 2bucketdepth2^{bucket depth}2bucketdepth  ","version":"Next","tagName":"h3"},{"title":"PLUR​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#plur","content":" PLUR (name inspired by the PLUR principles) is the smallest denomination of BZZ. 1 PLUR is equal to 1e-16 BZZ.  ","version":"Next","tagName":"h2"},{"title":"Bridged Tokens​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#bridged-tokens","content":" Bridged tokens are tokens from one blockchain which have been bridged to another chain through a smart contract powered bridge. For example, xDAI and xBZZ on Gnosis Chain are the bridged version of DAI and BZZ on Ethereum.  ","version":"Next","tagName":"h2"},{"title":"BZZ Token​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#bzz-token","content":" BZZ is Swarm's ERC-20 token issued on Ethereum.  Blockchain\tContract addressEthereum, BZZ\t0x19062190b1925b5b6689d7073fdfc8c2976ef8cb Gnosis Chain, xBZZ\t0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da Sepolia (testnet), sBZZ\t0x543dDb01Ba47acB11de34891cD86B675F04840db  ","version":"Next","tagName":"h2"},{"title":"xBZZ Token​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#xbzz-token","content":" xBZZ is BZZ bridged to the Gnosis Chain using the Gnosis Chain Bridge.  It is used as payment for postage stamps and as the unit of accounting between the nodes. It is used to incentivize nodes to provide resources to the Swarm.  ","version":"Next","tagName":"h2"},{"title":"DAI Token​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#dai-token","content":" DAI is an ERC-20 stable token issued on the Ethereum blockchain, tracking USD.  ","version":"Next","tagName":"h2"},{"title":"xDAI Token​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#xdai-token","content":" xDAI is DAI bridged to the Gnosis Chain using xDai Bridge. It is also the native token of the Gnosis Chain, i.e. transaction fees are paid in xDai.  ","version":"Next","tagName":"h2"},{"title":"Sepolia​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#sepolia","content":" Sepolia is an Ethereum testnet. It is an environment where smart contracts can be developed and tested without spending cryptocurrency with real value, and without putting valuable assets at risk. Tokens on Sepolia are often prefixed with a lower-case 's', example: 'sBZZ' and because this is a test network carry no monetary value. It is an environment where Bee smart contracts can be tested and interacted with without any risk of monetary loss.  ","version":"Next","tagName":"h2"},{"title":"Faucet​","type":1,"pageTitle":"Glossary","url":"/docs/learn/glossary#faucet","content":" A cryptocurrency faucet supplies small amounts of cryptocurrency to requestors (typically for testing purposes).  It supplies small amounts of sBZZ and Sepolia ETH for anyone who submits a request at the Swarm Discord server by using the /faucet command in the #develop-on-swarm channel. ","version":"Next","tagName":"h2"},{"title":"Chequebook","type":0,"sectionRef":"#","url":"/docs/learn/technology/contracts/chequebook","content":"Chequebook The chequebook contract is a smart contract used in the Swarm Accounting Protocol (SWAP) to manage cheques that are sent between nodes on the network. The contract is responsible for keeping track of the balances of each node and ensuring that cheques are valid and can be cashed out correctly. When a node sends a cheque to another node, it includes a signed message that specifies the amount of xBZZ tokens being transferred and the recipient's address. The chequebook contract receives this message and verifies that it is valid by checking the signature and ensuring that the sender has enough funds to cover the transfer. caution Settlement of cheques is not enforced by smart contract. If the cheque is valid, the contract updates the balances of both nodes accordingly. The recipient can then cash out their xBZZ tokens by sending a transaction to the blockchain that invokes a function in the chequebook contract. This function transfers the specified amount of xBZZ tokens from the sender's account to the recipient's account. The chequebook contract also includes some additional features to prevent abuse. For example, it can impose limits on how much debt a node can accumulate before requiring payment. The chequebook contract plays an important role in ensuring that SWAP operates smoothly and fairly by providing a secure and reliable way for nodes to exchange value on the network.","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/learn/technology/contracts/overview","content":"","keywords":"","version":"Next"},{"title":"Token Contracts​","type":1,"pageTitle":"Overview","url":"/docs/learn/technology/contracts/overview#token-contracts","content":" Contract\tBlockchain\tAddressBZZ token\tEthereum\t0x19062190b1925b5b6689d7073fdfc8c2976ef8cb xBZZ token\tGnosis Chain\t0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da sBZZ token\tSepolia (Ethereum testnet)\t0x543dDb01Ba47acB11de34891cD86B675F04840db BZZ Bonding Curve\tEthereum\t0x4F32Ab778e85C4aD0CEad54f8f82F5Ee74d46904  ","version":"Next","tagName":"h3"},{"title":"Storage Incentives Contracts​","type":1,"pageTitle":"Overview","url":"/docs/learn/technology/contracts/overview#storage-incentives-contracts","content":" Storage Incentives Github Repo.  Contract\tBlockchain\tAddressPostage Stamp\tGnosis Chain\t0x45a1502382541Cd610CC9068e88727426b696293 Staking\tGnosis Chain\t0x781c6D1f0eaE6F1Da1F604c6cDCcdB8B76428ba7 Redistribution\tGnosis Chain\t0x0964c834C660C44E0afd3B7F10F19f275ee31411 Price Oracle\tGnosis Chain\t0x86DE783Bf23Bc13DaeF5A55ec531C198da8f10cF ","version":"Next","tagName":"h3"},{"title":"Price Oracle","type":0,"sectionRef":"#","url":"/docs/learn/technology/contracts/price-oracle","content":"Price Oracle The job of the Oracle contract is to set the price of Postage Stamps. The oracle contract uses data from the Postage Stamp contract in order to set the appropriate price for Postage Stamps. The data in the Postage Stamp contract is used to calculate a &quot;utilisation signal&quot;. This signal is an indicator of how much the Swarm network’s data storage capacity is being utilized. Specifically, the signal is a measure of data redundancy on the network. Redundancy is a measure of how many copies of each piece of data can be stored by the network. The protocol targets a fourfold level of data redundancy as a safe minimum. For example, if there is an increase in postage stamps being purchased while the number of nodes remains constant, the data redundancy level will begin to fall as data storers’ available space begins to become reserved. If too many postage stamps are purchased without an equivalent increase in storage providers, the redundancy level may fall below four. In this case, the oracle will increase the price of postage stamps so that it becomes more expensive to store data on Swarm. The higher cost of storage will then lead to less postage stamps being purchased, and will push the redundancy level back up towards four. Conversely, if the amount of Stamps being purchased decreases while the number of storage provider nodes remains constant, the redundancy level will increase as there are fewer chunks of data to be distributed amongst the same number of nodes. In this case, the oracle will decrease the Postage Stamp price in order to promote more data storers to store their data on Swarm. The lower cost of storage will then lead to more Postage Stamps being purchased and push the redundancy level back down towards four.","keywords":"","version":"Next"},{"title":"DISC","type":0,"sectionRef":"#","url":"/docs/learn/technology/disc","content":"","keywords":"","version":"Next"},{"title":"Kademlia Topology and Routing​","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#kademlia-topology-and-routing","content":" Each node within Swarm connects to a certain number of its peers. When a chunk is first inserted into the network, the uploading node will send it to the peer which is closest (as measured by proximity order, which is based on a measure of Kademlia distance) to the destination of the chunk, and then the recipient node will then forward the chunk on to its peer which is closest to the destination of the chunk, and so on until the chunk arrives at its destination.  One of the advantages of using Kademlia as a model for network topology is that both the number of forwarding &quot;hops&quot; required to route a chunk to its destination and the number of peer connections required to maintain Kademlia topology are logarithmic to the size of the network (a minimum of two connections is required in order to maintain Kademlia topology in case of network churn - nodes dropping in and out of the network). This makes Swarm a highly scalable system which is efficient even at very large scales.  ","version":"Next","tagName":"h3"},{"title":"Neighborhoods​","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#neighborhoods","content":" Neighborhoods are groups of nodes which are responsible for sharing the same chunks. The chunks which each neighborhood is responsible for storing are defined by the proximity order of the nodes and the chunks. In other words, each node is responsible for storing chunks with which their overlay addresses share a certain number of prefix bits, and together with other nodes which share the same prefix bits, make up neighborhoods which share the responsibility for storing the same chunks.  ","version":"Next","tagName":"h3"},{"title":"Chunks​","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#chunks","content":" In the DISC model, chunks are the canonical unit of data. When a file is uploaded to Swarm, it gets broken down into 4kb pieces with attached metadata. The pieces then get distributed amongst nodes in the Swarm network based on their overlay addresses. There are two fundamental chunk types: content-addressed chunks and single-owner chunks.  ","version":"Next","tagName":"h3"},{"title":"Content-Addressed Chunks and Single-Owner Chunks​","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#content-addressed-chunks-and-single-owner-chunks","content":" Content-addressed chunks are chunks whose address is based on the hash digest of their data. Using a hash as the chunk address makes it possible to verify the integrity of chunk data. Swarm uses the BMT hash function based on a binary Merkle tree over small segments of the chunk data. A content-addressed chunk has an at most 4KB payload, and its address is calculated as the hash of the span (chunk metadata) and the Binary Merkle Tree hash of the payload.  For single-owner chunks on the other hand, the address is calculated as the hash of a unique id and the owner's overlay address. The content consists of an arbitrary data payload along with required headers. Unlike a content-addressed chunk, the contents of a single-owner chunk may be updated while the address remains unchanged. Single owner chunks form the basis for feeds, which are data structures that allow for mutable content with a static address.  ","version":"Next","tagName":"h3"},{"title":"Push-Sync, Pull-Sync, and Retrieval Protocols​","type":1,"pageTitle":"DISC","url":"/docs/learn/technology/disc#push-sync-pull-sync-and-retrieval-protocols","content":" When a file is first uploaded to Swarm, it gets broken down by the uploading Bee node chunks which are then distributed amongst other Bee nodes in the Swarm network. Chunks get distributed to the target neighborhood by the push-sync protocol. Once a chunk reaches its destination, it will then be duplicated and synced to other nodes in order to achieve data redundancy through the pull-sync protocol. The pull-sync protocol operates continuously as nodes enter or exit the network – ensuring that data redundancy is always maintained. When a client node requests a file for download, its request gets forwarded by the retrieval-protocol to all the nodes storing the relevant chunks, and then those chunks get returned to the requesting node and the file gets reconstructed from its constituent chunks. ","version":"Next","tagName":"h3"},{"title":"Postage Stamp","type":0,"sectionRef":"#","url":"/docs/learn/technology/contracts/postage-stamp","content":"","keywords":"","version":"Next"},{"title":"Batch Buckets​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-buckets","content":" Postage stamps are issued in batches with a certain number of storage slots partitioned into 2bucketDepth2^{bucketDepth}2bucketDepth equally sized address space buckets. Each bucket is responsible for storing chunks that fall within a certain range of the address space. When uploaded, files are split into 4kb chunks, each chunk is assigned a unique address, and each chunk is then assigned to the bucket in which its address falls. Falling into the same range means a match on n leading bits of the chunk and bucket. This restriction is necessary to ensure (incentivise) uniform utilisation of the address space and is fair since the distribution of content addresses are uniform as well. Uniformity depth is the number of leading bits determining bucket membership (also called bucket depth). The uniformity depth is set to 16, so there are a total of 216=65,5362^{16} = 65,536216=65,536 buckets.  ","version":"Next","tagName":"h2"},{"title":"Bucket Size​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#bucket-size","content":" Each bucket has a certain number of slots which can be &quot;filled&quot; by chunks (In other words, for each bucket, a certain number of chunks can be stamped). Once all the slots of a bucket are filled, the entire postage batch will be fully utilised and can no longer be used to upload additional data.  Together with batch depth, bucket depthdetermines how many chunks are allowed in each bucket. The number of chunks allowed in each bucket is calculated like so:  2(batchDepth−bucketDepth)2^{(batchDepth - bucketDepth)}2(batchDepth−bucketDepth)  So with a batch depth of 24 and a bucket depth of 16:  2(24−16)=28=256 chunks/bucket2^{(24 - 16)} = 2^{8} = 256 \\text{ chunks/bucket}2(24−16)=28=256 chunks/bucket  info Note that due how buckets fill as described above, a batch can become fully utilised before its theoretical maximum volume has been reached. See batch utilisation section below for more information.  ","version":"Next","tagName":"h3"},{"title":"Batch Depth and Batch Amount​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-depth-and-batch-amount","content":" Each batch of stamps has two key parameters, batch depth and amount, which are recorded on Gnosis Chain at issuance. Note that these &quot;depths&quot; do not refer to the depth terms used to describe topology which are outlined here in the glossary.  ","version":"Next","tagName":"h2"},{"title":"Batch Depth​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-depth","content":" caution The minimum value for depth is 17, however higher depths are recommended for most use cases due to the mechanics of stamp batch utilisation. See the depths utilisation table to help decide which depth is best for your use case.  Batch depth determines how much data can be stored by a batch. The number of chunks which can be stored (stamped) by a batch is equal to 2batchDepth2^{batchDepth}2batchDepth.  For a batch with a batch depth of 24, a maximum of 224=16,777,2162^{24} = 16,777,216224=16,777,216 chunks can be stamped.  Since we know that one chunk can store 4 kb of data, we can calculate the theoretical maximum amount of data which can be stored by a batch from the batch depth.  Theoretical maximum batch volume=2batchDepth×4 kb\\text{Theoretical maximum batch volume} = 2^{batchDepth} \\times \\text{4 kb} Theoretical maximum batch volume=2batchDepth×4 kb  However, due to the way postage stamp batches are utilised, batches will become fully utilised before stamping the theoretical maximum number of chunks. Therefore when deciding which batch depth to use, it is important to consider the effective amount of data that can be stored by a batch, and not the theoretical maximum. The effective rate of utilisation increases along with the batch depth. See section on stamp batch utilisation below for more information.  ","version":"Next","tagName":"h3"},{"title":"Batch Amount (& Batch Cost)​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-amount--batch-cost","content":" The amount parameter is the quantity of xBZZ in PLUR (1×1016PLUR=1 xBZZ)(1 \\times 10^{16}PLUR = 1 \\text{ xBZZ})(1×1016PLUR=1 xBZZ) that is assigned per chunk in the batch. The total number of xBZZ that will be paid for the batch is calculated from this figure and the batch depth like so:  2batchDepth×amount2^{batchDepth} \\times {amount}2batchDepth×amount  The paid xBZZ forms the balance of the batch. This balance is then slowly depleted as time ticks on and blocks are mined on Gnosis Chain.  For example, with a batch depth of 24 and an amount of 1000000000 PLUR:  224×1000000000=16777216000000000 PLUR=1.6777216 xBZZ2^{24} \\times 1000000000 = 16777216000000000 \\text{ PLUR} = 1.6777216 \\text{ xBZZ}224×1000000000=16777216000000000 PLUR=1.6777216 xBZZ  ","version":"Next","tagName":"h3"},{"title":"Calculating amount needed for desired TTL​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#calculating-amount-needed-for-desired-ttl","content":" The desired amount can be easily estimated based on the current postage stamp price and the desired amount of storage time in seconds with the given Gnosis block time of 5 seconds and the stamp price. For the example below we assume a stamp price of 24000 PLUR / chunk / block:  info The postage stamp price is dynamically determined according to a network utilisation signal. You can view the current storage price at Swarmscan.io.  (stamp price÷block time in seconds)×storage time in seconds(\\text{stamp price} \\div \\text{block time in seconds}) \\times \\text{storage time in seconds}(stamp price÷block time in seconds)×storage time in seconds  There are 1036800 seconds in 12 days, so the amount value required to store for 12 days can be calculated:  (24000÷5)×1036800=4976640000(\\text{24000} \\div \\text{5}) \\times \\text{1036800} = 4976640000(24000÷5)×1036800=4976640000  So we can use 4976640000 as our amount value in order for our postage batch to store data for 12 days.  ","version":"Next","tagName":"h3"},{"title":"Batch Utilisation​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#batch-utilisation","content":" ","version":"Next","tagName":"h2"},{"title":"Immutable Batches​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#immutable-batches","content":" Utilisation of an immutable batch is computed using a hash map of size 2bucketDepth2^{bucketDepth}2bucketDepth which is 2162^{16}216 for all batches, so 65536 total entries. For the keys of the key-value pairs of the hash map, the keys are 16 digit binary numbers from 0 to 65535, and the value is a counter.    As chunks are uploaded to Swarm, each chunk is assigned to a bucket based the first 16 binary digits of the chunk's hash. The chunk will be assigned to whichever bucket's key matches the first 16 bits of its hash, and that bucket's counter will be incremented by 1.  The batch is deemed &quot;full&quot; when ANY of these counters reach a certain max value. The max value is computed from the batch depth as such: 2(batchDepth−bucketDepth)2^{(batchDepth-bucketDepth)}2(batchDepth−bucketDepth). For example with batch depth of 24, the max value is 2(24−16)2^{(24-16)}2(24−16) or 256. A bucket can be thought of as have a number of &quot;slots&quot; equal to this maximum value, and every time the bucket's counter is incremented, one of its slots gets filled.  In the diagram below, the batch depth is 18, so there are 2(18−16)2^{(18-16)}2(18−16) or 4 slots for each bucket. The utilisation of a batch is simply the highest number of filled slots out of all 65536 entries or &quot;buckets&quot;. In this batch, none of the slots in any of the buckets have yet been filled with 4 chunks, so the batch is not yet fully utilised. The most filled slots out of all buckets is 2, so the stamp batch's utilisation is 2 out of 4.    As more chunks get uploaded and stamped, the bucket slots will begin to fill. As soon as the slots for any SINGLE bucket get filled, the entire batch is considered 100% utilised and can no longer be used to upload additional chunks.    ","version":"Next","tagName":"h3"},{"title":"Mutable Batches​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#mutable-batches","content":" Mutable batches use the same hash map structure as immutable batches, however its utilisation works very differently. In contrast with immutable batches, mutable batches are never considered fully utilised. Rather, at the point where an immutable batch would be considered fully utilised, a mutable batch can continue to stamp chunks. However, if any chunk's address lands in a bucket whose slots are already filled, rather than the batch becoming fully utilised, that bucket's counter gets reset, and the new chunk will replace the oldest chunk in that bucket.    Therefore rather than speaking of the number of slots as determining the utilisation of a batch as with immutable batches, we can think of the slots as defining a limit to the amount of data which can be uploaded before old data starts to get overwritten.  ","version":"Next","tagName":"h3"},{"title":"Which Type of Batch to Use​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#which-type-of-batch-to-use","content":" Immutable batches are suitable for long term storage of data or for data which otherwise does not need to be changed and should never be overwritten, such as records archival, legal documents, family photos, etc.  Mutable batches are great for data which needs to be frequently updated and does not require a guarantee of immutability. For example, a blog, personal or company websites, ephemeral messaging app, etc.  The default batch type when unspecified is immutable. This can be modified through the Bee api by setting the immutable header with the \\stamps POST endpoint to false.  ","version":"Next","tagName":"h3"},{"title":"Re-uploading​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#re-uploading","content":" There are several nuances to how the re-uploading of previously uploaded data to Swarm affect stamp batch utilisation. For single chunks, the behaviour is relatively straightforward, however with files that must get split into multiple chunks, the behaviour is less straightforward.  Single chunks​  When a chunk which has previously been uploaded to Swarm is re-uploaded from the same node while the initial postage batch it was stamped by is still valid, no additional stamp will be utilised from the batch. However if the chunk comes from a different node than the original node, then a stamp WILL be utilised, and as long as at least one of the batches the chunk was stamped by is still valid, the chunk will be retained by storer nodes in its neighbourhood.  Files​  When an identical file is re-uploaded then the stamp utilisation behaviour will be the same as with single chunks described in the section above. However, if part of the file has been modified and then re-uploaded, stamp utilisation behaviour will be different. This is due to how the chunking process works when a file is uploaded to Swarm. When uploaded to Swarm, files are split into 4kb sized chunks (2^12 bytes), and each chunk is assigned an address which is based on the content of the chunk. If even a single bit within the chunk is modified, then the address of the chunk will also be modified.  When a file which was previously uploaded with a single bit flipped is again split into chunks by a node before being uploaded to Swarm, then only the chunk with the flipped bit will have an updated address and require the utilisation of another stamp. The content of all the other chunks will remain the same, and therefore will not require new stamps to be utilised.  However, if rather than flipping a single bit we add some data to our file, this could cause changes in the content of every chunk of the file, meaning that every single chunk must be re-stamped. We can use a simplified example of why this is the case to more easily understand the stamp utilisation behaviour. Let us substitute a message containing letters of the alphabet rather than binary data.  Our initial message consists of 16 letters:  abcdefghijklmnop  When initially uploaded, it will be split into four chunks of four letters each:  abcdefghijklmnop =&gt; abcd | efgh | ijkl | mnop  Let us look at what happens when a single letter is changed (here we change a to z):  abcdefghijklmnop =&gt; zbcd | efgh | ijkl | mnop  In this case, only the first chunk is affected, all the other chunks retain the same content.  Now let is examine the case where a new letter is added rather than simply modifying an already existing one. Here we add the number 1 at the start of the message:  1abcdefghijklmnop =&gt; 1abc | defg | hijk | lmno | p  As you can see, by adding a single new letter at the start of the message, all the letters are shifted to the right by a single position, which a has caused EVERY chunk in the message to be modified rather than just a single chunk.  Affect on Batch Utilisation​  The implications of this behaviour are that even a small change to the data of a file may cause every single chunk from the file to be changed, meaning that new stamps must be utilised for every chunk from that file. In practice, this could lead to high costs in data which is frequently changed, since for even a small change, every chunk from the file must be re-stamped.  ","version":"Next","tagName":"h3"},{"title":"Implications for Swarm Users​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#implications-for-swarm-users","content":" Due to the nature of batch utilisation described above, batches are often fully utilised before reaching their theoretical maximum storage amount. However as the batch depth increases, the chance of a postage batch becoming fully utilised early decreases. At batch depth 24, there is a 0.1% chance that a batch will be fully utilised/start replacing old chunks before reaching 64.33% of its theoretical maximum.  Let's look at an example to make it clearer. Using the method of calculating the theoretical maximum storage amount outlined above, we can see that for a batch depth of 24, the theoretical maximum amount which can be stored is 68.72 gb:  224+12=68,719,476,736 bytes=68.72 gb2^{24+12} = \\text{68,719,476,736 bytes} = \\text{68.72 gb}224+12=68,719,476,736 bytes=68.72 gb  Therefore we should use 64.33% the effective rate of usage for the stamp batch:  68.72 gb×0.6433=44.21 gb \\text{68.72 gb} \\times{0.6433} = \\text{44.21 gb }68.72 gb×0.6433=44.21 gb   info The details of how the effective rates of utilisation are calculated will be published soon.  ","version":"Next","tagName":"h3"},{"title":"Effective Utilisation Table​","type":1,"pageTitle":"Postage Stamp","url":"/docs/learn/technology/contracts/postage-stamp#effective-utilisation-table","content":" When a user buys a batch of stamps they may make the naive assumption that they will be able to upload data equal to the sum total size of the maximum capacity of the batch. However, in practice this assumption is incorrect, so it is essential that Swarm users understand the relationship between batch depth and the theoretical and effective volumes of a batch.  The provided table shows the effective volume for each batch depth from 20 to 41 (note that currently the minimum stamp batch depth is 17, however 22 is the first depth with an effective volume above zero). The utilisation rate is the rate of utilisation of the theoretical max volume that a stamp batch can reach with a 0.1% failure rate (that is, there is only a one-in-a-thousand chance that the difference between the actual effectively utilised volume and effective volume shown in the table is greater than 0.1%). The &quot;effective volume&quot; figure shows the actual amount of data which can be stored at the effective rate. The effective volume figure is the one which should be used as the de-facto maximum amount of data that a batch can store before becoming either fully utilised (for immutable batches), or start overwriting older chunks (mutable batches).  Batch Depth\tUtilisation Rate\tTheoretical Max Volume\tEffective Volume20\t0.00%\t4.29 GB\t0.00 B 21\t0.00%\t8.59 GB\t0.00 B 22\t28.67%\t17.18 GB\t4.93 GB 23\t49.56%\t34.36 GB\t17.03 GB 24\t64.33%\t68.72 GB\t44.21 GB 25\t74.78%\t137.44 GB\t102.78 GB 26\t82.17%\t274.88 GB\t225.86 GB 27\t87.39%\t549.76 GB\t480.43 GB 28\t91.08%\t1.10 TB\t1.00 TB 29\t93.69%\t2.20 TB\t2.06 TB 30\t95.54%\t4.40 TB\t4.20 TB 31\t96.85%\t8.80 TB\t8.52 TB 32\t97.77%\t17.59 TB\t17.20 TB 33\t98.42%\t35.18 TB\t34.63 TB 34\t98.89%\t70.37 TB\t69.58 TB 35\t99.21%\t140.74 TB\t139.63 TB 36\t99.44%\t281.47 TB\t279.91 TB 37\t99.61%\t562.95 TB\t560.73 TB 38\t99.72%\t1.13 PB\t1.12 PB 39\t99.80%\t2.25 PB\t2.25 PB 40\t99.86%\t4.50 PB\t4.50 PB 41\t99.90%\t9.01 PB\t9.00 PB  info This table is based on preliminary calculations and may be subject to change.  Nodes' storage is actually defined as a number of chunks with a size of 4kb (2^12 bytes) each, but in fact some SOC chunks can be a few bytes longer, and some chunks can be smaller, so the conversion is not precise. Furthermore, due to the way Swarm represents files in a Merkle tree, the intermediate chunks are additional overhead which must also be accounted for.  Additionally, when a node stores chunks it uses additional indexes — therefore the disk space a maximally filled reserve would demand cannot be calculated with perfect accuracy. ","version":"Next","tagName":"h3"},{"title":"What is Swarm?","type":0,"sectionRef":"#","url":"/docs/learn/technology/what-is-swarm","content":"","keywords":"","version":"Next"},{"title":"1. Underlay Network​","type":1,"pageTitle":"What is Swarm?","url":"/docs/learn/technology/what-is-swarm#1-underlay-network","content":" The first part of Swarm is a peer-to-peer network protocol that serves as the underlay transport. The underlay transport layer is responsible for establishing connections between nodes in the network and routing data between them. It provides a low-level communication channel that enables nodes to communicate with each other directly, without relying on any centralised infrastructure.  Swarm is designed to be agnostic of the particular underlay transport used, as long as it satisfies certain requirements.  Addressing – Nodes are identified by their underlay address.Dialling – Nodes can initiate a direct connection to a peer by dialing them on their underlay address.Listening – Nodes can listen to other peers dialing them and can accept incoming connections. Nodes that do not accept incoming connections are called light nodes.Live connection – A node connection establishes a channel of communication which is kept alive until explicit disconnection, so that the existence of a connection means the remote peer is online and accepting messages.Channel security – The channel provides identity verification and implements encrypted and authenticated transport resisting man in the middle attacks.Protocol multiplexing – The underlay network service can accommodate several protocols running on the same connection.Delivery guarantees – Protocol messages have guaranteed delivery, i.e. delivery failures due to network problems result in direct error response. Order of delivery of messages within each protocol is guaranteed.Serialisation – The protocol message construction supports arbitrary data structure serialisation conventions.  As the libp2p library meets all these requirements it has been used to build the Swarm underlay network.  ","version":"Next","tagName":"h3"},{"title":"2. Overlay Network​","type":1,"pageTitle":"What is Swarm?","url":"/docs/learn/technology/what-is-swarm#2-overlay-network","content":" The second part of Swarm is an overlay network with protocols powering the Distributed Immutable Store of Chunks (DISC). This layer is responsible for storing and retrieving data in a decentralised and secure manner.  Swarm's overlay network is built on top of the underlay transport layer and uses Kademlia overlay routing to enable efficient and scalable communication between nodes. Kademlia is a distributed hash table (DHT) algorithm that allows nodes to locate each other in the network based on their unique identifier or hash.  Swarm's DISC is an implementation of a Kademlia DHT optimized for storage. While the use of DHTs in distributed data storage protocols is common, for many implementations DHTs are used only for indexing of specific file locations. Swarm's DISC distinguishes itself from other implementations by instead breaking files into chunks and storing the chunks themselves directly within a Kademlia DHT.  Each chunk has a fixed size of 4kb and is distributed across the network using the DISC model. Each chunk has a unique address taken from the same namespace as the network node addresses that allows it to be located and retrieved by other nodes in the network.  Swarm's distributed immutable storage provides several benefits, including data redundancy, tamper-proofing, and fault tolerance. Because data is stored across multiple nodes in the network, it can be retrieved even if some nodes fail or go offline.  ","version":"Next","tagName":"h3"},{"title":"3. Data Access Layer​","type":1,"pageTitle":"What is Swarm?","url":"/docs/learn/technology/what-is-swarm#3-data-access-layer","content":" The third part of Swarm is a component that provides high-level data access and defines APIs for base-layer features. This layer is responsible for providing an easy-to-use interface for developers to interact with Swarm's underlying storage and communication infrastructure.  Swarm's high-level data access component provides APIs that allow developers to perform various operations on the network, including uploading and downloading data and searching for content. These APIs are designed to be simple and intuitive, making it easy for developers to build decentralised applications on top of Swarm.  ","version":"Next","tagName":"h3"},{"title":"4. Application Layer​","type":1,"pageTitle":"What is Swarm?","url":"/docs/learn/technology/what-is-swarm#4-application-layer","content":" The fourth part of Swarm is an application layer that defines standards and outlines best practices for more elaborate use cases. This layer is responsible for providing guidance to developers on how to build complex applications on top of Swarm's underlying infrastructure. ","version":"Next","tagName":"h3"},{"title":"Incentives","type":0,"sectionRef":"#","url":"/docs/learn/technology/incentives","content":"","keywords":"","version":"Next"},{"title":"Storage Incentives​","type":1,"pageTitle":"Incentives","url":"/docs/learn/technology/incentives#storage-incentives","content":" Swarm's storage incentives protocol is defined in depth in the Future Proof Storage paper published by the Swarm team.  Swarm's storage incentives are based on postage stamps, which serve as verifiable proof of payment associated with chunks witnessed by their owner's signature. Postage stamps signal chunks' relative importance by ascribing them with xBZZ quantity which storer nodes can use when selecting which chunks to retain and which to evict from their reserve when their reserve capacity is exceeded.  The amount of xBZZ required for a postage stamp depends on the amount of data being stored and the duration for which it will be stored. The longer a chunk is stored, the more xBZZ is required for the postage stamp. This ensures that users are incentivised to store data for longer periods, which helps ensure that data remains available in the network.  Storer nodes can use the xBZZ associated with postage stamps when selecting which chunks to retain and serve or garbage collect during capacity shortage. This means that popular content will be widely distributed across the network, reducing retrieval latency.  ","version":"Next","tagName":"h2"},{"title":"Storage Incentives Details​","type":1,"pageTitle":"Incentives","url":"/docs/learn/technology/incentives#storage-incentives-details","content":" When someone wants to upload data to Swarm, they do so by buying postage stamp batches with xBZZ. The xBZZ is collected and later redistributed to storage provider nodes to pay for their services. Which node earns the reward is determined by playing a &quot;game&quot;. Every 152 Gnosis Chain blocks the game is played, and one node will win the accumulated xBZZ.  The game has 3 phases, commit, reveal, and claim. In the reveal phase of a previous game, an &quot;anchor&quot; overlay address is randomly generated and used to determine the neighborhood for the current round. Only nodes within that neighborhood (meaning they have a certain number of shared leading bits with the neighborhood address) may participate and have a chance to win.  In the commit phase, nodes issue an on-chain transaction including an encrypted hash of the data they are storing (the unencrypted hash is known as the &quot;reserve commitment&quot;) along with the depth for which they are reporting. This serves as an attestation of the data they are storing without revealing any other information.  In the reveal phase, each node reveals the decryption key for their encrypted hashes thereby publishing the hash. One of the nodes is chosen as the honest node, and from among the honest nodes, one node is chosen as the winner. The winner is chosen at random among the honest nodes, but it is weighted in proportion to each node's stake density. Stake density is calculated as so:  stake density=stake(xBZZ)×2storage depth\\text{stake density} = \\text{stake(xBZZ)} \\times {2}^\\text{storage depth}stake density=stake(xBZZ)×2storage depth  ","version":"Next","tagName":"h3"},{"title":"Penalties​","type":1,"pageTitle":"Incentives","url":"/docs/learn/technology/incentives#penalties","content":" During the reveal phase if a nodes' revealed hash does not match the honest nodes' hash, that node will be temporarily frozen and will not be able to participate in a number of upcoming rounds. Currently the freeze period is defined in the redistribution smart contract as:  152×2storage radius blocks (at 5s per block)152 \\times 2^\\text{storage radius} \\text{ blocks (at 5s per block)}152×2storage radius blocks (at 5s per block)  So for example at a storage radius of 10:  152×210 blocks (at 5s per block)≈ 9 days152 \\times 2^{10} \\text{ blocks (at 5s per block)} ≈ \\text{ 9 days}152×210 blocks (at 5s per block)≈ 9 days  ","version":"Next","tagName":"h3"},{"title":"Bandwidth Incentives (SWAP)​","type":1,"pageTitle":"Incentives","url":"/docs/learn/technology/incentives#bandwidth-incentives-swap","content":" The Swarm Accounting Protocol (SWAP) is a protocol used in the Swarm network to manage the exchange of resources between nodes. SWAP ensures that node operators collaborate in routing messages and data while protecting the network against frivolous use of bandwidth.  SWAP works by tracking the relative consumption of bandwidth between nodes. As nodes relay requests and responses, they keep track of their bandwidth usage with each of their peers. Within bounds, peers engage in a service-for-service exchange, where they provide resources to each other based on their relative usage.  However, once a limit is reached, the party in debt can either wait until their liabilities are amortized over time or can pay by sending cheques that cash out in xBZZ on the blockchain. Chequebook contracts are used to manage these cheques and ensure that they are valid and can be cashed out correctly.  SWAP uses built-in incentives to optimize the allocation of bandwidth and storage resources and render Swarm economically self-sustaining. Swarm nodes track their relative bandwidth contribution on each peer connection, and excess debt due to unequal consumption can be settled in xBZZ. Publishers in Swarm must spend xBZZ to purchase the right to write data to Swarm and prepay some rent for long-term storage.  The SWAP protocol also includes some additional features to prevent abuse or fraud. For example, it can impose limits on how much debt a node can accumulate before requiring payment or require nodes to provide collateral before sending cheques. ","version":"Next","tagName":"h2"},{"title":"PSS","type":0,"sectionRef":"#","url":"/docs/learn/technology/pss","content":"PSS PSS, or Postal Service over Swarm, is a messaging protocol that enables users to send and receive messages over Swarm. It is an essential component of Swarm's infrastructure, providing secure, private, and efficient communication between nodes. PSS is designed to be secure by encrypting messages for the intended recipient and wrapping them with a topic in a content-addressed chunk. The chunk is crafted in such a way that its content address falls into the recipient's neighborhood, ensuring that delivery is naturally taken care of by the push-sync protocol. This ensures that messages are delivered only to the intended recipient's neighborhood and cannot be intercepted or read by unauthorized parties. While the chunk will be delivered to all members of the recipient's neighborhood, only the recipient will be able to decrypt the message using their private key. PSS also provides privacy by allowing users to receive messages from previously unknown identities. This makes it an ideal communication primitive for sending anonymous messages to public identities such as registrations or initial contact to start a thread by setting up secure communication. Efficiency is another key feature of PSS. It uses direct node-to-node messaging in Swarm, which means that messages are delivered directly from one node to another without the need for intermediaries. This reduces latency and ensures that messages are delivered quickly and reliably. PSS also supports mailboxing, which allows users to deposit messages for download if the recipient is not online. This ensures that messages are not lost if the recipient is offline when they are sent.","keywords":"","version":"Next"},{"title":"Tokens","type":0,"sectionRef":"#","url":"/docs/learn/tokens","content":"","keywords":"","version":"Next"},{"title":"Swarm Ecosystem Tokens​","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#swarm-ecosystem-tokens","content":" ","version":"Next","tagName":"h2"},{"title":"BZZ​","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#bzz","content":" info On May 4th of 2024, as a result of a community vote, the bonding curve was shut down and the BZZ supply is now fixed at 63,149,437.  BZZ is the original token issued from the Ethswarm Bonding Curve contract on the Ethereum blockchain.  BZZ Ethereum address: 0x19062190b1925b5b6689d7073fdfc8c2976ef8cb  PLUR is the smallest denomination of BZZ. 1 PLUR is equal to 1e-16 BZZ.  ","version":"Next","tagName":"h3"},{"title":"xBZZ​","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#xbzz","content":" &quot;xBZZ&quot; is the term used to indicate BZZ on Gnosis Chain. It is the bridged version of the original Ethereum BZZ token issued on Gnosis Chain. xBZZ is the token used for staking and to pay for storage fees on Swarm.  xBZZ Gnosis Chain address: 0xdBF3Ea6F5beE45c02255B2c26a16F300502F68da  info Note that the ticker symbol is the same BZZ for both Gnosis Chain and Ethereum versions of the token. xBZZ is term of convenience used to differentiate the tokens within the Swarm community.  As with BZZ, PLUR is the smallest denomination of xBZZ. 1 PLUR is equal to 1e-16 xBZZ.  ","version":"Next","tagName":"h3"},{"title":"sBZZ​","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#sbzz","content":" sBZZ is the testnet version of BZZ on the Sepolia Ethereum testnet.  Sepolia testnet address: 0x543dDb01Ba47acB11de34891cD86B675F04840db  ","version":"Next","tagName":"h3"},{"title":"DAI​","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#dai","content":" DAI is the popular decentralized stablecoin from MakerDAO.  ","version":"Next","tagName":"h3"},{"title":"xDAI​","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#xdai","content":" xDAI is the bridged version of DAI on Gnosis Chain and also serves as the native gas token for Gnosis Chain and is used to pay transaction fees on Gnosis Chain in the same way ETH is used to pay for transactions on Ethereum. It is required by Bee nodes to pay for transaction fees when interacting with Swarm smart contracts on Gnosis Chain.  ","version":"Next","tagName":"h3"},{"title":"Getting BZZ / xBZZ​","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#getting-bzz--xbzz","content":" The Swarm official website has a page with a list of resources for getting BZZ tokens. Be careful to check whether it is BZZ on Ethereum or Gnosis Chain.  ","version":"Next","tagName":"h3"},{"title":"Bridging BZZ to xBZZ or DAI to xDAI​","type":1,"pageTitle":"Tokens","url":"/docs/learn/tokens#bridging-bzz-to-xbzz-or-dai-to-xdai","content":" If you already have DAI or BZZ on Ethereum then you can use the Gnosis Chain Bridge for swapping between DAI and xDAI or BZZ and xBZZ. ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}